String: ontology-"semantic annotation"-"knowledge graphs"-"qualitative analysis"-"quantitative analysis"-"qualitative coding"-"semantic web"

https://www.sciencedirect.com/search?qs=%28ontology%20and%20%E2%80%9Csemantic%20annotation%E2%80%9D%20and%20%E2%80%9Cknowledge%20graphs%E2%80%9D%20and%20%E2%80%9Cqualitative%20analysis%E2%80%9D%20and%20%E2%80%9Cquantitative%22%20analysis%E2%80%9D%29%20or%20%28ontology%20and%20%E2%80%9Ccoding%E2%80%9D%20and%20%E2%80%9Cqualitative%20analysis%E2%80%9D%29

Krystyna Milian, Rinke Hoekstra, Anca Bucur, Annette ten Teije, Frank van Harmelen, John Paulissen,
Enhancing reuse of structured eligibility criteria and supporting their relaxation,
Journal of Biomedical Informatics,
Volume 56,
2015,
Pages 205-219,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2015.05.005.
(https://www.sciencedirect.com/science/article/pii/S1532046415000842)
Abstract: Patient recruitment is one of the most important barriers to successful completion of clinical trials and thus to obtaining evidence about new methods for prevention, diagnostics and treatment. The reason is that recruitment is effort consuming. It requires the identification of candidate patients for the trial (the population under study), and verifying for each patient whether the eligibility criteria are met. The work we describe in this paper aims to support the comparison of population under study in different trials, and the design of eligibility criteria for new trials. We do this by introducing structured eligibility criteria, that enhance reuse of criteria across trials. We developed a method that allows for automated structuring of criteria from text. Additionally, structured eligibility criteria allow us to propose suggestions for relaxation of criteria to remove potentially unnecessarily restrictive conditions. We thereby increase the recruitment potential and generalizability of a trial. Our method for automated structuring of criteria enables us to identify related conditions and to compare their restrictiveness. The comparison is based on the general meaning of criteria, comprised of commonly occurring contextual patterns, medical concepts and constraining values. These are automatically identified using our pattern detection algorithm, state of the art ontology annotators and semantic taggers. The comparison uses predefined relations between the patterns, concept equivalences defined in medical ontologies, and threshold values. The result is a library of structured eligibility criteria which can be browsed using fine grained queries. Furthermore, we developed visualizations for the library that enable intuitive navigation of relations between trials, criteria and concepts. These visualizations expose interesting co-occurrences and correlations, potentially enhancing meta-research. The method for criteria structuring processes only certain types of criteria, which results in low recall of the method (18%) but a high precision for the relations we identify between the criteria (94%). Analysis of the approach from the medical perspective revealed that the approach can be beneficial for supporting trial design, though more research is needed.
Keywords: Formalizing eligibility criteria; Supporting trial design; Semantic annotation; Populating ontology from text; Data visualization

Mohamed Ali Hadj Taieb, Mohamed Ben Aouicha, Abdelmajid Ben Hamadou,
Ontology-based approach for measuring semantic similarity,
Engineering Applications of Artificial Intelligence,
Volume 36,
2014,
Pages 238-261,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2014.07.015.
(https://www.sciencedirect.com/science/article/pii/S0952197614001833)
Abstract: The challenge of measuring semantic similarity between words is to find a method that can simulate the thinking process of human. The use of computers to quantify and compare semantic similarities has become an important area of research in various fields, including artificial intelligence, knowledge management, information retrieval and natural language processing. The development of efficient measures for the computation of concept similarity is fundamental for computational semantics. Several computational measures rely on knowledge resources to quantify semantic similarity, such as the WordNet « is a » taxonomy. Several of these measures are based on taxonomical parameters to achieve the best expression possible for the semantics of content. This paper presents a new measure for quantifying the degree of the semantic similarity between concepts and words based on the WordNet hierarchy and using a number of topological parameters related to the “is a” taxonomy. Our proposal combines, in a complementary way, the hyponyms and depth parameters. This measure takes the problem of fine granularity into account. It is argued, however, that WordNet sense distinctions are highly fine-grained even for humans. We, therefore, propose a new method to quantify the hyponyms subgraph of a given concept based on depth distribution. Common nouns datasets (RG65, MC30 and AG203), medical terms dataset (MED38) and verbs dataset (YP130) formed by word pairs are used in the assessment. We start by calculating semantic similarities and then compute the correlation coefficient between human judgement and computational measures. The results demonstrate that, compared to other currently available computational methods, the measure presented in this study yields into better levels of performance. Compared to several measures, it shows good accuracy covering all the pairwises of the verbs dataset YP130.
Keywords: Semantic similarity; WordNet ontology; Taxonomic knowledge; Taxonomical parameters

Diego R. Mazzotti,
Landscape of biomedical informatics standards and terminologies for clinical sleep medicine research: A systematic review,
Sleep Medicine Reviews,
Volume 60,
2021,
101529,
ISSN 1087-0792,
https://doi.org/10.1016/j.smrv.2021.101529.
(https://www.sciencedirect.com/science/article/pii/S1087079221001143)
Abstract: Summary
A systematic literature review was conducted to understand the current landscape of standards and terminologies used in clinical sleep medicine. Literature search on PubMed, EMBASE, Medline and Web of Science was performed in March 2021 using terms related to sleep, terminologies, standards, harmonization, semantics, ontology, and electronic health records (EHR). Systematic review was carried out according to PRISMA. Among 128 included studies, 35 were eligible for review. Articles were broadly classified into six topics: standard terminology efforts, reporting standards, databases and resources, data integration efforts, EHR abstraction and standards for automated sleep scoring. This review highlights the progress and challenges related to establishing computable terminologies in sleep medicine, and identifies gaps, limitations and research opportunities related to data integration that could improve adoption of clinical research informatics in this field. There is a need for the systematic adoption of standardized terminologies in all areas of sleep medicine. Existing data aggregation resources could be leveraged to support the development of an integrated infrastructure and subsequent deployment in EHR systems within sleep centers. Ultimately, the adoption of standardized practices for documenting sleep disorders and related traits facilitates data sharing, thus accelerating discovery and clinical translation of informatics approaches applied to sleep medicine.
Keywords: Sleep; Standards; Terminologies; Harmonization; Clinical research informatics

Dewan M. Sarwar, David P. Nickerson,
CellML Model Discovery with the Physiome Model Repository,
Editor(s): Olaf Wolkenhauer,
Systems Medicine,
Academic Press,
2021,
Pages 354-361,
ISBN 9780128160787,
https://doi.org/10.1016/B978-0-12-801238-3.11681-2.
(https://www.sciencedirect.com/science/article/pii/B9780128012383116812)
Abstract: In this article we present a semantics-based model discovery tool that will enable scientists to discover relevant CellML models to utilize in their research. We describe relevant tools and technologies as well as the process of annotating models to include the biological semantics needed for this approach to work. In this example, we make use of the Physiome Model Repository to store the CellML models and the semantic annotations in a way that our tool is able to leverage in assisting in model discovery tasks.
Keywords: CellML; Protein ontology; Physiome model repository; Resource description framework; SBML; Virtual physiological human

Idoia Berges, Víctor Julio Ramírez-Durán, Arantza Illarramendi,
A Semantic Approach for Big Data Exploration in Industry 4.0,
Big Data Research,
Volume 25,
2021,
100222,
ISSN 2214-5796,
https://doi.org/10.1016/j.bdr.2021.100222.
(https://www.sciencedirect.com/science/article/pii/S2214579621000393)
Abstract: The growing trends in automation, Internet of Things, big data and cloud computing technologies have led to the fourth industrial revolution (Industry 4.0), where it is possible to visualize and identify patterns and insights, which results in a better understanding of the data and can improve the manufacturing process. However, many times, the task of data exploration results difficult for manufacturing experts because they might be interested in analyzing also data that does not appear in pre-designed visualizations and therefore they must be assisted by Information Technology experts. In this paper, we present a proposal materialized in a semantic-based visual query system developed for a real Industry 4.0 scenario that allows domain experts to explore and visualize data in a friendly way. The main novelty of the system is the combined use that it makes of captured data that are semantically annotated first, and a 2D customized digital representation of a machine that is also linked with semantic descriptions. Those descriptions are expressed using terms of an ontology, where, among others, the sensors that are used to capture indicators about the performance of a machine that belongs to a Industry 4.0 scenario have been modeled. Moreover, this semantic description allows to: formulate queries at a higher level of abstraction, provide customized graphical visualizations of the results based on the format and nature of the data, and download enriched data enabling further types of analysis.
Keywords: Data exploration; Industry 4.0; Ontologies

Qian Zhu, Robert R. Freimuth, Zonghui Lian, Scott Bauer, Jyotishman Pathak, Cui Tao, Matthew J. Durski, Christopher G. Chute,
Harmonization and semantic annotation of data dictionaries from the Pharmacogenomics Research Network: A case study,
Journal of Biomedical Informatics,
Volume 46, Issue 2,
2013,
Pages 286-293,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2012.11.004.
(https://www.sciencedirect.com/science/article/pii/S1532046412001785)
Abstract: The Pharmacogenomics Research Network (PGRN) is a collaborative partnership of research groups funded by NIH to discover and understand how genome contributes to an individual’s response to medication. Since traditional biomedical research studies and clinical trials are often conducted independently, common and standardized representations for data are seldom used. This leads to heterogeneity in data representation, which hinders data reuse, data integration and meta-analyses. This study demonstrates harmonization and semantic annotation work for pharmacogenomics data dictionaries collected from PGRN research groups. A semi-automated system was developed to support the harmonization/annotation process, which includes four individual steps, (1) pre-processing PGRN variables; (2) decomposing and normalizing variable descriptions; (3) semantically annotating words and phrases using controlled terminologies; (4) grouping PGRN variables into categories based on the annotation results and semantic types, for total 1514 PGRN variables. Our results demonstrate that there is a significant amount of variability in how pharmacogenomics data is represented and that additional standardization efforts are needed. This represents a critical first step toward identifying and creating data standards for pharmacogenomics studies.
Keywords: Data harmonization; Semantic annotation; Pharmacogenomics

Achille Souili, Denis Cavallucci, François Rousselot,
Natural Language Processing (NLP) – A Solution for Knowledge Extraction from Patent Unstructured Data,
Procedia Engineering,
Volume 131,
2015,
Pages 635-643,
ISSN 1877-7058,
https://doi.org/10.1016/j.proeng.2015.12.457.
(https://www.sciencedirect.com/science/article/pii/S1877705815043490)
Abstract: Patents are valuable source of knowledge and are extremely important for assisting engineers and decisions makers through the inventive process. This paper describes a new approach of automatic extraction of IDM (Inventive Design Method) related knowledge from patent documents. IDM derives from TRIZ, the theory of Inventive problem solving, which is largely based on patent's observation to theorize the act of inventing. Our method mainly consists in using natural language techniques (NLP) to match and extract knowledge relevant to IDM Ontology. The purpose of this paper is to investigate on the contribution of NLP techniques to effective knowledge extraction from patent documents. We propose in this paper to firstly report on progress made so far in data mining before describing our approach.
Keywords: TRIZ; Inventive Design; Text mining; Patent mining; Knowledge discovery ;

Élisabeth Ranisavljević, Florent Devin, Dominique Laffly, Yannick Le Nir,
Semantic orchestration of image processing services for environmental analysis,
ISPRS Journal of Photogrammetry and Remote Sensing,
Volume 83,
2013,
Pages 184-192,
ISSN 0924-2716,
https://doi.org/10.1016/j.isprsjprs.2013.06.006.
(https://www.sciencedirect.com/science/article/pii/S0924271613001536)
Abstract: In order to analyze environmental dynamics, a major process is the classification of the different phenomena of the site (e.g. ice and snow for a glacier). When using in situ pictures, this classification requires data pre-processing. Not all the pictures need the same sequence of processes depending on the disturbances. Until now, these sequences have been done manually, which restricts the processing of large amount of data. In this paper, we present how to realize a semantic orchestration to automate the sequencing for the analysis. It combines two advantages: solving the problem of the amount of processing, and diversifying the possibilities in the data processing. We define a BPEL description to express the sequences. This BPEL uses some web services to run the data processing. Each web service is semantically annotated using an ontology of image processing. The dynamic modification of the BPEL is done using SPARQL queries on these annotated web services. The results obtained by a prototype implementing this method validate the construction of the different workflows that can be applied to a large number of pictures.
Keywords: Environment; Image processing; Snow ice; Web service; Orchestration; Semantic

Delphine Battistelli, Cyril Bruneau, Valentina Dragos,
Building a formal model for hate detection in French corpora,
Procedia Computer Science,
Volume 176,
2020,
Pages 2358-2365,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2020.09.299.
(https://www.sciencedirect.com/science/article/pii/S1877050920322092)
Abstract: This paper investigates the development of a formal model in order to analyse online hate in French corpora. Relevant concepts are identified by exploiting several sources: the cognitive foundations of the appraisal theory, according to which people’s emotional response are based on their own evaluative judgments or appraisals of situations, events or objects; a linguistic model of how different kinds of modalities applied to predicative contents are expressed in textual data; several definitions of a hate speech. Based on those inputs, a formal model was developed to describe online hate speech. The model highlights different categories of hate targets and actions, and emphasizes the importance of context for online hate detection.
Keywords: online hate speech; ontology; semantics

Zheyuan Hu, Wenhao Zhao, Hui Xiong, Xu Zhang,
A hierarchical retrieval approach for automatically generating assembly instructions,
Journal of Manufacturing Systems,
Volume 68,
2023,
Pages 400-409,
ISSN 0278-6125,
https://doi.org/10.1016/j.jmsy.2023.05.002.
(https://www.sciencedirect.com/science/article/pii/S0278612523000766)
Abstract: Assembly instructions are critical for prompt fulfilment of assembly tasks. However, the design of such instructions is time-consuming and requires experience. Retrieve and reuse of previous cases shortens the design time and reduces design mistakes, whereas the traditional retrieval method encounters a bottleneck during encoding because the technical instructions include both structured and unstructured data. In this paper, we propose a hierarchical retrieval approach for automatic generation of assembly instructions based on previously used technical instruction cards. First, a case-based reasoning (CBR) method is employed to encode the assembly process and retrieve the suitable cases. Then, an improved weighted latent Dirichlet allocation text mining technique is applied to explore unstructured text topics and recommend the most optimal case. Finally, we utilize the proposed method to an automotive assembly process using data in 12,034 used instruction cards. The results demonstrate that technical instructions can be generated automatically for a specific topic using the proposed retrieval method. Compared to the traditional CBR method, the proposed hierarchical retrieval approach significantly improves the quality of new assembly instructions and the speed of generation.
Keywords: Text mining; Assembly instructions; Case-based reasoning; Weighted latent Dirichlet allocation

Martin Golebiewski,
Data Formats for Systems Biology and Quantitative Modeling,
Editor(s): Shoba Ranganathan, Michael Gribskov, Kenta Nakai, Christian Schönbach,
Encyclopedia of Bioinformatics and Computational Biology,
Academic Press,
2019,
Pages 884-893,
ISBN 9780128114322,
https://doi.org/10.1016/B978-0-12-809633-8.20471-8.
(https://www.sciencedirect.com/science/article/pii/B9780128096338204718)
Abstract: Data standards support the reliable exchange of information, the interoperability of tools, and the reproducibility of scientific results. In systems biology standards are agreed ways of structuring, describing, and associating models and data, as well as their respective parts, graphical visualization, and information about applied experimental or computational methods. Such standards also assist with describing how constituent parts interact together, or are linked, and how they are embedded in their environmental and experimental context. Here the focus will be on standards for formatting models and their content, and on metadata checklists and ontologies that support modeling.
Keywords: CellML; COMBINE; FAIR data; Modeling; NormSys registry; SBGN; SBML; SED-ML; Standards; Systems biology

Matej Petković, Gorjan Popovski, Barbara Koroušić Seljak, Dragi Kocev, Tome Eftimov,
DietHub: Dietary habits analysis through understanding the content of recipes,
Trends in Food Science & Technology,
Volume 107,
2021,
Pages 183-194,
ISSN 0924-2244,
https://doi.org/10.1016/j.tifs.2020.10.017.
(https://www.sciencedirect.com/science/article/pii/S0924224420306403)
Abstract: Background
Understanding the content of self-reported meals and online-published recipes is a basic requirement for further linking food and dietary concepts to heterogeneous health networks. Despite the huge amount of work that is done in the biomedical domain, the food and nutrition domains are relatively low-resourced. DietHub represents a step forward in food science & technology that requires knowledge from a broad spectrum of areas.
Scope and approach
DietHub is an AI workflow methodology that annotates online-published recipes or self-reported meals with the food concepts that are mentioned in them. The food semantic labels that are used are hierarchical food semantic tags from the Hansard taxonomy. DietHub overviews and exploits several state-of-the-art methods from two areas of AI: representation learning and predictive modelling. We evaluated DietHub by applying it on a corpus of online-published recipes of different styles, such as health, cooking and region. Once the selected recipes were annotated, we compared them considering their styles. The results show justifiable comparison of Mediterranean diet recipes with recipes from other diets. Key Findings and Conclusions: The experimental evaluation reveals that DietHub has high predictive power and correctly annotate the recipes with semantic tags. The analysis of the annotations shows that there is no statistically significant difference between Mediterranean diet and each of the diets: diabetic, weight loss, heart healthy recipes, low fat, low calorie, high fiber, and dairy free. All in all, the presented work shows that DietHub can be successfully used to analyze corpora of food-related textual documents and provide a deeper insight into the human dietary behaviour.
Keywords: food Semantic annotation; Dietary habits analysis; Hierarchical multi-label classification

Valentina Nejkovic, Nenad Petrovic, Milorad Tosic, Nenad Milosevic,
Semantic approach to RIoT autonomous robots mission coordination,
Robotics and Autonomous Systems,
Volume 126,
2020,
103438,
ISSN 0921-8890,
https://doi.org/10.1016/j.robot.2020.103438.
(https://www.sciencedirect.com/science/article/pii/S0921889019306414)
Abstract: Internet of Things (IoT) has recently become the key for innovation and progress in many industrial sectors and scientific areas. However, it brings many challenges and issues, such as growing number of connected devices, heterogeneity, amount of generated data, security and privacy issues, interoperability and many others. Since devices not only collect data, but also take actions that affect the environment, device coordination in the context of IoT systems is becoming more and more important, especially if the IoT convergence with robotics, known as “Internet of Robotic Things” (RIoT), is taken into consideration. In novel cyber–physical systems coordination is very important for situations when many devices working parallel have higher potential to achieve the given task more effectively, than a single device operating independently. RIoT experimentation testbeds facilitate development of such cyber–physical systems where devices need to be aware of the environment while interacting with other devices in order to achieve a common goal. In this paper, we propose a semantic-driven framework for automated autonomous robots coordination in the context of RIoT-based experimentation testbeds. Framework for automatic coordinated mission generation within the robotics experimentation platform testbed is evaluated. Results of evaluation are presented and discussed.
Keywords: Coordination; Autonomous robots; Internet of Things; Ontology; Robot sensing systems; Semantic technology; Testbeds

Blanca Vazquez, Alicia Martinez, Anna Perini, Hugo Estrada, Mirko Morandini,
Enriching Organizational Models through Semantic Annotation,
Procedia Technology,
Volume 7,
2013,
Pages 297-304,
ISSN 2212-0173,
https://doi.org/10.1016/j.protcy.2013.04.037.
(https://www.sciencedirect.com/science/article/pii/S2212017313000388)
Abstract: Semantic annotation of visual models is useful to provide a precise, formal meaning to model elements, thus making them more understandable to people, enabling a deeper analysis of requirements and automated reasoning. We present an approach for the enrichment of visual models of an organization, with annotations characterized by a semantics de- fined by an organized, structured source of knowledge. In order to carry out the semantic annotation a set of suggestions referring to the use of general and specific ontologies is presented. Moreover, we present a case study to validate the effectiveness of our approach.
Keywords: Semantic annotation; Visual models; Ontologies; Requirements engineering; OntoSem ontology; iStarML.

Ashnil Kumar, Shane Dyer, Jinman Kim, Changyang Li, Philip H.W. Leong, Michael Fulham, Dagan Feng,
Adapting content-based image retrieval techniques for the semantic annotation of medical images,
Computerized Medical Imaging and Graphics,
Volume 49,
2016,
Pages 37-45,
ISSN 0895-6111,
https://doi.org/10.1016/j.compmedimag.2016.01.001.
(https://www.sciencedirect.com/science/article/pii/S0895611116300015)
Abstract: The automatic annotation of medical images is a prerequisite for building comprehensive semantic archives that can be used to enhance evidence-based diagnosis, physician education, and biomedical research. Annotation also has important applications in the automatic generation of structured radiology reports. Much of the prior research work has focused on annotating images with properties such as the modality of the image, or the biological system or body region being imaged. However, many challenges remain for the annotation of high-level semantic content in medical images (e.g., presence of calcification, vessel obstruction, etc.) due to the difficulty in discovering relationships and associations between low-level image features and high-level semantic concepts. This difficulty is further compounded by the lack of labelled training data. In this paper, we present a method for the automatic semantic annotation of medical images that leverages techniques from content-based image retrieval (CBIR). CBIR is a well-established image search technology that uses quantifiable low-level image features to represent the high-level semantic content depicted in those images. Our method extends CBIR techniques to identify or retrieve a collection of labelled images that have similar low-level features and then uses this collection to determine the best high-level semantic annotations. We demonstrate our annotation method using retrieval via weighted nearest-neighbour retrieval and multi-class classification to show that our approach is viable regardless of the underlying retrieval strategy. We experimentally compared our method with several well-established baseline techniques (classification and regression) and showed that our method achieved the highest accuracy in the annotation of liver computed tomography (CT) images.
Keywords: Image annotation; Content-based image retrieval; Computed tomography; Liver; ImageCLEF

L. d’Orazio, A. Bartoli, A. Baetz, S. Beorchia, G. Calvary, Y. Chabane, F. Chadebecq, T. Collins, Y. Laurillau, L. Martins-Baltar, B. Mohamad, T. Ponchon, C. Rey, C. Tilmant, S. Torti,
Multimodal and multimedia image analysis and collaborative networking for digestive endoscopy,
IRBM,
Volume 35, Issue 2,
2014,
Pages 88-93,
ISSN 1959-0318,
https://doi.org/10.1016/j.irbm.2014.02.006.
(https://www.sciencedirect.com/science/article/pii/S1959031814000244)
Abstract: Objective
The ultimate goal of the Syseo project is to create a chain of collaborative processes to allow the hepato-gastroenterology endoscopy specialist to manage images easily.
Methods
A field study has been done to better understand and formalize practices and contexts of use. Based on these results, we have designed tools for gastroenterology, tackling several domains of computer science and reusing well-known format or concepts especially DICOM files, semantic retrieval and infocus-breakpoint.
Results
Syseo consists in four main components: (1) a data management system relying on the well-known standard DICOM format; (2) a polyp ontology and description logics to manage gastroenterological images; (3) software to estimate the size of a neoplasia from colonoscopic images and (4) pearly user interfaces to enhance collaboration.
Discussion
Preliminary results of Syseo are quite promising since the proposed solutions enable to efficiently store, annotate, retrieve medical data, while providing relatively accurate measuring tools for physicians and medical staff.

Leyla Jael Garcia Castro, Rafael Berlanga, Alexander Garcia,
In the pursuit of a semantic similarity metric based on UMLS annotations for articles in PubMed Central Open Access,
Journal of Biomedical Informatics,
Volume 57,
2015,
Pages 204-218,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2015.07.015.
(https://www.sciencedirect.com/science/article/pii/S1532046415001550)
Abstract: Motivation
Although full-text articles are provided by the publishers in electronic formats, it remains a challenge to find related work beyond the title and abstract context. Identifying related articles based on their abstract is indeed a good starting point; this process is straightforward and does not consume as many resources as full-text based similarity would require. However, further analyses may require in-depth understanding of the full content. Two articles with highly related abstracts can be substantially different regarding the full content. How similarity differs when considering title-and-abstract versus full-text and which semantic similarity metric provides better results when dealing with full-text articles are the main issues addressed in this manuscript.
Methods
We have benchmarked three similarity metrics – BM25, PMRA, and Cosine, in order to determine which one performs best when using concept-based annotations on full-text documents. We also evaluated variations in similarity values based on title-and-abstract against those relying on full-text. Our test dataset comprises the Genomics track article collection from the 2005 Text Retrieval Conference. Initially, we used an entity recognition software to semantically annotate titles and abstracts as well as full-text with concepts defined in the Unified Medical Language System (UMLS®). For each article, we created a document profile, i.e., a set of identified concepts, term frequency, and inverse document frequency; we then applied various similarity metrics to those document profiles. We considered correlation, precision, recall, and F1 in order to determine which similarity metric performs best with concept-based annotations. For those full-text articles available in PubMed Central Open Access (PMC-OA), we also performed dispersion analyses in order to understand how similarity varies when considering full-text articles.
Results
We have found that the PubMed Related Articles similarity metric is the most suitable for full-text articles annotated with UMLS concepts. For similarity values above 0.8, all metrics exhibited an F1 around 0.2 and a recall around 0.1; BM25 showed the highest precision close to 1; in all cases the concept-based metrics performed better than the word-stem-based one. Our experiments show that similarity values vary when considering only title-and-abstract versus full-text similarity. Therefore, analyses based on full-text become useful when a given research requires going beyond title and abstract, particularly regarding connectivity across articles.
Availability
Visualization available at ljgarcia.github.io/semsim.benchmark/, data available at http://dx.doi.org/10.5281/zenodo.13323.
Keywords: Semantic similarity; Scientific publications; Similarity metrics; Semantic annotations; Related articles

Thouraya Sakouhi, Jalel Akaichi,
Dynamic and multi-source semantic annotation of raw mobility data using geographic and social media data,
Pervasive and Mobile Computing,
Volume 71,
2021,
101310,
ISSN 1574-1192,
https://doi.org/10.1016/j.pmcj.2020.101310.
(https://www.sciencedirect.com/science/article/pii/S1574119220301413)
Abstract: Nowadays, positioning technologies have become widely available providing then large datasets of individuals’ mobility data. Actually, annotating raw traces with contextual information brings semantics to them and then provides a better understanding of people behavior. To do so, literature work explored novel techniques to enrich raw mobility data with contextual information using either geographic context represented by landmarks/points of interest or widely used social media feeds. Accordingly, in this work, a novel approach integrating three data sources: raw mobility data, geographic information and social media feeds for a two-fold trajectory semantic annotation process is presented. In a first step, structured trajectories are constructed using geographic information. Later, the former are annotated by event-related words grasped from social media. Indeed, combining both data sources could result in a more complete annotation of trajectories. The proposed approach is experimented and evaluated on datasets of tourists in Kyoto. Results showed that the proposed approach quantitatively performed well compared to previous work in terms of precision of annotation words that maintained ≃0.9 when recall reached 50%, while improving its quality by consolidating both sources of semantics.
Keywords: Mobility data; Trajectory; Semantic annotation; Activity recognition

Marlon Amaro Coelho Teixeira, Kele Teixeira Belloze, Maria Cláudia Cavalcanti, Floriano P. Silva-Junior,
Data mart construction based on semantic annotation of scientific articles: A case study for the prioritization of drug targets,
Computer Methods and Programs in Biomedicine,
Volume 157,
2018,
Pages 225-235,
ISSN 0169-2607,
https://doi.org/10.1016/j.cmpb.2018.01.010.
(https://www.sciencedirect.com/science/article/pii/S0169260717309252)
Abstract: Background and objectives
Semantic text annotation enables the association of semantic information (ontology concepts) to text expressions (terms), which are readable by software agents. In the scientific scenario, this is particularly useful because it reveals a lot of scientific discoveries that are hidden within academic articles. The Biomedical area has more than 300 ontologies, most of them composed of over 500 concepts. These ontologies can be used to annotate scientific papers and thus, facilitate data extraction. However, in the context of a scientific research, a simple keyword-based query using the interface of a digital scientific texts library can return more than a thousand hits. The analysis of such a large set of texts, annotated with such numerous and large ontologies, is not an easy task. Therefore, the main objective of this work is to provide a method that could facilitate this task.
Methods
This work describes a method called Text and Ontology ETL (TOETL), to build an analytical view over such texts. First, a corpus of selected papers is semantically annotated using distinct ontologies. Then, the annotation data is extracted, organized and aggregated into the dimensional schema of a data mart.
Results
Besides the TOETL method, this work illustrates its application through the development of the TaP DM (Target Prioritization data mart). This data mart has focus on the research of gene essentiality, a key concept to be considered when searching for genes showing potential as anti-infective drug targets.
Conclusions
This work reveals that the proposed approach is a relevant tool to support decision making in the prioritization of new drug targets, being more efficient than the keyword-based traditional tools.
Keywords: Semantic annotation; Decision support systems; Drug target prioritization

Mariel A. Ale, Carlos M. Toledo, Omar Chiotti, María R. Galli,
A conceptual model and technological support for organizational knowledge management,
Science of Computer Programming,
Volume 95, Part 1,
2014,
Pages 73-92,
ISSN 0167-6423,
https://doi.org/10.1016/j.scico.2013.12.012.
(https://www.sciencedirect.com/science/article/pii/S0167642314000100)
Abstract: Knowledge Management (KM) models proposed in the literature do not take into account all necessary aspects for effective knowledge management. First, to address this issue, this paper presents a set of requirements that any KM model or initiative should take into account to cover all aspects implied in knowing processes. These requirements were identified through a critical and evolutionary analysis of KM. Second; the paper presents a new distributed KM Conceptual Model whose building blocks are the knowledge activities involved in knowing processes. These activities are: knowledge creation, knowledge sharing, and knowledge representation and retrieval. This model provides a holistic view of KM whose purpose is helping managers understand the scope of this initiative, and supplying a guide for research and implementation in organizations. In this sense, the model presents KM as a highly social rather than technological process. Third; the paper briefly describes an architecture to provide a technological support for knowledge representation and retrieval activities of the proposed KM Conceptual Model. This architecture allows implementing a distributed organizational memory that helps to represent the knowledge context through an ontological model, providing a local perspective of each knowledge domain within the organization. Strategies for knowledge annotation, knowledge retrieval, and ontology evolution are briefly described and results of preliminary performance analysis are shown. Finally; based on the available literature, a comparative analysis of different KM models shows their adequacy for previously presented requirements.
Keywords: Knowledge management; Knowledge management model; Distributed organizational memory; Semantic information retrieval; Ontology evolution

Wiem Zaouga, Latifa Ben Arfa Rabai, Wafa Rashid Alalyani,
Towards an Ontology Based-Approach for Human Resource Management,
Procedia Computer Science,
Volume 151,
2019,
Pages 417-424,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2019.04.057.
(https://www.sciencedirect.com/science/article/pii/S1877050919305198)
Abstract: Human Resources (HRs) as one of the most valuable asset of any organizations play a crucial role in their success. Within the context of project management, HR Management (HRM) is perceived as an entire knowledge area with defined processes, Tools and Techniques (T&T) in PMBOK 5th Guide. Although this guide shows a strong focus on HRM, it does not illustrate, in a clear way, how to perform each process with the required competencies and the related T&T. Hence, by using only PMBOK processes, the project manager cannot be assisted to select the suitable team according to their skills, also the project team members are not able to interchange their competencies. To address these issues, we propose to use an ontological approach in order to build a common shared representation in the HRM domain. This approach fosters the interoperability among HRs as well as their efficient use of T&T; further it can provide whom using PMBOK a better understanding and guidance for managing HRs with evidence.
Keywords: Human Resource Ontology; Domain Ontology; Human Resource Management Processes; PMBOK

Mohamad Mehdi, Chitu Okoli, Mostafa Mesgari, Finn Årup Nielsen, Arto Lanamäki,
Excavating the mother lode of human-generated text: A systematic review of research that uses the wikipedia corpus,
Information Processing & Management,
Volume 53, Issue 2,
2017,
Pages 505-529,
ISSN 0306-4573,
https://doi.org/10.1016/j.ipm.2016.07.003.
(https://www.sciencedirect.com/science/article/pii/S0306457316303004)
Abstract: Although primarily an encyclopedia, Wikipedia’s expansive content provides a knowledge base that has been continuously exploited by researchers in a wide variety of domains. This article systematically reviews the scholarly studies that have used Wikipedia as a data source, and investigates the means by which Wikipedia has been employed in three main computer science research areas: information retrieval, natural language processing, and ontology building. We report and discuss the research trends of the identified and examined studies. We further identify and classify a list of tools that can be used to extract data from Wikipedia, and compile a list of currently available data sets extracted from Wikipedia.
Keywords: Information retrieval; Information extraction; Natural language processing; Ontologies; Wikipedia; Literature review

Yongsun Choi, Minh Duc Nguyen, Thomas N. Kerr,
Syntactic and semantic information extraction from NPP procedures utilizing natural language processing integrated with rules,
Nuclear Engineering and Technology,
Volume 53, Issue 3,
2021,
Pages 866-878,
ISSN 1738-5733,
https://doi.org/10.1016/j.net.2020.08.010.
(https://www.sciencedirect.com/science/article/pii/S1738573320308275)
Abstract: Procedures play a key role in ensuring safe operation at nuclear power plants (NPPs). Development and maintenance of a large number of procedures reflecting the best knowledge available in all relevant areas is a complex job. This paper introduces a newly developed methodology and the implemented software, called iExtractor, for the extraction of syntactic and semantic information from NPP procedures utilizing natural language processing (NLP)-based technologies. The steps of the iExtractor integrated with sets of rules and an ontology for NPPs are described in detail with examples. Case study results of the iExtractor applied to selected procedures of a U.S. commercial NPP are also introduced. It is shown that the iExtractor can provide overall comprehension of the analyzed procedures and indicate parts of procedures that need improvement. The rich information extracted from procedures could be further utilized as a basis for their enhanced management.
Keywords: Nuclear power plant; Procedure; Information extraction; Natural language processing; Rule; Ontology

Bill Karakostas, Zannis Kalamboukis,
API mashups: How well do they support the travellers’ information needs?,
Procedia Computer Science,
Volume 109,
2017,
Pages 204-209,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2017.05.326.
(https://www.sciencedirect.com/science/article/pii/S1877050917309900)
Abstract: Abstract:
This paper investigates the adequacy of API mashups for supporting the information requirements of travelers, prior, during and after the journey. We analyse the information requirements of travelers at the various stages of a multimodal journey and perform a gap analysis against the actual information supplied by travel APIs. We base the approach on the analysis of three publicly available travel related APIs. Finally, we propose how semantically annotating APIs can support intelligent travel assistants that address the information requirements of travelers.
Keywords: API; API composition (mashup); cognitive travel process; semantic annotation; travel API

Kehua Guo, Ruifang Zhang, Li Kuang,
TMR: Towards an efficient semantic-based heterogeneous transportation media big data retrieval,
Neurocomputing,
Volume 181,
2016,
Pages 122-131,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2015.06.101.
(https://www.sciencedirect.com/science/article/pii/S0925231215018445)
Abstract: In media retrieval system for intelligent transportation, media data variety and heterogeneity have been one of the most critical features. Documents with different formats may express similar semantic information, thus, searching documents reflecting users׳ intention has been a crucial and important task. For solving this problem, this paper proposes a novel semantic-based heterogeneous transportation media retrieval (TMR) approach to improve the performance. TMR supports the function of retrieving various media types such as image, video, audio and text by using a single media type. Firstly, semantic fields are extracted from the user annotating and automatic learning to express the users׳ intention. Secondly, ontology is used to represent the semantic fields of a media, and the ontology represented semantic information is saved together with the media document data. Thirdly, the semantic field adjustment process is described. Finally, fuzzy matching is employed to measure the similarity between the users׳ intention and media documents. For the returned results, we carry out the performance evaluation models in comparison with the existing approaches. Experimental result indicates the superiority of TMR in term of precision rate, computing speed, storage cost and user experience.
Keywords: Heterogeneous media retrieval; Ontology; Fuzzy matching; Information retrieval; Big data; Intelligent transportation

Yuanting Zheng, Neil D. Young, Jiangning Song, Bill C.H. Chang, Robin B. Gasser,
An informatic workflow for the enhanced annotation of excretory/secretory proteins of Haemonchus contortus,
Computational and Structural Biotechnology Journal,
Volume 21,
2023,
Pages 2696-2704,
ISSN 2001-0370,
https://doi.org/10.1016/j.csbj.2023.03.025.
(https://www.sciencedirect.com/science/article/pii/S2001037023001289)
Abstract: Major advances in genomic and associated technologies have demanded reliable bioinformatic tools and workflows for the annotation of genes and their products via comparative analyses using well-curated reference data sets, accessible in public repositories. However, the accurate in silico annotation of molecules (proteins) encoded in organisms (e.g., multicellular parasites) which are evolutionarily distant from those for which these extensive reference data sets are available, including invertebrate model organisms (e.g., Caenorhabditis elegans – free-living nematode, and Drosophila melanogaster – the vinegar fly) and vertebrate species (e.g., Homo sapiens and Mus musculus), remains a major challenge. Here, we constructed an informatic workflow for the enhanced annotation of biologically-important, excretory/secretory (ES) proteins (“secretome”) encoded in the genome of a parasitic roundworm, called Haemonchus contortus (commonly known as the barber’s pole worm). We critically evaluated the performance of five distinct methods, refined some of them, and then combined the use of all five methods to comprehensively annotate ES proteins, according to gene ontology, biological pathways and/or metabolic (enzymatic) processes. Then, using optimised parameter settings, we applied this workflow to comprehensively annotate 2591 of all 3353 proteins (77.3%) in the secretome of H. contortus. This result is a substantial improvement (10–25%) over previous annotations using individual, “off-the-shelf” algorithms and default settings, indicating the ready applicability of the present, refined workflow to gene/protein sequence data sets from a wide range of organisms in the Tree-of-Life.
Keywords: Informatics; Machine learning; Protein annotation; Genome; Proteome; Haemonchus contortus; Parasitic nematode; Excretory/secretory proteins

S.N. Kinawy, T.E. El-Diraby, H. Konomi,
Customizing information delivery to project stakeholders in the smart city,
Sustainable Cities and Society,
Volume 38,
2018,
Pages 286-300,
ISSN 2210-6707,
https://doi.org/10.1016/j.scs.2017.12.012.
(https://www.sciencedirect.com/science/article/pii/S2210670717309800)
Abstract: In the smart city, citizens are integral participants in the decision making process. They possess equally important knowledge to that of professionals. Effectively informing them about new project features is the first step in engaging them in the decision making and harnessing their knowledge. However, given the complexity and diversity of project information, citizens could face an information overload. Our objective is to support the delivery of the right information to the right person; and doing so in an adaptive manner that recognizes the needs of local context. We have developed a system that allows users to profile their information needs based on an ontology of user communications. Recommender algorithms are then used to match user profile to the most relevant knowledge items, such as documents, web pages, and tagged videos or images. Users who wish to rate or tag documents can do so through using concepts from the ontology or free text. If free text is used, semantic analysis is conducted to extract relevant tags. Tags are then fed-back into the recommender system to enhance its accuracy. Capturing community tags provides a good opportunity to use crowd input to contextualise the matching algorithms.
Keywords: Project communication system; Co-creation; Community engagement; Crowdsourcing; Ontology; Recommender systems

Camille Kurtz, Christopher F. Beaulieu, Sandy Napel, Daniel L. Rubin,
A hierarchical knowledge-based approach for retrieving similar medical images described with semantic annotations,
Journal of Biomedical Informatics,
Volume 49,
2014,
Pages 227-244,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2014.02.018.
(https://www.sciencedirect.com/science/article/pii/S1532046414000574)
Abstract: Computer-assisted image retrieval applications could assist radiologist interpretations by identifying similar images in large archives as a means to providing decision support. However, the semantic gap between low-level image features and their high level semantics may impair the system performances. Indeed, it can be challenging to comprehensively characterize the images using low-level imaging features to fully capture the visual appearance of diseases on images, and recently the use of semantic terms has been advocated to provide semantic descriptions of the visual contents of images. However, most of the existing image retrieval strategies do not consider the intrinsic properties of these terms during the comparison of the images beyond treating them as simple binary (presence/absence) features. We propose a new framework that includes semantic features in images and that enables retrieval of similar images in large databases based on their semantic relations. It is based on two main steps: (1) annotation of the images with semantic terms extracted from an ontology, and (2) evaluation of the similarity of image pairs by computing the similarity between the terms using the Hierarchical Semantic-Based Distance (HSBD) coupled to an ontological measure. The combination of these two steps provides a means of capturing the semantic correlations among the terms used to characterize the images that can be considered as a potential solution to deal with the semantic gap problem. We validate this approach in the context of the retrieval and the classification of 2D regions of interest (ROIs) extracted from computed tomographic (CT) images of the liver. Under this framework, retrieval accuracy of more than 0.96 was obtained on a 30-images dataset using the Normalized Discounted Cumulative Gain (NDCG) index that is a standard technique used to measure the effectiveness of information retrieval algorithms when a separate reference standard is available. Classification results of more than 95% were obtained on a 77-images dataset. For comparison purpose, the use of the Earth Mover’s Distance (EMD), which is an alternative distance metric that considers all the existing relations among the terms, led to results retrieval accuracy of 0.95 and classification results of 93% with a higher computational cost. The results provided by the presented framework are competitive with the state-of-the-art and emphasize the usefulness of the proposed methodology for radiology image retrieval and classification.
Keywords: Image retrieval; Semantic image annotation; Semantic-based distances; Ontologies; Computed tomographic (CT) images; Liver lesions

Mariam Bouchakwa, Yassine Ayadi, Ikram Amous,
An ambiguous tag-based query reformulation technique for an effective semantic-based social image research,
Procedia Computer Science,
Volume 176,
2020,
Pages 508-520,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2020.08.053.
(https://www.sciencedirect.com/science/article/pii/S1877050920318779)
Abstract: The Tag-based Document Retrieval technique was adopted for long time as an intuitive way to search for images shared on social networks. Nevertheless, the tag-based queries are often too ambiguous, and consequently they do not constitute an efficient solution for retrieving the most relevant images that meet the users’ needs. As an alternative, the Semantic-based Social Image Retrieval technique has emerged. The policy of this technique consists in retrieving the relevant images covering as much possible the topics that a given ambiguous query may have. In this paper, we propose a novel technique at the ambiguous query pre-processing level, which aims at moving from an ambiguous tag-based query towards a semantic-based one, by relying on a set of predefined ontological semantic rules. Thorough experiments using 8 ambiguous queries over a collection of 25.000 socio-tagged images shared on Flickr service prove the effectiveness of our technique.
Keywords: social image research; ambiguous tag-based query; semantic-based query; semantic rules; semantic reformulation; search result diversification

Greta Adamo, Chiara Di Francescomarino, Chiara Ghidini, Fabrizio Maria Maggi,
Beyond arrows in process models: A user study on activity dependences and their rationales,
Information Systems,
Volume 100,
2021,
101762,
ISSN 0306-4379,
https://doi.org/10.1016/j.is.2021.101762.
(https://www.sciencedirect.com/science/article/pii/S0306437921000260)
Abstract: Despite the number and variety of business process modelling languages and notations available in the Business Process Management field, all of them mainly focus on a single type of relationship holding between business process activities, namely the activity execution order within the control flow. However, other types of relationships may hold between activities (e.g., co-occurrence or causal constraints) and the motivation behind these relationships can also be different (e.g., a norm or an ontological law-of-nature). In this paper, we focus on one type of these activity relationships whose semantics goes beyond the semantics of arrows in traditional business process modelling languages, i.e., on the so called occurrence dependences. In particular, we aim at evaluating whether making these occurrence dependences explicit in business process models could support business process modellers and analysts in their tasks. To this aim, we propose a notation for representing the occurrence dependences and their rationale, and carry out an empirical study with human subjects for evaluating their support in comprehension and redesign tasks; in addition, we qualitatively investigate the effort required for enriching business process models with these dependences.
Keywords: Empirical study with human subjects; Business process activity dependences; Business process modelling; Business process re-design; Business process understandability

John Cuzzola, Jelena Jovanović, Ebrahim Bagheri,
RysannMD: A biomedical semantic annotator balancing speed and accuracy,
Journal of Biomedical Informatics,
Volume 71,
2017,
Pages 91-109,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2017.05.016.
(https://www.sciencedirect.com/science/article/pii/S1532046417301119)
Abstract: Recently, both researchers and practitioners have explored the possibility of semantically annotating large and continuously evolving collections of biomedical texts such as research papers, medical reports, and physician notes in order to enable their efficient and effective management and use in clinical practice or research laboratories. Such annotations can be automatically generated by biomedical semantic annotators – tools that are specifically designed for detecting and disambiguating biomedical concepts mentioned in text. The biomedical community has already presented several solid automated semantic annotators. However, the existing tools are either strong in their disambiguation capacity, i.e., the ability to identify the correct biomedical concept for a given piece of text among several candidate concepts, or they excel in their processing time, i.e., work very efficiently, but none of the semantic annotation tools reported in the literature has both of these qualities. In this paper, we present RysannMD (Ryerson Semantic Annotator for Medical Domain), a biomedical semantic annotation tool that strikes a balance between processing time and performance while disambiguating biomedical terms. In other words, RysannMD provides reasonable disambiguation performance when choosing the right sense for a biomedical term in a given context, and does that in a reasonable time. To examine how RysannMD stands with respect to the state of the art biomedical semantic annotators, we have conducted a series of experiments using standard benchmarking corpora, including both gold and silver standards, and four modern biomedical semantic annotators, namely cTAKES, MetaMap, NOBLE Coder, and Neji. The annotators were compared with respect to the quality of the produced annotations measured against gold and silver standards using precision, recall, and F1 measure and speed, i.e., processing time. In the experiments, RysannMD achieved the best median F1 measure across the benchmarking corpora, independent of the standard used (silver/gold), biomedical subdomain, and document size. In terms of the annotation speed, RysannMD scored the second best median processing time across all the experiments. The obtained results indicate that RysannMD offers the best performance among the examined semantic annotators when both quality of annotation and speed are considered simultaneously.
Keywords: Automated semantic annotation; Entity linking; UMLS metathesaurus; Biomedical ontologies; Natural language processing; Medical terminology

Fan Zhang, Yawei Zhang, Tingting Hou, Fangtao Ren, Xi Liu, Runan Zhao, Xinhong Zhang,
Screening of genes related to breast cancer prognosis based on the DO-UniBIC method,
The American Journal of the Medical Sciences,
Volume 364, Issue 3,
2022,
Pages 333-342,
ISSN 0002-9629,
https://doi.org/10.1016/j.amjms.2022.04.022.
(https://www.sciencedirect.com/science/article/pii/S0002962922001872)
Abstract: Background
Early screening is the most effective way to control breast cancer. Due to the lack of accurate biomarkers, early diagnosis of breast cancer is still very difficult. Therefore, it is necessary to discover new candidate genes of breast cancer and improve the early diagnosis and prognosis.
Methods
A DO-UniBIC gene screening method was proposed. First, Disease Ontology (DO) analysis was used to screen out breast cancer related genes from differentially expressed genes, and then the UniBIC algorithm was used to find all gene clusters with the same changing trend based on the longest common subsequence. In addition, an eight-gene prognostic model was constructed to assess the prognostic risk of breast cancer patients.
Results
The prognostic analysis of the candidate genomes based on multivariate Cox proportional regression model revealed eight genes that were significantly related to prognosis. The eight genes were ACSL1, CD24, EMP1, JPH3, CAMK4, JUN, S100B and TP53AIP1. Among them, ACSL1 was a new potential breast cancer related gene screened by the DO-UniBIC method.
Conclusions
More comprehensive cancer-related genes can be screened based on the DO-UniBIC method, which can be used as the candidate gene set for prognostic analysis.
Keywords: Breast cancer; Prognosis; Differentially expressed gene; DO-UniBIC

Mounira Harzallah, Giuseppe Berio,
A Unified Framework for Semantic Comparison of Objects: Extension to Semantic Graphcomparison,
Procedia Computer Science,
Volume 60,
2015,
Pages 547-556,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2015.08.177.
(https://www.sciencedirect.com/science/article/pii/S1877050915023042)
Abstract: Comparing objects described (or annotated) with an ontology is quite important in several application domains. In our previous work we have shown that when objects are annotated with single concepts or sets of concepts, most of the semantic measures found in literature can be rewritten by using a unified form and the notion of approximated information content. In this paper, we argue how this unified form and the notion of approximated information content can be extended to semantic graphs, being the latter resulting from objects annotated with graphs of concepts connected via relationships found in the ontology. We show that open issues and distortions found in the relevant state of the art are put under control. The resulting unified framework (covering all types of annotations, from single concepts to graphs of concepts) is therefore relevant for practicing, in a unique environment, several semantic measures, as required for building a new one or selecting a measure for given application objectives.
Keywords: Semantic measures for comparing graphs; Semantic common graph; Ontology; Information content

Clément Guérin, Christophe Rigaud, Karell Bertet, Arnaud Revel,
An ontology-based framework for the automated analysis and interpretation of comic books’ images,
Information Sciences,
Volume 378,
2017,
Pages 109-130,
ISSN 0020-0255,
https://doi.org/10.1016/j.ins.2016.10.032.
(https://www.sciencedirect.com/science/article/pii/S0020025516313688)
Abstract: Since the beginning of the twenty-first century, the cultural industry has been through a massive and historical mutation induced by the rise of digital technologies. The comic books industry keeps looking for the right solution and has not yet produced anything as convincing as the music or movie have. A lot of energy has been spent to transfer printed material to digital supports so far. The specificities of those supports are not always exploited at the best of their capabilities, while they could potentially be used to create new reading conventions. In spite of the needs induced by the large amount of data created since the beginning of the comics history, content indexing has been left behind. It is indeed quite a challenge to index such a composition of textual and visual information. While a growing number of researchers are working on comic books’ image analysis from a low-level point of view, only a few are tackling the issue of representing the content at a high semantic level. We propose in this article a framework to handle the content of a comic book, to support the automatic extraction of its visual components and to formalize the semantic of the domain’s codes. We tested our framework over two applications: 1) the unsupervised content discovery of comic books’ images, 2) its capabilities to handle complex layouts and to produce a respectful browsing experience to the digital comics reader.
Keywords: Comic books; Images; Complex data; Knowledge representation; Ontologies; Spatial reasoning

Metta Santiputri, Aditya K. Ghose, Hoa Khanh Dam,
Mining task post-conditions: Automating the acquisition of process semantics,
Data & Knowledge Engineering,
Volume 109,
2017,
Pages 112-125,
ISSN 0169-023X,
https://doi.org/10.1016/j.datak.2017.03.007.
(https://www.sciencedirect.com/science/article/pii/S0169023X17301106)
Abstract: Semantic annotation of business process model in the business process designs has been addressed in a large and growing body of work, but these annotations can be difficult and expensive to acquire. This paper presents a data-driven approach to mining and validating these annotations (and specifically context-independent semantic annotations). We leverage event objects in process execution histories which describe both activity execution events (typically represented as process events) and state update events (represented as object state transition events). We present an empirical evaluation, which suggests that the approach provides generally reliable results.
Keywords: Business process semantics; Mining post-conditions; Semantic annotation

Ye Tian, Jingbei Zhang,
Employment discrimination analysis of Library and Information Science based on entity recognition,
The Journal of Academic Librarianship,
Volume 47, Issue 2,
2021,
102325,
ISSN 0099-1333,
https://doi.org/10.1016/j.acalib.2021.102325.
(https://www.sciencedirect.com/science/article/pii/S0099133321000161)
Abstract: The main purpose of this research is to provide an aggregated description of current employment discrimination in Library and Information Science (LIS) job market in Mainland China utilizing entity recognition approach. Specifically, a recruitment corpus with ontology-driven rules is firstly built. Then, the Bi-LSTM-CRF model on an annotated subset of the corpus is trained and verified by the rest of the corpus. Further, a quantitative statistics of the discrimination (via entities annotations) and an aggregation of the prediction of annual demand for jobs were conducted. Finally, we evaluate our approach by collecting 5297 LIS job advertisements in the public sector from 2015 to 2019 and conclude the result that average F1 of the entity recognition on 520 posts with 5411 entities is up to 91.06%. We statistically find that there exists serious institutional and employer discrimination ranging from political status (22.5%), age (15.4%), household registration (14.0%), to educational background (13.8%), etc. To our best knowledge, this is the first study investigating employment discrimination in the field of LIS in mainland China.
Keywords: Employment discrimination; Named entity recognition; Mainland China; Library and Information Sciences

Julius Köpke,
Annotation paths for matching XML-Schemas,
Data & Knowledge Engineering,
Volume 122,
2019,
Pages 25-54,
ISSN 0169-023X,
https://doi.org/10.1016/j.datak.2017.12.002.
(https://www.sciencedirect.com/science/article/pii/S0169023X17300654)
Abstract: Annotation paths are a technique for the semantic annotation of XML-Schemas. The design rationale was to develop an embedded annotation method on top of SAWSDL which is fully declarative, easily applicable and still provides the proper expressiveness for high-quality logic-based schema matching. Annotation paths capture significantly more semantics than plain model references, the declarative annotation method of the W3C standard SAWSDL. While the concept of annotation paths was introduced in earlier works, we provide a new formalization of their structure and based thereon define their semantics and introduce matching methods to derive simple and complex value correspondences. Such correspondences can be used for the generation of executable schema mappings using state of the art mapping tools. We provide a comprehensive evaluation of our annotation method and the proposed matching algorithms using real-world schemas and reference ontologies and demonstrate the feasibility of generating executable mappings using a state of the art mapping system. Our evaluations show that our annotation-based matcher achieves outstanding matching quality (avg. f-measure between 0.98 and 1.0).
Keywords: Semantic annotation; Schema matching; Schema mapping; Document transformations; SAWSDL; XML; Interoperability

Livio Robaldo, Luigi Di Caro,
OpinionMining-ML,
Computer Standards & Interfaces,
Volume 35, Issue 5,
2013,
Pages 454-469,
ISSN 0920-5489,
https://doi.org/10.1016/j.csi.2012.10.004.
(https://www.sciencedirect.com/science/article/pii/S0920548912001225)
Abstract: In this paper we propose OpinionMining-ML, a new XML-based formalism for tagging textual expressions conveying opinions on objects that are considered relevant in the state of affairs. The need of such a formalism is motivated by the lack of standards for Opinion Mining (a.k.a. Sentiment Analysis) that obey to certain requirements of efficiency, ease of manual annotation, scalability, and, most of all, that aim at satisfying the real goal of Sentiment Analysis applications. Opinion Mining is an Information Retrieval task, so that its output should be designed for being usable and fruitful from the perspective of a search engine. Our contribution is twofold. First, we present a standard methodology for the annotation of affective statements in text that is strictly independent from any application domain. The second and orthogonal part of the approach regards instead the domain-specific adaptation that relies on the use of an ontology of support, that is domain-dependent by definition. We finally evaluate our proposal by means of fine-grained analyses of the disagreement between different annotators.
Keywords: Opinion mining; Sentiment analysis

Cristine Griffo, João Paulo A. Almeida, Giancarlo Guizzardi, Julio Cesar Nardi,
Service contract modeling in Enterprise Architecture: An ontology-based approach,
Information Systems,
Volume 101,
2021,
101454,
ISSN 0306-4379,
https://doi.org/10.1016/j.is.2019.101454.
(https://www.sciencedirect.com/science/article/pii/S030643791930506X)
Abstract: Service contracts bind parties legally, regulating their behavior in the scope of a (business) service relationship. Given that there are legal consequences attached to service contracts, understanding the elements of a contract is key to managing services in an enterprise. After all, provisions in a service contract and in legislation establish obligations and rights for service providers and customers that must be respected in service delivery. The importance of service contracts to service provisioning in an enterprise has motivated us to investigate their representation in enterprise models. We have observed that approaches fall into two extremes of a spectrum. Some approaches, such as ArchiMate, offer an opaque “contract” construct, not revealing the rights and obligations in the scope of the governed service relationship. Other approaches, under the umbrella term “contract languages”, are devoted exactly to the formal representation of the contents of contracts. Despite the applications of contract languages, they operate at a level of detail that does not match that of enterprise architecture models. In this paper, we explore and bridge the gap between these two extremes. We address the representation of service contract elements with a systematic approach: we first propose a well-founded service contract ontology, and then extend the ArchiMate language to reflect the elements of the service contract ontology. The applicability of the proposed extension is assessed in the representation of a real-world cloud service contract.
Keywords: Legal contracts; Service modeling; Enterprise architecture; Service contract ontology; ArchiMate

Valentina Dragos, Sylvain Gatepaille,
On-the-fly integration of soft and sensor data for enhanced situation assessment,
Procedia Computer Science,
Volume 112,
2017,
Pages 1263-1272,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2017.08.081.
(https://www.sciencedirect.com/science/article/pii/S1877050917314254)
Abstract: Situation assessment is at the core of many critical tasks in the civilian and military domains: border monitoring, surveillance of areas and facilities, entity tracking and identification, all require accurate and up-to-day descriptions of the course of events. For all those applications, situations to be built are complex, dynamic and uncertain and their assessment is based on the integration of diverse sources, including sensors and their row values, images, observations, tactical information and knowledge expressed by domain experts or synthesized through discovery techniques. This paper presents a method to combine soft and sensor data to create enhanced situation assessment for a track-and-detect application. First we create a situation of entities and relationships by using only hard data provided by sensors and then we enrich this situation thanks to soft data, in the form of succinct or more complex observation reports. The system relies on semantic mediation to combine observations and sensor data by using ontologies as a common ground creating a bridge between two complementary yet incomplete representations of the world. The result is an augmented situation, having more precise, accurate or complete descriptions of entities and which is easier to analyze. This enhanced assessment allows for the situation to be understood and processed in a meaningful way by decision makers.
Keywords: Situation assessment; heterogeneous fusion; soft data; sensor data; semantic mediation

Nai-Wen Chi, Yu-Huei Jin, Shang-Hsien Hsieh,
Developing base domain ontology from a reference collection to aid information retrieval,
Automation in Construction,
Volume 100,
2019,
Pages 180-189,
ISSN 0926-5805,
https://doi.org/10.1016/j.autcon.2019.01.001.
(https://www.sciencedirect.com/science/article/pii/S0926580517310713)
Abstract: Information Retrieval (IR) is a common technique used to manage a growing technical document collection. Owing to the complexity of technical documents, the direct application of IR often leads to unsatisfactory results. Therefore, many semantic approaches, such as ontology, are applied to enhance IR performance. However, ontology development is often a labor-intensive process and the availability of ontologies significantly influences the applicability of IR. Consequently, many efforts are dedicated to the automation of the ontology development process for reducing the human labors. In addition, reference collections, which are developed as the golden standards to evaluate IR performance in many IR research, can be regarded as an available resource for ontology development. To ease domain ontology development for supporting IR, this research proposes a semi-automated approach to develop a base domain ontology from a reference collection. This research also validates the base ontology on an Earthquake Engineering reference collection, called the NCREE (National Center for Research on Earthquake Engineering) collection. The results reveal that the human workload of the proposed approach is affordable. Furthermore, the base domain ontology can help achieve a satisfactory IR performance.
Keywords: Ontology; Information retrieval; Earthquake engineering

Marco Masseroli,
Integrative Bioinformatics,
Editor(s): Shoba Ranganathan, Michael Gribskov, Kenta Nakai, Christian Schönbach,
Encyclopedia of Bioinformatics and Computational Biology,
Academic Press,
2019,
Pages 1092-1098,
ISBN 9780128114322,
https://doi.org/10.1016/B978-0-12-809633-8.20388-9.
(https://www.sciencedirect.com/science/article/pii/B9780128096338203889)
Abstract: The increasingly reliable and affordable high-throughput production of many different types of biomolecular data, and the development of pipelines for their automatic processing and annotation, have further stressed the need for integrative structures able to simplify accessibility and decrease redundancy and variety of semantic representations of life science data. Integrative bioinformatics addresses these aspects taking advantage of several different approaches and implementations proposed to integrate distributed heterogeneous data, with notable applications in the biological domain; they include information linkage, federated databases, multi-databases, mediator-based solutions, and data warehousing. Additionally, ontology-based, statistical and network-based data integration can also be effectively used for better biologically-driven data integration.
Keywords: Data integration; Data warehousing; Federated databases; Information linkage; Mediator-based integration; Multi-databases; Network-based integration; Semantic integration

Camille Kurtz, Adrien Depeursinge, Sandy Napel, Christopher F. Beaulieu, Daniel L. Rubin,
On combining image-based and ontological semantic dissimilarities for medical image retrieval applications,
Medical Image Analysis,
Volume 18, Issue 7,
2014,
Pages 1082-1100,
ISSN 1361-8415,
https://doi.org/10.1016/j.media.2014.06.009.
(https://www.sciencedirect.com/science/article/pii/S1361841514001030)
Abstract: Computer-assisted image retrieval applications can assist radiologists by identifying similar images in archives as a means to providing decision support. In the classical case, images are described using low-level features extracted from their contents, and an appropriate distance is used to find the best matches in the feature space. However, using low-level image features to fully capture the visual appearance of diseases is challenging and the semantic gap between these features and the high-level visual concepts in radiology may impair the system performance. To deal with this issue, the use of semantic terms to provide high-level descriptions of radiological image contents has recently been advocated. Nevertheless, most of the existing semantic image retrieval strategies are limited by two factors: they require manual annotation of the images using semantic terms and they ignore the intrinsic visual and semantic relationships between these annotations during the comparison of the images. Based on these considerations, we propose an image retrieval framework based on semantic features that relies on two main strategies: (1) automatic “soft” prediction of ontological terms that describe the image contents from multi-scale Riesz wavelets and (2) retrieval of similar images by evaluating the similarity between their annotations using a new term dissimilarity measure, which takes into account both image-based and ontological term relations. The combination of these strategies provides a means of accurately retrieving similar images in databases based on image annotations and can be considered as a potential solution to the semantic gap problem. We validated this approach in the context of the retrieval of liver lesions from computed tomographic (CT) images and annotated with semantic terms of the RadLex ontology. The relevance of the retrieval results was assessed using two protocols: evaluation relative to a dissimilarity reference standard defined for pairs of images on a 25-images dataset, and evaluation relative to the diagnoses of the retrieved images on a 72-images dataset. A normalized discounted cumulative gain (NDCG) score of more than 0.92 was obtained with the first protocol, while AUC scores of more than 0.77 were obtained with the second protocol. This automatical approach could provide real-time decision support to radiologists by showing them similar images with associated diagnoses and, where available, responses to therapies.
Keywords: Image retrieval; Riesz wavelets; Image annotation; Semantic dissimilarities; Computed tomographic (CT) images

David Werner, Christophe Cruz,
Precision Difference Management using a Common Sub-vector to Extend the Extended VSM Method,
Procedia Computer Science,
Volume 18,
2013,
Pages 1179-1188,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2013.05.284.
(https://www.sciencedirect.com/science/article/pii/S1877050913004274)
Abstract: Contractors, commercial and business decision-makers need economical information to drive their decisions. The production and distribution of a press review about French regional economic actors represents a prospecting tool on partners and competitors for the businessman. Our goal is to propose a customized review for each user, thus reducing the overload of useless information. Some systems for recommending news items already exist. The usefulness of external knowledge to improve the process has already been explained in information retrieval. The system's knowledge base in- cludes the domain knowledge used during the recommendation process. Our recommender system architecture is standard, but during the indexing task, the representations of content of each article and interests of users’ profiles created are based on this domain knowledge. Articles and Profiles are semantically defined in the Knowledge base via concepts, instances and relations. This paper deals with the relevance measure, a critical sub-task in recommendation systems and relationships between relevance and similarity concepts. The Vector Space Model is a well-known model used for relevance ranking. The problematic exposed here is the utilization of the standard VSM method with our indexing method.
Keywords: recommender system; news; domain ontology; ontologies; knowledge base; indexing; recommendation; vector space model

Davide Russo, Paolo Carrara, Giancarlo Facoetti,
Technical problem identification for supervised state of the art,
IFAC-PapersOnLine,
Volume 51, Issue 11,
2018,
Pages 1341-1346,
ISSN 2405-8963,
https://doi.org/10.1016/j.ifacol.2018.08.344.
(https://www.sciencedirect.com/science/article/pii/S240589631831468X)
Abstract: This paper presents a method for extracting technical information from a patent pool. It was designed to support the construction of the state of the art of a technology or a product/process by automatically identifying the list of problems that the inventors have faced. The method is based on a strict ontology, which defines what a patent problem is, and a set of IR strategies, which identify all alternative ways adopted in the pool to describe problems. More in detail, the authors propose a set of syntactic dependency patterns, and lemmas in order to extract only the sentences including the information dealing with problems. The output is a coarse list of technical problems, automatically extracted without the user being an expert in the problems of the sector. An exemplary case dealing with injection molding field is proposed.
Keywords: problem identification; information retrieval; syntactic dependency pattern; state of the art

Thiago S.R. Silva, Rodrigo M. Feitosa,
Using controlled vocabularies in anatomical terminology: A case study with Strumigenys (Hymenoptera: Formicidae),
Arthropod Structure & Development,
Volume 52,
2019,
100877,
ISSN 1467-8039,
https://doi.org/10.1016/j.asd.2019.100877.
(https://www.sciencedirect.com/science/article/pii/S1467803919300234)
Abstract: Morphological studies of insects can help us to understand the concomitant or sequential functionality of complex structures and may be used to hypothetize distinct levels of phylogenetic relationship among groups. Traditional morphological works, generally, have encompassed a set of elements, including descriptions of structures and their respective conditions, literature references and images, all combined in a single document. Fast forward to the digital era, it is now possible to release this information simultaneously but also independently as data sets linked to the original publication in an external environment. In order to link data from various fields of knowledge, disseminating morphological information in an open environment, it is important to use tools that enhance interoperability. For example, semantic annotations facilitate the dissemination and retrieval of phenotypic data in digital environments. The integration of semantic (i.e. web-based) components with anatomic treatments can be used to generate a traditional description in natural language along with a set of semantic annotations. The ant genus Strumigenys currently comprises about 840 described species distributed worldwide. In the Neotropical region, almost 200 species are currently known, but it is possible that much of the species' diversity there remains unexplored and undescribed. The morphological diversity in the genus is high, reflecting an extreme generic reclassification that occurred in the late 20th and early 21st centuries. Here we define the anatomical concepts in this highly diverse group of ants using semantic annotations to enrich the anatomical ontologies available online, focussing on the definition of terms through subjacent conceptualization.
Keywords: Morphology; Ants; Ontology; Semantic annotation; Terminology

Akhtar Zeb, Juha-Pekka Soininen, Nesli Sozer,
Data harmonisation as a key to enable digitalisation of the food sector: A review,
Food and Bioproducts Processing,
Volume 127,
2021,
Pages 360-370,
ISSN 0960-3085,
https://doi.org/10.1016/j.fbp.2021.02.005.
(https://www.sciencedirect.com/science/article/pii/S0960308521000237)
Abstract: The food sector is driven by a large number of actors, including primary producers, manufacturers, logistics providers, retailers, and consumers. At each phase of the food value chain, a significant amount of data is generated that provides important information to the agents involved in processing and flow of food products from farm to fork. Proper handling of food data has a crucial role in providing safe, quality and affordable products to the increasing world population. The independent production of food data, without following any specific guidelines and procedures, often results in inconsistent and incomparable datasets that cannot be directly utilised by multiple users. Data harmonisation means reconciling various types, levels and sources of data in formats that are compatible and comparable, and thus useful for better decision making. In the food sector, one way of performing data harmonisation is to represent food data according to reliable classification and description systems. Another approach towards harmonisation is to match various food concepts to the existing and widely used ontologies. Furthermore, harmonisation is facilitated by following specific guidelines and procedures during data collection processes. This study explores some of the most important tools, frameworks and methodologies for data harmonisation in the food sector.
Keywords: Food sector; Data harmonisation; Food ontology; Harmonisation challenges

Michael Villamizar, Olivier Canévet, Jean-Marc Odobez,
Multi-scale sequential network for semantic text segmentation and localization,
Pattern Recognition Letters,
Volume 129,
2020,
Pages 63-69,
ISSN 0167-8655,
https://doi.org/10.1016/j.patrec.2019.11.001.
(https://www.sciencedirect.com/science/article/pii/S0167865519303150)
Abstract: We present a novel method for semantic text document analysis which in addition to localizing text it labels the text in user-defined semantic categories. More precisely, it consists of a fully-convolutional and sequential network that we apply to the particular case of slide analysis to detect title, bullets and standard text. Our contributions are twofold: (1) A multi-scale network consisting of a series of stages that sequentially refine the prediction of text and semantic labels (text, title, bullet); (2) A synthetic database of slide images with text and semantic annotation that is used to train the network with abundant data and wide variability in text appearance, slide layouts, and noise such as compression artifacts. We evaluate our method on a collection of real slide images collected from multiple conferences, and show that it is able to localize text with an accuracy of 95%, and to classify titles and bullets with accuracies of 94% and 85% respectively. In addition, we show that our method is competitive on scene and born-digital image datasets, such as ICDAR 2011, where it achieves an accuracy of 91.1%.

David Sánchez, Montserrat Batet,
A semantic similarity method based on information content exploiting multiple ontologies,
Expert Systems with Applications,
Volume 40, Issue 4,
2013,
Pages 1393-1399,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2012.08.049.
(https://www.sciencedirect.com/science/article/pii/S095741741201010X)
Abstract: The quantification of the semantic similarity between terms is an important research area that configures a valuable tool for text understanding. Among the different paradigms used by related works to compute semantic similarity, in recent years, information theoretic approaches have shown promising results by computing the information content (IC) of concepts from the knowledge provided by ontologies. These approaches, however, are hampered by the coverage offered by the single input ontology. In this paper, we propose extending IC-based similarity measures by considering multiple ontologies in an integrated way. Several strategies are proposed according to which ontology the evaluated terms belong. Our proposal has been evaluated by means of a widely used benchmark of medical terms and MeSH and SNOMED CT as ontologies. Results show an improvement in the similarity assessment accuracy when multiple ontologies are considered.
Keywords: Information content; Semantic similarity; Ontologies; MeSH; SNOMED CT

Ida Sim, Samson W. Tu, Simona Carini, Harold P. Lehmann, Brad H. Pollock, Mor Peleg, Knut M. Wittkowski,
The Ontology of Clinical Research (OCRe): An informatics foundation for the science of clinical research,
Journal of Biomedical Informatics,
Volume 52,
2014,
Pages 78-91,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2013.11.002.
(https://www.sciencedirect.com/science/article/pii/S1532046413001792)
Abstract: To date, the scientific process for generating, interpreting, and applying knowledge has received less informatics attention than operational processes for conducting clinical studies. The activities of these scientific processes – the science of clinical research – are centered on the study protocol, which is the abstract representation of the scientific design of a clinical study. The Ontology of Clinical Research (OCRe) is an OWL 2 model of the entities and relationships of study design protocols for the purpose of computationally supporting the design and analysis of human studies. OCRe’s modeling is independent of any specific study design or clinical domain. It includes a study design typology and a specialized module called ERGO Annotation for capturing the meaning of eligibility criteria. In this paper, we describe the key informatics use cases of each phase of a study’s scientific lifecycle, present OCRe and the principles behind its modeling, and describe applications of OCRe and associated technologies to a range of clinical research use cases. OCRe captures the central semantics that underlies the scientific processes of clinical research and can serve as an informatics foundation for supporting the entire range of knowledge activities that constitute the science of clinical research.
Keywords: Ontology; Clinical research; Eligibility criteria; Clinical research informatics; Evidence-based medicine; Trial registration

Renato Fileto, Cleto May, Chiara Renso, Nikos Pelekis, Douglas Klein, Yannis Theodoridis,
The Baquara2 knowledge-based framework for semantic enrichment and analysis of movement data,
Data & Knowledge Engineering,
Volume 98,
2015,
Pages 104-122,
ISSN 0169-023X,
https://doi.org/10.1016/j.datak.2015.07.010.
(https://www.sciencedirect.com/science/article/pii/S0169023X15000555)
Abstract: The analysis of movements frequently requires more than just spatio-temporal data. Thus, despite recent progresses in trajectory handling, there is still a gap between movement data and formal semantics. This gap hinders movement analyses benefiting from available knowledge, with well-defined and widely agreed semantics. This article describes the Baquara2 framework to help narrow this gap by exploiting knowledge bases to semantically enrich and analyze movement data. It provides an ontological model for structuring and abstracting movement data in a multilevel hierarchy of progressively detailed movement segments that generalize concepts such as trajectories, stops, and moves. Baquara2 also includes a general customizable process to annotate movement data with concepts and objects described in ontologies and Linked Open Data (LOD) collections. The resulting semantic annotations enable queries for movement analyses based on application and domain specific knowledge. The proposed framework has been used in experiments to semantically enrich movement data collected from social media with geo-referenced LOD. The obtained results enable powerful queries that illustrate Baquara2 capabilities.
Keywords: Trajectories of moving objects; Social media; Ontologies; Linked open data; Semantic enrichment; Movement data analysis

Kehua Guo, Zhonghe Liang, Yayuan Tang, Tao Chi,
SOR: An optimized semantic ontology retrieval algorithm for heterogeneous multimedia big data,
Journal of Computational Science,
Volume 28,
2018,
Pages 455-465,
ISSN 1877-7503,
https://doi.org/10.1016/j.jocs.2017.02.005.
(https://www.sciencedirect.com/science/article/pii/S187775031730176X)
Abstract: Semantic information can express the search intentions of users, and this approach has become an important tool in the field of information retrieval. To support semantic-based multimedia retrieval in big data environment, this paper presents an optimized algorithm called semantic ontology retrieval (SOR), which uses big data processing tools to store and retrieve ontologies from heterogeneous multimedia data. First, the background of semantic extraction and ontology representation for multimedia big data are addressed. Second, the methodology of SOR, including the model definition and retrieval algorithm, is proposed. Third, for parallel processing SOR in distributed nodes, a MapReduce-based retrieval framework is presented. Finally, to achieve high retrieval precision and good user experience, a user feedback scheme is designed. The experimental results illustrate that SOR is suitable for semantic-based retrieval for heterogeneous multimedia big data.
Keywords: Ontology; Semantic-based retrieval; MapReduce; Multimedia big data; Big data retrieval; Retrieval algorithm

Ramesh Saha, Sayani Sen, Jayita Saha, Asmita Nandy, Suparna Biswas, Chandreyee Chowdhury,
Chapter 13 - Ontology-based intelligent decision support systems: A systematic approach,
Editor(s): Sarika Jain, Vishal Jain, Valentina Emilia Balas,
Web Semantics,
Academic Press,
2021,
Pages 177-193,
ISBN 9780128224687,
https://doi.org/10.1016/B978-0-12-822468-7.00005-5.
(https://www.sciencedirect.com/science/article/pii/B9780128224687000055)
Abstract: Intelligent and smart health monitoring is prevalent nowadays with the support of advancement in Internet of Things, machine learning, and ontology-based decision support systems. As a decision support system can analyze current patient vitals based on historical data, effective data representation from different data sources into a common knowledge base is essential. Web semantics has an increasingly important role to play here in terms of storing data following ontology for more usable knowledge repository. The findings of the decision support system can be fed to doctor’s smartphone as a message based on which the doctor may intervene in a specific scenario or may validate his own diagnosis with the one provided by the decision support system. As the comfort and convenience of the end-users of remote healthcare is important, in addition to quality of service, quality of experience is a matter of concern among other issues and challenges. This work emphasizes on several Machine Learning (ML) algorithms, ontology techniques to design and implement intelligent decision support system for effective healthcare support satisfying quality of service and quality of experience requirements.
Keywords: Ontology; decision support system; web semantics; machine learning; internet of things

Mehmed Yüksel, Thomas M. Roehr, Marko Jankovic, Wiebke Brinkmann, Frank Kirchner,
A reference implementation for knowledge assisted robot development for planetary and orbital robotics,
Acta Astronautica,
Volume 210,
2023,
Pages 197-211,
ISSN 0094-5765,
https://doi.org/10.1016/j.actaastro.2023.05.015.
(https://www.sciencedirect.com/science/article/pii/S0094576523002485)
Abstract: The use of modular, yet reusable and interchangeable components offers flexibility for system designers and thus leads to a higher degree of reconfigurability with cost effective, adaptable and scalable solutions. The development of a robotic system remains, however, a complex task with increasing demands depending on the targeted capabilities, functionalities and operational constraints of the robot. This requires expert knowledge in mechanical, electrical and software engineering, as well as tools that can collect, process and use relevant data. As a result, the amount of available and managed information is increasing, while its relevance, usability, and knowledge that can be extracted from it are consequently decreasing: a situation described as information overload paradox. The design of a modular system can be based on the use of standardized modules with universal interconnects, but it should be aligned with a strong formalism, e.g. the semantic representation of components, to allow for a better management of the overall complexity. An ontology-based knowledge representation, for instance, which provides domain and application specific knowledge for the design of robotic systems, can be used as a method of storing and sharing data in a uniform, machine-readable and standardized way. In this paper, we introduce the Knowledge-based Open Robot voCabulary as Utility Toolkit (korcut) as a core component to support the ontology-driven development of robotic systems as part of a reference implementation of the Q-Rock development cycle. Q-Rock gathers methods to assist users in designing robot configuration by: (a) automatically exploring robot capabilities based on robot hardware, (b) proposing robot designs to meet the user’s needs, (c) refining the proposed robot designs. We use korcut to improve the robot design process in orbital and planetary robotics from the requirements definition to the development phases of complex modular robotic structures (e.g. composition of an unmanned ground vehicle with robotic arms). This is implemented by embedding semantic component descriptions in a state-of-the-art open-source 3D modeling software. Furthermore, the korcut ontology family includes various sub-ontologies to address specific space-related tasks, such as the modeling of a Standard Interconnect (SI) for On Orbit Services (OOS) and Orbital Factory. The ontology design follows criteria that have been derived from a survey conducted as part of this work. This paper also covers an evaluation of its current applications from a methodological, knowledge representation, and software tool perspectives. In the application part, we introduce the use of the ontology to model a multi-functional SI that enables mechanical connections between various (modular) robotic components, as well as transfer of power and data. Finally, we provide a critical analysis of our work and outline its future work.
Keywords: Robotics; System design; Domain ontology; Space interface benchmark; In-orbit servicing; Planetary robotics

T. Lakshmi Surekha, N. Chandra Sekhara Rao, C.K. Shahnazeer, Syed Mufassir Yaseen, Surendra Kumar Shukla, Singh Bharat, Mahendran Arumugam,
Digital misinformation and fake news detection using WoT integration with Asian social networks fusion based feature extraction with text and image classification by machine learning architectures,
Theoretical Computer Science,
Volume 927,
2022,
Pages 1-14,
ISSN 0304-3975,
https://doi.org/10.1016/j.tcs.2022.05.017.
(https://www.sciencedirect.com/science/article/pii/S0304397522003322)
Abstract: Fake News contains potentially false data that is verified. Concerns about author accountability are being addressed by organizations such as the House of Commons and the Crosscheck initiative. So here proposed system designs the web classification based on ontology with fuzzy logic. The feature extraction has been carried out using deep learning techniques. The initial feature extraction has been done using AE-CNN (Autoencoder with convolution neural network). After the characteristics were extracted, high dimensionality indexing was used. Then based on indexing measures ranking process takes place to identify the feature whether it is image or text. After this training part the trained file has been annotated with the fuzzy-based ontology rules to detect online fake news web content. Then the decision is taken and finally genetic algorithm-based deep learning is carried out for web classification. The simulation results have been analyzed regarding accuracy, precision, recall and F-1 score.
Keywords: Ontology; Social media; AE-CNN; Fuzzy-based ontology; Web classification

Ujwala Bharambe, Surya S. Durbha,
Adaptive Pareto-based approach for geo-ontology matching,
Computers & Geosciences,
Volume 119,
2018,
Pages 92-108,
ISSN 0098-3004,
https://doi.org/10.1016/j.cageo.2018.06.008.
(https://www.sciencedirect.com/science/article/pii/S0098300417310671)

Ivano Gatto, Fabio Pittarello,
Creating Web3D educational stories from crowdsourced annotations,
Journal of Visual Languages & Computing,
Volume 25, Issue 6,
2014,
Pages 808-817,
ISSN 1045-926X,
https://doi.org/10.1016/j.jvlc.2014.10.010.
(https://www.sciencedirect.com/science/article/pii/S1045926X14001037)
Abstract: 3D representation and storytelling are two powerful means for educating students while engaging them. This paper describes a novel software architecture that couples them for creating engaging linear narrations that can be shared on the web. The architecture takes advantage of a previous work focused on the semantic annotation of 3D worlds that allows the users to go beyond the simple navigation of 3D objects, permitting to retrieve them with different search tools. The novelty of our architecture is that authors don’t have to build stories from scratch, but can take advantage of the crowdsourced effort of all the users accessing the platform, which can contribute providing assets or annotating objects. At our best knowledge no existing workflow includes the collaborative annotation of 3D worlds and the possibility to create stories on the top of it. Another feature of our design is the possibility for users to switch from and to any of the available activities during the same session. This integration offers the possibility to define a complex user experience, even starting from a simple linear narration. The visual interfaces of the system will be described in relation to a case study focused on culture heritage.
Keywords: Annotation; Education; Ontology; Storytelling; Tag; Web3D

Max Hoffmann, Christian Büscher, Tobias Meisen, Sabina Jeschke,
Continuous Integration of Field Level Production Data into Top-level Information Systems Using the OPC Interface Standard,
Procedia CIRP,
Volume 41,
2016,
Pages 496-501,
ISSN 2212-8271,
https://doi.org/10.1016/j.procir.2015.12.059.
(https://www.sciencedirect.com/science/article/pii/S2212827115011385)
Abstract: On the way to the fourth industrial revolution, one major requirement lies in reaching interoperability between hardware and software systems. Especially real-time propagation of shop floor information in top-level production planning and control systems as well as the consolidation of distributed information into a consistent data basis for comprehensive data analysis are still missing in most production environments. Existing approaches to serve interoperability through standardized interfaces are limited by proprietary data exchange protocols and information models. Within industrial manufacturing and automation, standardization attempts between these systems are primarily focused on industrial interfaces like OPC/OPC-UA. However, the aggregation of data created by devices like sensors or machinery control units into useful information has not been satisfactorily solved yet as their underlying models are carried out using different modeling paradigms and programming languages, thus intercommunication is difficult to implement and to maintain. In this work, an integration chain for data from field level to top-level information systems is presented. As Manufacturing Execution Systems or Enterprise Resource Planning tools are implemented in higher programming languages, the modeling of field level information has to be adapted in terms of a semantic interpretation. The approach provides integration capabilities for OPC-conform data generated on the field level. The information is extracted from low level information systems, transformed according to object-oriented programming paradigms and object-relational standards and finally integrated into databases that allow full semantic annotation and interpretation compatible to a common information model. Hence, users on management levels of the enterprise are able to perform holistic data treatment and data exploration along with personalized information views based on this central data storage by means of a reliable and comfortable data acquisition. This increases the quality of data and of the decision support itself, as more time remains for the actual task of data evaluation.
Keywords: Interoperability; Information integration; Ontology; Semantic data; OPC; OPC UA

Giuseppe Di Modica, Orazio Tomarchio,
Matchmaking semantic security policies in heterogeneous clouds,
Future Generation Computer Systems,
Volume 55,
2016,
Pages 176-185,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2015.03.008.
(https://www.sciencedirect.com/science/article/pii/S0167739X15000631)
Abstract: The adoption of the cloud paradigm to access IT resources and services has posed many security issues which need to be cared of. Security becomes even a much bigger concern when services built on top of many commercial clouds have to interoperate. Among others, the value of the service delivered to end customers is strongly affected by the security of network which providers are able to build in typical SOA contexts. Currently, every provider advertises its own security strategy by means of proprietary policies, which are sometimes ambiguous and very often address the security problem from a non-uniform perspective. Even policies expressed in standardized languages do not appear to fit a dynamic scenario like the SOA’s, where services need to be sought and composed on the fly in a way that is compatible with the end-to-end security requirements. We then propose an approach that leverages on the semantic technology to enrich standardized security policies with an ad-hoc content. The semantic annotation of policies enables machine reasoning which is then used for both the discovery and the composition of security-enabled services. In the presented approach the semantic enrichment of policies is enforced by an automatic procedure. We further developed a semantic framework capable of matchmaking in a smart way security capabilities of providers and security requirements of customers, and tested it on a use case scenario.
Keywords: Cloud computing; Security policies; Semantic; Ontology

Linsey Koo, Nikolaos Trokanas, Franjo Cecelja,
A semantic framework for enabling model integration for biorefining,
Computers & Chemical Engineering,
Volume 100,
2017,
Pages 219-231,
ISSN 0098-1354,
https://doi.org/10.1016/j.compchemeng.2017.02.004.
(https://www.sciencedirect.com/science/article/pii/S0098135417300601)
Abstract: This paper introduces a new paradigm for establishing a framework that enables interoperability between process models and datasets using ontology engineering. Semantics are used to model the knowledge in the domain of biorefining including both tacit and explicit knowledge, which supports registration and instantiation of the models and datasets. Semantic algorithms allow the formation of model integration through input/output matching based on semantic relevance between the models and datasets. In addition, partial matching is employed to facilitate flexibility to broaden the horizon to find opportunities in identifying an appropriate model and/or dataset. The proposed algorithm is implemented as a web service and demonstrated using a case study.
Keywords: Ontology engineering; Model integration; Computer aided process engineering (CAPE); Biorefining

Yasmine Emara, Barbara Koroušić Seljak, Eileen R. Gibney, Gorjan Popovski, Igor Pravst, Peter Fantke,
Workflow for building interoperable food and nutrition security (FNS) data platforms,
Trends in Food Science & Technology,
Volume 123,
2022,
Pages 310-321,
ISSN 0924-2244,
https://doi.org/10.1016/j.tifs.2022.03.022.
(https://www.sciencedirect.com/science/article/pii/S0924224422001133)
Abstract: Background
In response to growing needs for the integration of heterogeneous data on food and nutrition security (FNS), and the current fragmentation of interoperability resources, the ‘FNS-Cloud project’ aims to develop a cross-domain, interoperable data platform that integrates diverse FNS data. Currently, there is insufficient guidance on how to develop such an FNS data platform and integrate a variety of FNS data types that differ in both their syntax and semantics.
Scope and approach
In the present study, we propose a generalizable workflow to guide data managers in building interoperable, cross-domain FNS data platforms, which centres around the definition of interoperability criteria that capture standardized data structures, terminologies and reporting formats for key variables across FNS data types. Information technology tools for automating different workflow steps are discussed. Finally, we include an illustrative case study, where we harmonize and link branded food datasets based on pre-defined interoperability criteria to answer an example research question.
Key findings and conclusions
Our work highlights the unique harmonization requirements within the FNS field. We provide two examples of how generic and domain-specific interoperability criteria addressing these requirements can be defined. Incoming FNS data must comply with defined criteria in order to enable their (semi-)automated integration into any data platform. Our case study reinforces the importance of semantic annotation of FNS data, and the need for clear mapping rules to be included into platform-internal semantic data models. The proposed workflow can be applied to any setting in which data managers strive towards harmonized and linked FNS data, and, thus, promotes an open-data and open-science environment.
Keywords: Data integration; Interoperability criteria; FNS-Cloud; Ontology; Machine learning; Natural language processing; Branded food data

Juan C. Caicedo, Jorge A. Vanegas, Fabian Páez, Fabio A. González,
Histology image search using multimodal fusion,
Journal of Biomedical Informatics,
Volume 51,
2014,
Pages 114-128,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2014.04.016.
(https://www.sciencedirect.com/science/article/pii/S1532046414001014)
Abstract: This work proposes a histology image indexing strategy based on multimodal representations obtained from the combination of visual features and associated semantic annotations. Both data modalities are complementary information sources for an image retrieval system, since visual features lack explicit semantic information and semantic terms do not usually describe the visual appearance of images. The paper proposes a novel strategy to build a fused image representation using matrix factorization algorithms and data reconstruction principles to generate a set of multimodal features. The methodology can seamlessly recover the multimodal representation of images without semantic annotations, allowing us to index new images using visual features only, and also accepting single example images as queries. Experimental evaluations on three different histology image data sets show that our strategy is a simple, yet effective approach to building multimodal representations for histology image search, and outperforms the response of the popular late fusion approach to combine information.
Keywords: Histology; Digital pathology; Image search; Multimodal fusion; Visual representation; Semantic spaces

Wiem Abbes, Dorra Sellami,
Automatic Skin Lesions Classification Using Ontology-Based Semantic Analysis of Optical Standard Images,
Procedia Computer Science,
Volume 112,
2017,
Pages 2096-2105,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2017.08.226.
(https://www.sciencedirect.com/science/article/pii/S1877050917316307)
Abstract: This paper describes ontology-based semantic analysis of lesion images. We first present our ontology focusing on its main concepts, as well as the semantic annotation. Accordingly, the Bag-of-Words (BoW), modeling these concepts in skin lesion diagnosis, is inspired from experts in dermatology. These BoWs are modeled from the lesion images. Firstly, we extract low-level features describing the lesion shape, color and texture. Secondly, the BoWs are generated from these features using a machine learning classifier (SVM). An important step in semantic analysis is to define rules relating the different concepts. In our case, these rules are inspired from the score of the ABCD rule for decision making. Experimental results on a public database of 206 lesion images demonstrate that ontology offers a more efficient frame of analysis, where semantic relations between concepts can handle more knowledge of experts, and can be more appropriate for lesion severity classification with a good accuracy. Comparing to the previous works, our approach yields good sensitivity (97.4%) and accuracy (76.9%).
Keywords: Melanoma; ontology; semantic analysis; bag of words; semantic rules; ABCD rule; semantic annotation; CAD system; features extraction

Hye Hyeon Kim, Soo Youn Lee, Su Youn Baik, Ju Han Kim,
MELLO: Medical lifelog ontology for data terms from self-tracking and lifelog devices,
International Journal of Medical Informatics,
Volume 84, Issue 12,
2015,
Pages 1099-1110,
ISSN 1386-5056,
https://doi.org/10.1016/j.ijmedinf.2015.08.005.
(https://www.sciencedirect.com/science/article/pii/S1386505615300290)
Abstract: Objective
The increasing use of health self-tracking devices is making the integration of heterogeneous data and shared decision-making more challenging. Computational analysis of lifelog data has been hampered by the lack of semantic and syntactic consistency among lifelog terms and related ontologies. Medical lifelog ontology (MELLO) was developed by identifying lifelog concepts and relationships between concepts, and it provides clear definitions by following ontology development methods. MELLO aims to support the classification and semantic mapping of lifelog data from diverse health self-tracking devices.
Methods
MELLO was developed using the General Formal Ontology method with a manual iterative process comprising five steps: (1) defining the scope of lifelog data, (2) identifying lifelog concepts, (3) assigning relationships among MELLO concepts, (4) developing MELLO properties (e.g., synonyms, preferred terms, and definitions) for each MELLO concept, and (5) evaluating representative layers of the ontology content. An evaluation was performed by classifying 11 devices into 3 classes by subjects, and performing pairwise comparisons of lifelog terms among 5 devices in each class as measured using the Jaccard similarity index.
Results
MELLO represents a comprehensive knowledge base of 1998 lifelog concepts, with 4996 synonyms for 1211 (61%) concepts and 1395 definitions for 926 (46%) concepts. The MELLO Browser and MELLO Mapper provide convenient access and annotating non-standard proprietary terms with MELLO (http://mello.snubi.org/). MELLO covers 88.1% of lifelog terms from 11 health self-tracking devices and uses simple string matching to match semantically similar terms provided by various devices that are not yet integrated. The results from the comparisons of Jaccard similarities between simple string matching and MELLO matching revealed increases of 2.5, 2.2, and 5.7 folds for physical activity,body measure, and sleep classes, respectively.
Conclusions
MELLO is the first ontology for representing health-related lifelog data with rich contents including definitions, synonyms, and semantic relationships. MELLO fills the semantic gap between heterogeneous lifelog terms that are generated by diverse health self-tracking devices. The unified representation of lifelog terms facilitated by MELLO can help describe an individual's lifestyle and environmental factors, which can be included with user-generated data for clinical research and thereby enhance data integration and sharing.
Keywords: Ontology; Consumer health; Lifelog

Retno A. Vinarti, Lucy M. Hederman,
A personalized infectious disease risk prediction system,
Expert Systems with Applications,
Volume 131,
2019,
Pages 266-274,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2019.04.042.
(https://www.sciencedirect.com/science/article/pii/S0957417419302726)
Abstract: This article presents a system for predicting a human's risk of contracting infectious diseases based on their personal attributes and environments (region, specific location features and climate contexts). This system is also intended to help human experts in the domain (i.e. epidemiologists) to represent their knowledge and ease their jobs related to personalized infectious disease risk prediction. The system consists of a knowledge representation to encode epidemiological knowledge about infectious disease risk, and an algorithm that auto-converts the encoded knowledge into a model that predicts the risk as a probability. The knowledge representation, Infectious Disease Risk (IDR), consists of an ontology and rules to represent the knowledge structure and its quantification in a way that allows auto-generation to a prediction model, Bayesian Network (BN). The algorithm, BN-Builder, converts the IDR knowledge-base to an infectious disease risk BN, including populating the basis of predictive reasoning from the IDR rules. A user interface facilitates encoding of epidemiological knowledge into the IDR knowledge-base. The system's output, personalized infectious disease risk prediction, is validated for three disease-country contexts: Dengue Fever and Tuberculosis in Indonesia, and Cholera in India. The personalized infectious disease risks are reliable (p values > 0.05) for each population parameter. The personalized infectious disease risk probability can be reliably predicted using this system. Inclusion of more granularity on contexts in this domain will be considered in further development of this system.
Keywords: Infectious disease; Risk prediction; Bayesian network; Ontology; Rule

Mustafa M. Al-Sayed, Hesham A. Hassan, Fatma A. Omara,
An intelligent cloud service discovery framework,
Future Generation Computer Systems,
Volume 106,
2020,
Pages 438-466,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.12.027.
(https://www.sciencedirect.com/science/article/pii/S0167739X19322630)
Abstract: Due to the heterogeneity nature and the huge number of published cloud services, as well as, the continuous increase of the users’ complexity demands, the discovery process of cloud services remains an open issue. According to our conducted comprehensive survey study, the existing cloud service discovery solutions suffer from a set of drawbacks; missing a standardized and comprehensive specification of cloud services, not considering the lack of knowledge about cloud concepts, and not considering the continuous increase of the complex cloud services published to meet the complexity of the users’ demands. In this paper, a comprehensive, standardized, flexible, and intelligent cloud service discovery framework is proposed to overcome these drawbacks. According to this framework, a comprehensive ontology has been developed to provide a standardized semantic specification of cloud services based on their functional features and non-functional features. For exploiting the high performance of the relational databases in managing the large amounts of data and improving the effectiveness of the proposed framework, instances of the non- functional features’ ontological concepts are separated from the developed ontology to be stored in a relational database, where Ontop OBDA platform is used to translate the complex queries over the ontology to SQL queries that are understood by the underlying relational database source. Also, our proposed framework provides a user-centric interface that enables users with low knowledge about cloud concepts to compose complex queries using their natural English language. The effectiveness evaluation shows that the proposed framework provides better results than other solutions. According to the implementation results, the average amount of error expected to identify a service by using the proposed framework is 11% compared to 31% by using the Cloudle service discovery solution. Also, the framework achieves entirely the cloud service discovery evaluation criteria compared to 66% of these criteria for the current cloud service discovery solutions. For efficiency, the proposed framework achieves 88% F-Score for mapping cloud services into suitable functional features and 80% for discovering services that match the users’ queries.
Keywords: Cloud computing; Cloud ontology; OBDA; Information retrieval; Cloud service discovery; Functional features; Non-functional features

Chong Chai Chua, Tek Yong Lim, Lay-Ki Soon, Enya Kong Tang, Bali Ranaivo-Malançon,
Meaning preservation in Example-based Machine Translation with structural semantics,
Expert Systems with Applications,
Volume 78,
2017,
Pages 242-258,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2017.02.021.
(https://www.sciencedirect.com/science/article/pii/S0957417417301033)
Abstract: The main tasks in Example-based Machine Translation (EBMT) comprise of source text decomposition, following with translation examples matching and selection, and finally adaptation and recombination of the target translation. As the natural language is ambiguous in nature, the preservation of source text’s meaning throughout these processes is complex and challenging. A structural semantics is introduced, as an attempt towards meaning-based approach to improve the EBMT system. The structural semantics is used to support deeper semantic similarity measurement and impose structural constraints in translation examples selection. A semantic compositional structure is derived from the structural semantics of the selected translation examples. This semantic compositional structure serves as a representation structure to preserve the consistency and integrity of the input sentence’s meaning structure throughout the recombination process. In this paper, an English to Malay EBMT system is presented to demonstrate the practical application of this structural semantics. Evaluation of the translation test results shows that the new translation framework based on the structural semantics has outperformed the previous EBMT framework.
Keywords: Example-based Machine Translation; Structured String-Tree Correspondence; Synchronous Structured String-Tree Correspondence; Structural semantics; Semantic roles

Mario José Diván, María Laura Sánchez-Reynoso, Silvio Miguel Gonnet,
Measurement project interoperability for real-time data gathering systems,
Future Generation Computer Systems,
Volume 129,
2022,
Pages 298-314,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.11.031.
(https://www.sciencedirect.com/science/article/pii/S0167739X21004738)
Abstract: The Internet-of-Things devices have allowed the increment of the Spatiotemporal resolution in the Real-time Data Gathering Systems. However, this has increased the complexity due to the heterogeneity of sensors. Thus, it is important to know how data can be understood aware of its context at the edge. Also, a transversal perspective is expected for aligning and reusing as many sensors as possible (based on an experimental design). Because of the hardware limitations, the experimental design should be communicated as simple, integrated, and consistent as possible under single content fostering the projects’ complementarity. This work describes a strategy for defining and communicating a set of measurement projects, following a conceptual hierarchy derived from a measurement ontology. Every project definition is obtained from the user requirements following a well-established strategy. BriefPD is introduced as a data interchange format that is self-contained, partially verifiable (through a Merkle tree insight), consistent, and does not require the use of tags to determine the content meaning. BriefPD generates a project definition in 6.27 ms (1.9 times quicker than JSON-equivalent), consuming 20.39% of JSON’s size. A Github library is provided with the reference implementation jointly with a ‘Code Ocean’ capsule for reproducing the simulation results.
Keywords: Measurement; Interoperability; Internet-of-Things; Merkle tree; Metadata-guided processing

Khalid Belhajjame, Jun Zhao, Daniel Garijo, Matthew Gamble, Kristina Hettne, Raul Palma, Eleni Mina, Oscar Corcho, José Manuel Gómez-Pérez, Sean Bechhofer, Graham Klyne, Carole Goble,
Using a suite of ontologies for preserving workflow-centric research objects,
Journal of Web Semantics,
Volume 32,
2015,
Pages 16-42,
ISSN 1570-8268,
https://doi.org/10.1016/j.websem.2015.01.003.
(https://www.sciencedirect.com/science/article/pii/S1570826815000049)
Abstract: Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions. In this article, we present a novel approach to the preservation of scientific workflows through the application of research objects—aggregations of data and metadata that enrich the workflow specifications. Our approach is realised as a suite of ontologies that support the creation of workflow-centric research objects. Their design was guided by requirements elicited from previous empirical analyses of workflow decay and repair. The ontologies developed make use of and extend existing well known ontologies, namely the Object Reuse and Exchange (ORE) vocabulary, the Annotation Ontology (AO) and the W3C PROV ontology (PROVO). We illustrate the application of the ontologies for building Workflow Research Objects with a case-study that investigates Huntington’s disease, performed in collaboration with a team from the Leiden University Medial Centre (HG-LUMC). Finally we present a number of tools developed for creating and managing workflow-centric research objects.
Keywords: Research object; Scientific workflow; Preservation; Annotation; Ontologies; Provenance

Philipp Kestel, Patricia Kügler, Christoph Zirngibl, Benjamin Schleich, Sandro Wartzack,
Ontology-based approach for the provision of simulation knowledge acquired by Data and Text Mining processes,
Advanced Engineering Informatics,
Volume 39,
2019,
Pages 292-305,
ISSN 1474-0346,
https://doi.org/10.1016/j.aei.2019.02.001.
(https://www.sciencedirect.com/science/article/pii/S1474034618304270)
Abstract: Numerical simulation techniques such as Finite Element Analyses are essential in today's engineering design practices. However, comprehensive knowledge is required for the setup of reliable simulations to verify strength and further product properties. Due to limited capacities, design-accompanying simulations are performed too rarely by experienced simulation engineers. Therefore, product models are not sufficiently verified or the simulations lead to wrong design decisions, if they are applied by less experienced users. This results in belated redesigns of already detailed product models and to highly cost- and time-intensive iterations in product development. Thus, in order to support less experienced simulation users in setting up reliable Finite Element Analyses, a novel ontology-based approach is presented. The knowledge management tools developed on the basis of this approach allow an automated acquisition and target-oriented provision of necessary simulation knowledge. This knowledge is acquired from existing simulation models and text-based documentations from previous product developments by Text and Data Mining. By offering support to less experienced simulation users, the presented approach may finally lead to a more efficient and extensive application of reliable FEA in product development.
Keywords: Knowledge-based engineering; Simulation, Finite Element Analysis; Ontology-based knowledge representation; Text Mining; Data Mining

K.A. de Graaf, P. Liang, A. Tang, W.R. van Hage, H. van Vliet,
An exploratory study on ontology engineering for software architecture documentation,
Computers in Industry,
Volume 65, Issue 7,
2014,
Pages 1053-1064,
ISSN 0166-3615,
https://doi.org/10.1016/j.compind.2014.04.006.
(https://www.sciencedirect.com/science/article/pii/S0166361514000840)
Abstract: The usefulness of Software Architecture (SA) documentation depends on how well its Architectural Knowledge (AK) can be retrieved by the stakeholders in a software project. Recent findings show that the use of ontology-based SA documentation is promising. However, different roles in software development have different needs for AK, and building an ontology to suit these needs is challenging. In this paper we describe an approach to build an ontology for SA documentation. This approach involves the use of typical questions for eliciting and constructing an ontology. We outline eight contextual factors, which influence the successful construction of an ontology, especially in complex software projects with diverse AK users. We tested our ‘typical question’ approach in a case study and report how it can be used for acquiring and modeling AK needs.
Keywords: Ontology engineering; Software architecture; Software ontology; Ontology-based documentation; Knowledge acquisition; Knowledge management

R. Lakshmi Tulasi, M. Srinivasa Rao, K. Usha, R.H. Goudar,
Ontology- Based Annotation for Semantic Multimedia Retrieval,
Procedia Computer Science,
Volume 92,
2016,
Pages 148-154,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2016.07.339.
(https://www.sciencedirect.com/science/article/pii/S1877050916315861)
Abstract: Numerous educational video lectures, CCTV surveillance, transport and other types have upgraded the impact of multimedia video content. In order to make large video databases realistic, video data has to be automatically indexed in order to search and retrieve relevant material. An annotation is a markup reference made to data in video in order to improve the video accessibility. Video annotation is used to examine the massive quantity of multimedia data in the repositories. Video annotation refers to the taking out of significant data present in video and placing this data to the video can benefit in “retrieval, browsing, analysis, searching comparison and categorization”. Video annotation implies taking out of data and to attach such metadata to the video which will “accelerate the retrieval speed, ease of access, analysis and categorization”. It permits fast and better understanding of video content and improves the performance of retrieval and decreases human time & efforts for better study of videos. Video annotation is imperative technique that assists in video access. Proposed system provides effortless access to the data of the video and decrease the time necessary to access and evaluate the video. Ontology-based video annotation helps the user to get the semantic information from video, which is essential to search the needful data from a video.
Keywords: Video Annotation; Ontology; Multimedia; Feature Extraction ;

Anastasios Nentidis, Anastasia Krithara, Grigorios Tsoumakas, Georgios Paliouras,
Beyond MeSH: Fine-grained semantic indexing of biomedical literature based on weak supervision,
Information Processing & Management,
Volume 57, Issue 5,
2020,
102282,
ISSN 0306-4573,
https://doi.org/10.1016/j.ipm.2020.102282.
(https://www.sciencedirect.com/science/article/pii/S0306457319312415)
Abstract: In this work, we propose a method for the automated refinement of subject annotations in biomedical literature at the level of concepts. Semantic indexing and search of biomedical articles in MEDLINE/PubMed are based on semantic subject annotations with MeSH descriptors that may correspond to several related but distinct biomedical concepts. Such semantic annotations do not adhere to the level of detail available in the domain knowledge and may not be sufficient to fulfil the information needs of experts in the domain. To this end, we propose a new method that uses weak supervision to train a concept annotator on the literature available for a particular disease. We test this method on the MeSH descriptors for two diseases: Alzheimer’s Disease and Duchenne Muscular Dystrophy. The results indicate that concept-occurrence is a strong heuristic for automated subject annotation refinement and its use as weak supervision can lead to improved concept-level annotations. The fine-grained semantic annotations can enable more precise literature retrieval, sustain the semantic integration of subject annotations with other domain resources and ease the maintenance of consistent subject annotations, as new more detailed entries are added in the MeSH thesaurus over time.
Keywords: Semantic indexing; MeSH; Biomedical literature; Weak supervision

Michael Diepenbroek, Uwe Schindler, Robert Huber, Stéphane Pesant, Markus Stocker, Janine Felden, Melanie Buss, Matthias Weinrebe,
Terminology supported archiving and publication of environmental science data in PANGAEA,
Journal of Biotechnology,
Volume 261,
2017,
Pages 177-186,
ISSN 0168-1656,
https://doi.org/10.1016/j.jbiotec.2017.07.016.
(https://www.sciencedirect.com/science/article/pii/S0168165617315419)
Abstract: Exemplified on the information system PANGAEA, we describe the application of terminologies for archiving and publishing environmental science data. A terminology catalogue (TC) was embedded into the system, with interfaces allowing to replicate and to manually work on terminologies. For data ingest and archiving, we show how the TC can improve structuring and harmonizing lineage and content descriptions of data sets. Key is the conceptualization of measurement and observation types (parameters) and methods, for which we have implemented a basic syntax and rule set. For data access and dissemination, we have improved findability of data through enrichment of metadata with TC terms. Semantic annotations, e.g. adding term concepts (including synonyms and hierarchies) or mapped terms of different terminologies, facilitate comprehensive data retrievals. The PANGAEA thesaurus of classifying terms, which is part of the TC is used as an umbrella vocabulary that links the various domains and allows drill downs and side drills with various facets. Furthermore, we describe how TC terms can be linked to nominal data values. This improves data harmonization and facilitates structural transformation of heterogeneous data sets to a common schema. Technical developments are complemented by work on the metadata content. Over the last 20 years, more than 100 new parameters have been defined on average per week. Recently, PANGAEA has increasingly been submitting new terms to various terminology services. Matching terms from terminology services with our parameter or method strings is supported programmatically. However, the process ultimately needs manual input by domain experts. The quality of terminology services is an additional limiting factor, and varies with respect to content, editorial, interoperability, and sustainability. Good quality terminology services are the building blocks for the conceptualization of parameters and methods. In our view, they are essential for data interoperability and arguably the most difficult hurdle for data integration. In summary, the application of terminologies has a mutual positive effect for terminology services and information systems such as PANGAEA. On both sides, the application of terminologies improves content, reliability and interoperability.
Keywords: Data publishing; Data interoperability; Data findability; Terminologies; Semantics

F. De Backere, F. Ongenae, F. Van den Abeele, J. Nelis, P. Bonte, E. Clement, M. Philpott, J. Hoebeke, S. Verstichel, A. Ackaert, F. De Turck,
Towards a social and context-aware multi-sensor fall detection and risk assessment platform,
Computers in Biology and Medicine,
Volume 64,
2015,
Pages 307-320,
ISSN 0010-4825,
https://doi.org/10.1016/j.compbiomed.2014.12.002.
(https://www.sciencedirect.com/science/article/pii/S0010482514003345)
Abstract: For elderly people fall incidents are life-changing events that lead to degradation or even loss of autonomy. Current fall detection systems are not integrated and often associated with undetected falls and/or false alarms. In this paper, a social- and context-aware multi-sensor platform is presented, which integrates information gathered by a plethora of fall detection systems and sensors at the home of the elderly, by using a cloud-based solution, making use of an ontology. Within the ontology, both static and dynamic information is captured to model the situation of a specific patient and his/her (in)formal caregivers. This integrated contextual information allows to automatically and continuously assess the fall risk of the elderly, to more accurately detect falls and identify false alarms and to automatically notify the appropriate caregiver, e.g., based on location or their current task. The main advantage of the proposed platform is that multiple fall detection systems and sensors can be integrated, as they can be easily plugged in, this can be done based on the specific needs of the patient. The combination of several systems and sensors leads to a more reliable system, with better accuracy. The proof of concept was tested with the use of the visualizer, which enables a better way to analyze the data flow within the back-end and with the use of the portable testbed, which is equipped with several different sensors.
Keywords: Fall detection; Fall risk assessment; Ontology; Semantic; Context-aware

Valentina Nejkovic, Muhammed Maruf Öztürk, Nenad Petrovic,
Head pose healthiness prediction using a novel image quality based stacked autoencoder,
Digital Signal Processing,
Volume 130,
2022,
103696,
ISSN 1051-2004,
https://doi.org/10.1016/j.dsp.2022.103696.
(https://www.sciencedirect.com/science/article/pii/S105120042200313X)
Abstract: This paper introduces an approach aiming to determine head pose healthiness of computer users. The main contributions of this paper are: 1) Image Quality Assessment (IQA) based Stacked Autoencoder (referred to as IQASAE) which adjusts the value of learning rate based on the quality of images; 2) Head Pose Healthiness Prediction (HPHP) framework which leverages the proposed IQASAE algorithm in combination with image processing operations; 3) A set of features suitable for face analysis applications; 4) Ontology-driven semantic framework which enables further exploiting pose estimation results within applications in synergy with healthcare expert domain knowledge about pose healthiness. Our framework was evaluated on both offline (BIWI and AFLW) and online (our own, collected using Arduino) datasets. Furthermore, it was compared to several state-of-art methods, including Multi-Layer Perceptron (MLP), CART, Random Forest, Convolutional Neural Networks (CNN), Temporal Deep Learning Model (TDLM), hybrid CNN with Support Vector Machine (SVM), Quatnet and Trinet. According to the achieved experimental results, it reaches accuracy up to 79.63% outperforming all of them, except Quatnet and Trinet. However, the main advantages of IQASAE compared to state-of-art methods are: 1) it does not require selection of features, so the processing time is reduced, 2) utilizing angle between chin and mouth reduces training time for SAE, 3) leveraging vector-based feature set to create training data resulted in a significant improvement, especially in offline facial images.
Keywords: Head pose healthiness prediction; Stacked autoencoder; Image quality; Pose estimation; Ontology

Carlos Agostinho, Yves Ducq, Gregory Zacharewicz, João Sarraipa, Fenareti Lampathaki, Raul Poler, Ricardo Jardim-Goncalves,
Towards a sustainable interoperability in networked enterprise information systems: Trends of knowledge and model-driven technology,
Computers in Industry,
Volume 79,
2016,
Pages 64-76,
ISSN 0166-3615,
https://doi.org/10.1016/j.compind.2015.07.001.
(https://www.sciencedirect.com/science/article/pii/S0166361515300191)
Abstract: In a turbulent world, global competition and the uncertainty of markets have led organizations and technology to evolve exponentially, surpassing the most imaginary scenarios predicted at the beginning of the digital manufacturing era, in the 1980s. Business paradigms have changed from a standalone vision into complex and collaborative ecosystems where enterprises break down organizational barriers to improve synergies with others and become more competitive. In this context, paired with networking and enterprise integration, enterprise information systems (EIS) interoperability gained utmost importance, ensuring an increasing productivity and efficiency thanks to a promise of more automated information exchange in networked enterprises scenarios. However, EIS are also becoming more dynamic. Interfaces that are valid today are outdated tomorrow, thus static interoperability enablers and communication software services are no longer the solution for the future. This paper is focused on the challenge of sustaining networked EIS interoperability, and takes up input from solid research initiatives in the areas of knowledge management and model driven development, to propose and discuss several research strategies and technological trends towards next EIS generation.
Keywords: Sustainable interoperability; Model-driven interoperability; Knowledge management semantic matching; Ontologies; Model-driven service engineering; Enterprise information systems

Nenad Petrovic, Milorad Tosic,
SMADA-Fog: Semantic model driven approach to deployment and adaptivity in fog computing,
Simulation Modelling Practice and Theory,
Volume 101,
2020,
102033,
ISSN 1569-190X,
https://doi.org/10.1016/j.simpat.2019.102033.
(https://www.sciencedirect.com/science/article/pii/S1569190X19301649)
Abstract: The deployment, monitoring and configuration of applications in Fog Computing are becoming quite challenging, due to heterogeneity of mobile and IoT devices involved, data movement constraints imposed by legal regulations as well as frequent changes in the execution environment that may affect quality of service. As a consequence, the system administration procedures are becoming more complex and time-consuming, especially if done manually. In this paper, a Semantic Model driven Approach to Deployment and Adaptivity of container-based applications in Fog Computing (SMADA-Fog) is proposed. Modeling tools, semantic framework, linear optimization model, simulation environment and infrastructure management code generator leveraging the semantic annotations are implemented and presented. According to results of the two experimentally tested scenarios, the proposed approach improves the application performance, while the time required for deployment as well as service adaptation is reduced for at least an order of magnitude.
Keywords: DevOps; Fog Computing; Infrastructure as code; Linear optimization; Model-driven engineering; Semantic technology

Stefania Rubrichi, Silvana Quaglini, Alex Spengler, Paola Russo, Patrick Gallinari,
A system for the extraction and representation of summary of product characteristics content,
Artificial Intelligence in Medicine,
Volume 57, Issue 2,
2013,
Pages 145-154,
ISSN 0933-3657,
https://doi.org/10.1016/j.artmed.2012.08.004.
(https://www.sciencedirect.com/science/article/pii/S0933365712001121)
Abstract: Objective
Information about medications is critical in supporting decision-making during the prescription process and thus in improving the safety and quality of care. In this work, we propose a methodology for the automatic recognition of drug-related entities (active ingredient, interaction effects, etc.) in textual drug descriptions, and their further location in a previously developed domain ontology.
Methods and material
The summary of product characteristics (SPC) represents the basis of information for health professionals on how to use medicines. However, this information is locked in free-text and, as such, cannot be actively accessed and elaborated by computerized applications. Our approach exploits a combination of machine learning and rule-based methods. It consists of two stages. Initially it learns to classify this information in a structured prediction framework, relying on conditional random fields. The classifier is trained and evaluated using a corpus of about a hundred SPCs. They have been hand-annotated with different semantic labels that have been derived from the domain ontology. At a second stage the extracted entities are added in the domain ontology corresponding concepts as new instances, using a set of rules manually-constructed from the corpus.
Results
Our evaluations show that the extraction module exhibits high overall performance, with an average F1-measure of 88% for contraindications and 90% for interactions.
Conclusion
SPCs can be exploited to provide structured information for computer-based decision support systems.
Keywords: Information extraction; Conditional random fields; Ontology; Summary of product characteristics; Adverse drug events; Medication errors

Antonio De Nicola, Anna Formica, Michele Missikoff, Elaheh Pourabbas, Francesco Taglino,
A parametric similarity method: Comparative experiments based on semantically annotated large datasets,
Journal of Web Semantics,
Volume 76,
2023,
100773,
ISSN 1570-8268,
https://doi.org/10.1016/j.websem.2023.100773.
(https://www.sciencedirect.com/science/article/pii/S1570826823000021)
Abstract: We present the parametric method SemSimp aimed at measuring semantic similarity of digital resources. SemSimp is based on the notion of information content, and it leverages a reference ontology and taxonomic reasoning, encompassing different approaches for weighting the concepts of the ontology. In particular, weights can be computed by considering either the available digital resources or the structure of the reference ontology of a given domain. SemSimp is assessed against six representative semantic similarity methods for comparing sets of concepts proposed in the literature, by carrying out an experimentation that includes both a statistical analysis and an expert judgment evaluation. To the purpose of achieving a reliable assessment, we used a real-world large dataset based on the Digital Library of the Association for Computing Machinery (ACM), and a reference ontology derived from the ACM Computing Classification System (ACM-CCS). For each method, we considered two indicators. The first concerns the degree of confidence to identify the similarity among the papers belonging to some special issues selected from the ACM Transactions on Information Systems journal, the second the Pearson correlation with human judgment. The results reveal that one of the configurations of SemSimp outperforms the other assessed methods. An additional experiment performed in the domain of physics shows that, in general, SemSimp provides better results than the other similarity methods.
Keywords: Semantic similarity reasoning; Weighted ontology; Information content; Statistical analysis; Expert judgment; Benchmarking

Thiago Sanches Ranzani da Silva,
Species descriptions and digital environments: alternatives for accessibility of morphological data,
Revista Brasileira de Entomologia,
Volume 61, Issue 4,
2017,
Pages 277-281,
ISSN 0085-5626,
https://doi.org/10.1016/j.rbe.2017.06.005.
(https://www.sciencedirect.com/science/article/pii/S0085562617300328)
Abstract: Taxonomists’ efforts throughout history provide significant amount of data that give support for establishing the specific identity of several groups of biological systems. In addition to identifying species, taxonomic research offers a wide range of biological information that can be used in other disciplines, e.g. evolution, ecology, integrated pest management. However, most of this information remains unappreciated due to certain aspects: (1) the advent of analytical tools have led to a shift in interest and investment in researches, focusing mainly in molecular studies; (2) the erroneous concept that the extensive data offered by taxonomic studies can be replaced by other datasets, separating it from its hypothesis-driven and investigative nature; (3) the final products found in taxonomic works are commonly restricted to a small group of researchers, due to its low accessibility and specific language. Considering this last aspect, web-based tools can be valuable to simplify the dissemination of the taxonomic product. Semantic annotation provide a condition in which species descriptions can be readily available and be far more extensive, enabling rapid exchange of countless data related to biological systems.
Keywords: Data accessibility; Interoperability; Annotations; Web-based taxonomy

Satya S. Sahoo, Priya Ramesh, Elisabeth Welter, Ashley Bukach, Joshua Valdez, Curtis Tatsuoka, Yvan Bamps, Shelley Stoll, Barbara C. Jobst, Martha Sajatovic,
Insight: An ontology-based integrated database and analysis platform for epilepsy self-management research,
International Journal of Medical Informatics,
Volume 94,
2016,
Pages 21-30,
ISSN 1386-5056,
https://doi.org/10.1016/j.ijmedinf.2016.06.009.
(https://www.sciencedirect.com/science/article/pii/S1386505616301381)
Abstract: We present Insight as an integrated database and analysis platform for epilepsy self-management research as part of the national Managing Epilepsy Well Network. Insight is the only available informatics platform for accessing and analyzing integrated data from multiple epilepsy self-management research studies with several new data management features and user-friendly functionalities. The features of Insight include, (1) use of Common Data Elements defined by members of the research community and an epilepsy domain ontology for data integration and querying, (2) visualization tools to support real time exploration of data distribution across research studies, and (3) an interactive visual query interface for provenance-enabled research cohort identification. The Insight platform contains data from five completed epilepsy self-management research studies covering various categories of data, including depression, quality of life, seizure frequency, and socioeconomic information. The data represents over 400 participants with 7552 data points. The Insight data exploration and cohort identification query interface has been developed using Ruby on Rails Web technology and open source Web Ontology Language Application Programming Interface to support ontology-based reasoning. We have developed an efficient ontology management module that automatically updates the ontology mappings each time a new version of the Epilepsy and Seizure Ontology is released. The Insight platform features a Role-based Access Control module to authenticate and effectively manage user access to different research studies. User access to Insight is managed by the Managing Epilepsy Well Network database steering committee consisting of representatives of all current collaborating centers of the Managing Epilepsy Well Network. New research studies are being continuously added to the Insight database and the size as well as the unique coverage of the dataset allows investigators to conduct aggregate data analysis that will inform the next generation of epilepsy self-management studies.

David Gil, Il-Yeol Song, José F. Aldana, Juan Trujillo,
Big Data. New approaches of modelling and management,
Computer Standards & Interfaces,
Volume 54, Part 2,
2017,
Pages 61-63,
ISSN 0920-5489,
https://doi.org/10.1016/j.csi.2017.03.006.
(https://www.sciencedirect.com/science/article/pii/S0920548917301022)
Abstract: Nowadays, there are a huge number of autonomous and diverse information sources providing heterogeneous data. Sensors, social media data, data on the Web, open data, just to name a few, resulting in a major confluence of Big Data. In this survey, we discuss these diverse data sources and detail the way in which data are acquired, stored, processed and analysed. Although some of the opportunities in this new state are mentioned, the main objective of this analysis is to present the challenges for Big Data. To accomplish this goal, we examine the new proposals and approaches presented in this special issue with the aim of establishing new models for improving the management of the volume, velocity, and variety, of Big Data. Some of these schemes establish the use of Ontologies, Semantic Processing, Cloud Computing and Data Management and could be seen as intelligent services integrated as context-aware services.

Chris McMahon, Alex Ball,
Information Systems Challenges for through-life Engineering,
Procedia CIRP,
Volume 11,
2013,
Pages 1-7,
ISSN 2212-8271,
https://doi.org/10.1016/j.procir.2013.07.071.
(https://www.sciencedirect.com/science/article/pii/S2212827113005453)
Abstract: Information technologies hold great promise in achieving reduction in through-life support costs for long-lived complex artefacts such as aircraft and ships, and may allow very much improved assessment of asset condition, but in order for these to be achieved a number of technical and socio-technical challenges have to be overcome. Based on a perspective gained in the EPSRC Knowledge and Information Management Through-Life Grand Challenge project this paper gives an over view of these challenges, of recent research achievement and of areas where further research is needed. In particular, it notes that it is important to identify what information needs to be captured through the life of the artefact and how the information may be organised and sustained over long timescales. Important standards are reviewed, as are emerging developments such as classification systems and ontologies for organisation and the use of lightweight representations and annotation. Finally, socio-technical challenges including data accuracy and quality issues, security and privacy and the latency in multi-faceted information systems are reviewed.
Keywords: Through-life engineering; information systems; knowledge management; information standards ;

Antonio A. Lopez-Lorca, Ghassan Beydoun, Rafael Valencia-Garcia, Rodrigo Martinez-Bejar,
Supporting agent oriented requirement analysis with ontologies,
International Journal of Human-Computer Studies,
Volume 87,
2016,
Pages 20-37,
ISSN 1071-5819,
https://doi.org/10.1016/j.ijhcs.2015.10.007.
(https://www.sciencedirect.com/science/article/pii/S1071581915001755)
Abstract: Requirements analysis activities underpin the success of the software development lifecycle. Subsequent errors in the requirements models can propagate to models in later phases and become much costlier to fix. Errors in requirement analysis are more likely in developing complex systems. Particularly, errors due to miscommunication and misinterpretation of a client׳s intentions are common. Ontologies relying on formal descriptions of semantics have often been used in multi agent systems (MAS) development to support various activities and generally improve the complex systems produced. However, their use during requirements analysis to validate match with the client׳s conceptualisation is largely unexplored. This article presents an ontology driven validation process to support requirement analysis of MAS models. This process is underpinned by an agent-based metamodel that describes commonly used informal agent requirement models. The process concurrently and incrementally validates the informal MAS requirement models produced. The synthesis of the process is first justified and illustrated in a manual tracing of the process. The paper then describes an interactive support tool to harness the formal semantics of ontologies and by pass the costly manual effort. The validation process is evaluated and illustrated using three case studies.
Keywords: Requirements; Validation; Ontology; Ontology modelling; Multi agent systems

Muhammad Intizar Ali, Naomi Ono, Mahedi Kaysar, Zia Ush Shamszaman, Thu-Le Pham, Feng Gao, Keith Griffin, Alessandra Mileo,
Real-time data analytics and event detection for IoT-enabled communication systems,
Journal of Web Semantics,
Volume 42,
2017,
Pages 19-37,
ISSN 1570-8268,
https://doi.org/10.1016/j.websem.2016.07.001.
(https://www.sciencedirect.com/science/article/pii/S1570826816300324)
Abstract: Enterprise Communication Systems are designed in such a way to maximise the efficiency of communication and collaboration within the enterprise. With users becoming mobile, the Internet of Things (IoT) can play a crucial role in this process, but is far from being seamlessly integrated into modern online communications. In this paper, we present a semantic infrastructure for gathering, integrating and reasoning upon heterogeneous, distributed and continuously changing data streams by means of semantic technologies and rule-based inference. Our solution exploits semantics to go beyond today’s ad-hoc integration and processing of heterogeneous data sources for static and streaming data. It provides flexible and efficient processing techniques that can transform low-level data into high-level abstractions and actionable knowledge, bridging the gap between IoT and online Enterprise Communication Systems. We document the technologies used for acquisition and semantic enrichment of sensor data, continuous semantic query processing for integration and filtering, as well as stream reasoning for decision support. Our main contributions are the following, (i) we define and deploy a semantic processing pipeline for IoT-enabled Communication Systems, which builds upon existing systems for semantic data acquisition, continuous query processing and stream reasoning, detailing the implementation of each component of our framework; (ii) we present a rich semantic information model for representing and linking IoT data, social data and personal data in the Enterprise Communication scenario, by reusing and extending existing standard semantic models; (iii) we define and develop an expressive stream reasoning component as part of our framework, based on continuous query processing and non-monotonic reasoning for semantic streams, (iv) we conduct experiments to comparatively evaluate the performance of our data acquisition and semantic annotation layer based on OpenIoT, and the performance of our expressive reasoning layer in the scenario of Enterprise Communication.
Keywords: RDF stream processing; Stream federation; Internet of Things (IoT); Communication systems; Linked data; Stream reasoning

Dumitru Dan Burdescu, Cristian Gabriel Mihai, Liana Stanescu, Marius Brezovan,
Automatic image annotation and semantic based image retrieval for medical domain,
Neurocomputing,
Volume 109,
2013,
Pages 33-48,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2012.07.030.
(https://www.sciencedirect.com/science/article/pii/S0925231212006698)
Abstract: Automatic image annotation is the process of assigning meaningful words to an image taking into account its content. This process is of great interest as it allows indexing, retrieving, and understanding of large collections of image data. This paper presents a system used in the medical domain for three distinct tasks: image annotation, semantic based image retrieval and content based image retrieval. An original image segmentation algorithm based on a hexagonal structure was used to perform the segmentation of medical images. Image's regions are described using a vocabulary of blobs generated from image features using the K-means clustering algorithm. The annotation and semantic based retrieval task is evaluated for two annotation models: Cross Media Relevance Model and Continuous-space Relevance Model. Semantic based image retrieval is performed using the methods provided by the annotation models. The ontology used by the annotation process was created in an original manner starting from the information content provided by the Medical Subject Headings (MeSH). The experiments were made using a database containing color images retrieved from medical domain using an endoscope and related to digestive diseases.
Keywords: Image annotation; Image segmentation; Relevance models; Ontologies; Content based image retrieval

Athon Francisco Curi de Moura Leite, Matheus Beltrame Canciglieri, Anderson Luis Szejka, Osiris Canciglieri Junior,
The reference view for semantic interoperability in Integrated Product Development Process: The conceptual structure for injecting thin walled plastic products,
Journal of Industrial Information Integration,
Volume 7,
2017,
Pages 13-23,
ISSN 2452-414X,
https://doi.org/10.1016/j.jii.2017.06.002.
(https://www.sciencedirect.com/science/article/pii/S2452414X1630111X)
Abstract: The ongoing sophistication in costumer's needs have increased the complexity in the Integrated Product Development Process (IPDP). This remarkable growth demands that systems offer support to many different stages of IPDP, going across early planning and practical development phases. Concurrently, the level of detail and quantity of information are exponentially growing in this applications. However, there was also an increase in the number of semantic obstacles to sharing information across systems. These semantic obstacles are mainly related to the heterogenic nature of the information, which has its captured meaning interpreted in a divergent way, increasing the project costs and development time. In this context, this article contributes to the development of the core ontologies from the Reference View, to aid the semantic interoperability in the Product Design stage, further improving the exchange of information during different phases of the IPDP. The core ontologies research is focused on supporting the development of a plastic injected product, gathering and sharing information across the domains of part and mould design, mould manufacture and materials selection. The experimental system has demonstrated the potential to aid the exchange of information and inconsistency analysis in the Integrated Product Development Process.
Keywords: Integrated Product Development Process; Semantic Interoperability; Ontology; Reference View

Petr Křemen, Martin Nečaský,
Improving discoverability of open government data with rich metadata descriptions using semantic government vocabulary,
Journal of Web Semantics,
Volume 55,
2019,
Pages 1-20,
ISSN 1570-8268,
https://doi.org/10.1016/j.websem.2018.12.009.
(https://www.sciencedirect.com/science/article/pii/S1570826818300714)
Abstract: The descriptive metadata gathered by open data catalogs are often simple key–value pairs that describe provenance information, but not concepts from the domain of the described dataset. Search engines relying on such metadata cannot make use of semantic connections among datasets. In this paper, we present a Semantic Government Vocabulary that is used for creating rich annotations of Open Government Data, allowing to find their mutual interconnections, as well as document their meaning in the machine readable form. We discuss how the Semantic Government Vocabulary is layered based on the different ontological types of terms occurring in the Open Government Data. Next, we show how the vocabularies can be used to annotate Open Government Data on different levels of detail and how to formalize the whole stack in the Web Ontology Language. We evaluate feasibility and usability of our approach using a study in the elections domain.
Keywords: Linked data; Open data; Ontology; Data catalog; Dataset discovery

Shu-Bo Zhang, Qiang-Rong Tang,
Predicting protein subcellular localization based on information content of gene ontology terms,
Computational Biology and Chemistry,
Volume 65,
2016,
Pages 1-7,
ISSN 1476-9271,
https://doi.org/10.1016/j.compbiolchem.2016.09.009.
(https://www.sciencedirect.com/science/article/pii/S1476927116301748)
Abstract: Predicting the location where a protein resides within a cell is important in cell biology. Computational approaches to this issue have attracted more and more attentions from the community of biomedicine. Among the protein features used to predict the subcellular localization of proteins, the feature derived from Gene Ontology (GO) has been shown to be superior to others. However, most of the sights in this field are set on the presence or absence of some predefined GO terms. We proposed a method to derive information from the intrinsic structure of the GO graph. The feature vector was constructed with each element in it representing the information content of the GO term annotating to a protein investigated, and the support vector machines was used as classifier to test our extracted features. Evaluation experiments were conducted on three protein datasets and the results show that our method can enhance eukaryotic and human subcellular location prediction accuracy by up to 1.1% better than previous studies that also used GO-based features. Especially in the scenario where the cellular component annotation is absent, our method can achieved satisfied results with an overall accuracy of more than 87%.
Keywords: Protein subcellular localization; Gene ontology; Information content; Support vector machines

Jamal Malki, Alain Bouju,
6 - An Ontologically-based Trajectory Modeling Approach for an Early Warning System,
Editor(s): Florence Sèdes,
How Information Systems Can Help in Alarm/Alert Detection,
Elsevier,
2018,
Pages 165-198,
ISBN 9781785483028,
https://doi.org/10.1016/B978-1-78548-302-8.50006-X.
(https://www.sciencedirect.com/science/article/pii/B978178548302850006X)
Abstract: Abstract:
Thanks to advances in scientific research, communication and information studies, and sensor technologies, considerable progress has been made in the field of early warning systems, especially for animal zone tracking. A global early warning system is needed to inform us of pending threats. The basic idea behind early warning is that the earlier and more accurately we are able to predict short- and long-term potential risks associated with natural and human-induced hazards, the more likely we will be able to manage and mitigate a disaster’s impact on society, economies and the environment. Early warning is “the provision of timely and effective information, through identified institutions, that allows individuals exposed to hazard to take action to avoid or reduce their risk and prepare for effective response”.
Keywords: Design and methodology; Domain inference implementation; Domain trajectory ontology; Ontologically-based Trajectory Modeling; Semantic trajectory ontology; Temporal inference implementation; Time ontology; Trajectory ontology inference entailment

Mehdi Mirzapour, Amine Abdaoui, Andon Tchechmedjiev, William Digan, Sandra Bringay, Clement Jonquet,
French FastContext: A publicly accessible system for detecting negation, temporality and experiencer in French clinical notes,
Journal of Biomedical Informatics,
Volume 117,
2021,
103733,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2021.103733.
(https://www.sciencedirect.com/science/article/pii/S1532046421000629)
Abstract: The context of medical conditions is an important feature to consider when processing clinical narratives. NegEx and its extension ConText became the most well-known rule-based systems that allow determining whether a medical condition is negated, historical or experienced by someone other than the patient in English clinical text. In this paper, we present a French adaptation and enrichment of FastContext which is the most recent, n-trie engine-based implementation of the ConText algorithm. We compiled an extensive list of French lexical cues by automatic and manual translation and enrichment. To evaluate French FastContext, we manually annotated the context of medical conditions present in two types of clinical narratives: (i)death certificates and (ii)electronic health records. Results show good performance across different context values on both types of clinical notes (on average 0.93 and 0.86 F1, respectively). Furthermore, French FastContext outperforms previously reported French systems for negation detection when compared on the same datasets and it is the first implementation of contextual temporality and experiencer identification reported for French. Finally, French FastContext has been implemented within the SIFR Annotator: a publicly accessible Web service to annotate French biomedical text data (http://bioportal.lirmm.fr/annotator). To our knowledge, this is the first implementation of a Web-based ConText-like system in a publicly accessible platform allowing non-natural-language-processing experts to both annotate and contextualize medical conditions in clinical notes.
Keywords: French clinical notes; Electronic health records; ConText; Biomedical terminologies; Semantic annotation; Negation detection; Temporality and experiencer detection

Hao Liu, Simona Carini, Zhehuan Chen, Spencer Phillips Hey, Ida Sim, Chunhua Weng,
Ontology-based categorization of clinical studies by their conditions,
Journal of Biomedical Informatics,
Volume 135,
2022,
104235,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2022.104235.
(https://www.sciencedirect.com/science/article/pii/S1532046422002404)
Abstract: Objective
The free-text Condition data field in the ClinicalTrials.gov is not amenable to computational processes for retrieving, aggregating and visualizing clinical studies by condition categories. This paper contributes a method for automated ontology-based categorization of clinical studies by their conditions.
Materials and Methods
Our method first maps text entries in ClinicalTrials.gov’s Condition field to standard condition concepts in the OMOP Common Data Model by using SNOMED CT as a reference ontology and using Usagi for concept normalization, followed by hierarchical traversal of the SNOMED ontology for concept expansion, ontology-driven condition categorization, and visualization. We compared the accuracy of this method to that of the MeSH-based method.
Results
We reviewed the 4,506 studies on Vivli.org categorized by our method. Condition terms of 4,501 (99.89%) studies were successfully mapped to SNOMED CT concepts, and with a minimum concept mapping score threshold, 4,428 (98.27%) studies were categorized into 31 predefined categories. When validating with manual categorization results on a random sample of 300 studies, our method achieved an estimated categorization accuracy of 95.7%, while the MeSH-based method had an accuracy of 85.0%.
Conclusion
We showed that categorizing clinical studies using their Condition terms with referencing to SNOMED CT achieved a better accuracy and coverage than using MeSH terms. The proposed ontology-driven condition categorization was useful to create accurate clinical study categorization that enables clinical researchers to aggregate evidence from a large number of clinical studies.
Keywords: Ontology; Clinical Study; SNOMED CT; Data Visualization; Categorization

Mingcong Huang, Shaocun Sui, Wenping Mou, Shusheng Zhang, Wenjun Cao,
Three-dimensional CAD Model Retrieval Algorithm Based on Ontology,
Procedia CIRP,
Volume 56,
2016,
Pages 590-593,
ISSN 2212-8271,
https://doi.org/10.1016/j.procir.2016.10.116.
(https://www.sciencedirect.com/science/article/pii/S2212827116311088)
Abstract: For resolving problems such as how the user implement three-dimensional CAD model retrievals and how to reuse the retrieval results during the smart process planning, this paper present an ontology-based algorithm for three-dimensional CAD model retrieval. The CAD model is segmented into several relevant sub-parts. Then, attach semantic descriptions and annotations to these sub-parts. Finally, evaluate the similarity of the models based on the semantic ontology 3D CAD model. The experimental results show the efficiency of this method to meet the requirements of engineering retrieval and reuse of design and manufacture.
Keywords: CAD model retrieval; reuse; semantic ontology; MBD

Sarah Dahir, Abderrahim El Qadi, Hamid Bennis,
Query expansion based on term distribution and DBpedia features,
Expert Systems with Applications,
Volume 176,
2021,
114909,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2021.114909.
(https://www.sciencedirect.com/science/article/pii/S095741742100350X)
Abstract: Query Expansion (QE) approaches that involve the reformulation of queries by adding new terms to the initial user query, are intended to ameliorate the vocabulary mismatch between the query keywords and the documents’ in Information Retrieval Systems (IRS). One big issue in QE is the selection of the right candidate terms for expansion. For this purpose Linked Data can be used, as a valuable resource, for providing additional expansion features such as the values of sub- and super classes of resources. The underlying research question is whether interlinked data and vocabulary items provide features which can be taken into account for query expansion. In this paper, we introduced a new QE approach that aimed at improving IRS by using the well-known distribution based method Bose-Einstein statistics (Bo1) as well as Linked Data from the knowledge base DBpedia using different numbers of expansion terms. We evaluated the effectiveness of each method individually as well as their combinations using two Text REtrieval Conference (TREC) test collections. Our approach has lead to significant improvement in terms of precision, recall, Mean Average Precision (MAP) at rank 10, and normalized Discounted Cumulative Gain (nDCG) at different ranks compared to Pseudo Relevance Feedback (PRF) that we used as a baseline. The results show that the inclusion of semantic annotations clearly improves the retrieval performance over the baseline method.
Keywords: Information retrieval; Query expansion; DBpedia spotlight; Term distribution; Language model

Vincenzo Moscato, Antonio Picariello, Antonio M. Rinaldi,
Towards a user based recommendation strategy for digital ecosystems,
Knowledge-Based Systems,
Volume 37,
2013,
Pages 165-175,
ISSN 0950-7051,
https://doi.org/10.1016/j.knosys.2012.07.021.
(https://www.sciencedirect.com/science/article/pii/S0950705112002092)
Abstract: In digital ecosystems, the possibility of discovering useful digital objects based on recommendation techniques is one of the most useful tasks that still remains to realize. In this realm, it seems really powerful to have both an a priori user profiling and an a posteriori knowledge, extracted from historical and current user behavior, thus recommender systems can be easily adapted to these environments. In this paper, we present a hybrid strategy that proposes customized recommendations using semantic contents and potentially low-level features of multimedia objects, past behavior of users in terms of usage patterns and user interests expressed by ontologies. We have implemented a prototype that supports our proposed recommendation strategy and a real use of our system based on a 3D interface is shown. Finally, some preliminary experimental results are presented and discussed.
Keywords: Recommender systems; Digital ecosystems; Semantic analysis; User based systems; Ontologies

Jonathan Cooper, James Osborne,
Connecting Models to Data in Multiscale Multicellular Tissue Simulations,
Procedia Computer Science,
Volume 18,
2013,
Pages 712-721,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2013.05.235.
(https://www.sciencedirect.com/science/article/pii/S1877050913003785)
Abstract: System level biological behaviour typically arises from highly dynamic, strongly nonlinear, tightly coupled interactions between component processes occurring across multiple space and time scales. The interdependent nature of these processes often makes it difficult to apply standard mathematical techniques to separate out the scales, uncouple the physical processes or average over contributions from discrete components. To make rapid progress we need to address interoperability challenges: to build integrated models from reusable components, and to relate simulation results to experimental data both for parameter fitting and model analysis. In this paper we describe how work we have done to address these issues in the domain of cardiac electrophysiology can be applied in a completely different field: multicellular models of intestinal crypts, with cells treated as discrete entities, and the sub-cellular, cellular, and tissue scales interacting. In this application the model and simulation are intertwined in software, with no suitable markup language model representation. Different modelling paradigms are available for each of the scales, and comparing their predictions is of particular interest. We use our concept of ‘functional curation’ to separate the experimental protocols applied to models from the model descriptions themselves, allowing easier comparison of model behaviour with experimental data. We also describe the use of ontological annotation for providing semantically rich model interfaces, facilitating coupling models to each other and to protocol descriptions. Finally, we show how these uses of semantic annotation and markup languages may be mixed incrementally with legacy code. This work suggests that the ideas we have developed have the potential to be useful across computational science, and we discuss these wider implications.
Keywords: Functional curation; Chaste; SED-ML

Zhoupeng Han, Rong Mo, Haicheng Yang, Li Hao,
CAD assembly model retrieval based on multi-source semantics information and weighted bipartite graph,
Computers in Industry,
Volume 96,
2018,
Pages 54-65,
ISSN 0166-3615,
https://doi.org/10.1016/j.compind.2018.01.003.
(https://www.sciencedirect.com/science/article/pii/S0166361517303780)
Abstract: Content-based CAD assembly model retrieval focusing on similarity measure of bottom-level geometry and topology information can hardly meet design demand of engineering designers in the product design reuse. To search quickly and effectively the CAD assembly models with specific functions by using requirements of mechanical product design, inspire designers’ design thought and product design innovation, an assembly retrieval approach based on multi-source semantics information and weighted bipartite graph is proposed. The approach concerns the high-level semantics information rather than geometry and topology information of assembly model. First, the semantic ontology for mechanical assembly model is constructed considering multi-source semantics information including part semantics, assembly constraint semantics and functional semantics. Function region extraction algorithm is given for identifying functional structures in assembly model, and functional semantics of assembly model is annotated from the aspects of assembly structure granularity and part shape granularity. Subsequently, semantic similarity between concepts is evaluated comprehensively with WordNet and domain ontology, and a semantic matching model is built based on weighted bipartite graph (WBG) for solving the many-to-many semantics matching problem of semantic retrieval for CAD assembly models. Finally, a semantic-based assembly retrieval prototype system is developed and the experiment cases verify effectiveness and feasibility of the proposed assembly retrieval approach.
Keywords: Assembly model retrieval; Multi-source semantics information; Function region; Semantic similarity evaluation; Semantic matching; Weighted bipartite graph

Achille Souili, Denis Cavallucci, François Rousselot, Cecilia Zanni,
Starting from Patents to Find Inputs to the Problem Graph Model of IDM-TRIZ,
Procedia Engineering,
Volume 131,
2015,
Pages 150-161,
ISSN 1877-7058,
https://doi.org/10.1016/j.proeng.2015.12.365.
(https://www.sciencedirect.com/science/article/pii/S1877705815042496)
Abstract: TRIZ theory is primarily based on patent's observation and one of its extensions for complex situations is known as Inventive Design Method (IDM). This paper proposes a new approach of automatic retrieval of IDM concepts from patents. It mainly consists of using generic linguistic markers to locate and extract IDM related knowledge, such as problems, partial solutions and parameters, to automatically populate IDM Ontology. Lastly, a comparison is made between respective results of manual and automatic retrieval to assess the current accuracy of our approach.
Keywords: Patent mining; Knowledge engineering; Ontology; Inventive design

Jorge Martinez-Gil, Jose Manuel Chaves-Gonzalez,
Interpretable ontology meta-matching in the biomedical domain using Mamdani fuzzy inference,
Expert Systems with Applications,
Volume 188,
2022,
116025,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2021.116025.
(https://www.sciencedirect.com/science/article/pii/S0957417421013713)
Abstract: Ontology meta-matching techniques have been consolidated as one of the best approaches to face the problem of discovering semantic relationships between knowledge models that belong to the same domain but have been developed independently. After more than a decade of research, the community has reached a stage of maturity characterized by increasingly better results and aspects such as the robustness and scalability of solutions have been solved. However, the resulting models remain practically intelligible to a human operator. In this work, we present a novel approach based on Mamdani fuzzy inference exploiting a model very close to natural language. This fact has a double objective: to achieve results with high degrees of accuracy but at the same time to guarantee the interpretability of the resulting models. After validating our proposal with several ontological models popular in the biomedical field, we can conclude that the results obtained are promising.
Keywords: Knowledge engineering; Mamdani inference; Biomedical ontologies; Biomedical ontology matching


C.A. Iglesias, J.F. Sánchez-Rada, G. Vulcu, P. Buitelaar,
Chapter 4 - Linked Data Models for Sentiment and Emotion Analysis in Social Networks,
Editor(s): Federico Alberto Pozzi, Elisabetta Fersini, Enza Messina, Bing Liu,
Sentiment Analysis in Social Networks,
Morgan Kaufmann,
2017,
Pages 49-69,
ISBN 9780128044124,
https://doi.org/10.1016/B978-0-12-804412-4.00004-8.
(https://www.sciencedirect.com/science/article/pii/B9780128044124000048)
Abstract: Language resource interoperability is still a major challenge in sentiment analysis. One of the current trends for solving this issue is the adoption of a linked data perspective for semantically modeling, interlinking, and publishing lexical and other linguistic resources. This chapter contributes to the development of the linguistic linked open data through a linked data model for sentiment and emotion analysis in social networks that is based on two vocabularies: Marl and Onyx for sentiment and emotion modeling respectively. These vocabularies are used for (1) affective corpus annotation, (2) affective lexicon annotation, and (3) sentiment and emotion services interoperability. Several aspects of the solution are discussed, such as the transformation of legacy resources, the generation of domain-specific sentiment lexicons, and the benefits of interlinking language resources for sentiment analysis with other resources such as WordNet or DBpedia.
Keywords: Linguistic linked open data; Ontology; Corpus annotation; Marl; Onyx; Lemon; Emotion analysis; Sentiment analysis

Ralph Bergmann, Yolanda Gil,
Similarity assessment and efficient retrieval of semantic workflows,
Information Systems,
Volume 40,
2014,
Pages 115-127,
ISSN 0306-4379,
https://doi.org/10.1016/j.is.2012.07.005.
(https://www.sciencedirect.com/science/article/pii/S0306437912001020)
Abstract: In the recent years, the use of workflows has significantly expanded from its original domain of business processes towards new areas. The increasing demand for individual and more flexible workflows asks for new methods that support domain experts to create, monitor, and adapt workflows. The emergent field of process-oriented case-based reasoning addresses this problem by proposing methods for reasoning with workflows based on experience. New workflows can be constructed by reuse of already available similar workflows from a repository. Hence, methods for the similarity assessment of workflows and for the efficient retrieval of similar workflows from a repository are of core importance. To this end, we describe a new generic model for representing workflows as semantically labeled graphs, together with a related model for knowledge intensive similarity measures. Further, new algorithms for workflow similarity computation, based on A⁎ search are described. A new retrieval algorithm is introduced that goes beyond traditional sequential retrieval for graphs, interweaving similarity computation with case selection. We describe the application of this model and several experimental evaluations of the algorithms in the domain of scientific workflows and in the domain of business workflows, thereby showing its broad applicability.
Keywords: Business workflows; Scientific workflows; Workflow similarity; Retrieval; Case-based reasoning


Index,
Editor(s): Valentina Emilia Balas, Vijender Kumar Solanki, Raghvendra Kumar, Manju Khari,
Handbook of Data Science Approaches for Biomedical Engineering,
Academic Press,
2020,
Pages 291-303,
ISBN 9780128183182,
https://doi.org/10.1016/B978-0-12-818318-2.20001-9.
(https://www.sciencedirect.com/science/article/pii/B9780128183182200019)

Bo Liu, Ravi K Madduri, Borja Sotomayor, Kyle Chard, Lukasz Lacinski, Utpal J Dave, Jianqiang Li, Chunchen Liu, Ian T Foster,
Cloud-based bioinformatics workflow platform for large-scale next-generation sequencing analyses,
Journal of Biomedical Informatics,
Volume 49,
2014,
Pages 119-133,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2014.01.005.
(https://www.sciencedirect.com/science/article/pii/S1532046414000070)
Abstract: Due to the upcoming data deluge of genome data, the need for storing and processing large-scale genome data, easy access to biomedical analyses tools, efficient data sharing and retrieval has presented significant challenges. The variability in data volume results in variable computing and storage requirements, therefore biomedical researchers are pursuing more reliable, dynamic and convenient methods for conducting sequencing analyses. This paper proposes a Cloud-based bioinformatics workflow platform for large-scale next-generation sequencing analyses, which enables reliable and highly scalable execution of sequencing analyses workflows in a fully automated manner. Our platform extends the existing Galaxy workflow system by adding data management capabilities for transferring large quantities of data efficiently and reliably (via Globus Transfer), domain-specific analyses tools preconfigured for immediate use by researchers (via user-specific tools integration), automatic deployment on Cloud for on-demand resource allocation and pay-as-you-go pricing (via Globus Provision), a Cloud provisioning tool for auto-scaling (via HTCondor scheduler), and the support for validating the correctness of workflows (via semantic verification tools). Two bioinformatics workflow use cases as well as performance evaluation are presented to validate the feasibility of the proposed approach.
Keywords: Bioinformatics; Scientific workflow; Sequencing analyses; Cloud computing; Galaxy

Basra Jehangir, Saravanan Radhakrishnan, Rahul Agarwal,
A survey on Named Entity Recognition — datasets, tools, and methodologies,
Natural Language Processing Journal,
Volume 3,
2023,
100017,
ISSN 2949-7191,
https://doi.org/10.1016/j.nlp.2023.100017.
(https://www.sciencedirect.com/science/article/pii/S2949719123000146)
Abstract: Natural language processing (NLP) is crucial in the current processing of data because it takes into account many sources, formats, and purposes of data as well as information from various sectors of our economy, government, and private and public lives. We perform a variety of NLP operations on the text in order to complete certain tasks. One of them is NER (Named Entity Recognition). An act of recognizing and categorizing named entities that are presented in a text document is known as named entity recognition. The purpose of NER is to find references of rigid designators in the text which belong to established semantic kinds like a person, place, organization, etc. It acts as a cornerstone for many Information Extraction-related activities. In this work, we present a thorough analysis of several methodologies for NER ranging from unsupervised learning, rule-based, supervised learning, and various Deep Learning based approaches. We examine the most relevant datasets, tools, and deep learning approaches like Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Bidirectional Long Short Term Memory, Transfer learning approaches, and numerous other approaches currently being used in present-day NER problem environments and their applications. Finally, we outline the difficulties NER systems encounter and future directions.
Keywords: Natural language processing; Named Entity Recognition; Deep Learning; Convolutional Neural Network; Bidirectional Long Short Term Memory; Recurrent Neural Networks

N. Zeni, E.A. Seid, P. Engiel, J. Mylopoulos,
NómosT: Building large models of law with a tool-supported process,
Data & Knowledge Engineering,
Volume 117,
2018,
Pages 407-418,
ISSN 0169-023X,
https://doi.org/10.1016/j.datak.2018.04.009.
(https://www.sciencedirect.com/science/article/pii/S0169023X18301733)
Abstract: Laws and regulations impact the design of software systems, as they introduce new requirements and constrain existing ones. The analysis of a software system and the degree to which it complies with applicable laws can be greatly facilitated by models of applicable laws. However, laws are inherently voluminous, often consisting of hundreds of pages of text, and so are their models, consisting of thousands of concepts and relationships. This paper studies the possibility of building models of law semi-automatically by using the NómosT tool. Specifically, we present the NómosT architecture and the process by which a user constructs a model of law semi-automatically, by first annotating the text of a law and then generating from it a model. We then evaluate the performance of the tool relative to building a model of a fragment of law manually. In addition, we offer statistics on the quality of the final output that suggest that tool supported generation of models of law reduces substantially human effort without affecting the quality of the output.

Amjad Fayoumi, Richard Williams,
An integrated socio-technical enterprise modelling: A scenario of healthcare system analysis and design,
Journal of Industrial Information Integration,
Volume 23,
2021,
100221,
ISSN 2452-414X,
https://doi.org/10.1016/j.jii.2021.100221.
(https://www.sciencedirect.com/science/article/pii/S2452414X21000212)
Abstract: One of the crucial issues facing enterprise modelling (EM) practices is that EM is considered technical, and rarely or never has a social focus. Social aspects referred to here are the soft aspects of the organisation that lead to organic organisation development (communication, collaboration, culture, skills and personal goals). There are many EM approaches and enterprise architecture frameworks were proposed recently. These cover different enterprise aspects, perspectives, artefacts and models with different qualities and levels of details. Yet, the imperative determination has overlaid the declarative exploration in EM as a necessity of the design effort. Rethinking the assumptions underlying EM should bring a new and different understanding on how EM can be tackled within the enterprise, in particular the joint development and optimisation of socio-technical systems. This paper discusses EM from a socio-technical systems (STS) perspective, and towards forming a new model of EM that is driven from STS theory and combined with STS practices. Then proposes a conceptual integrated model that incorporates the new concepts of STS toward building an EM framework for balanced socio-technical joint development and optimisation. The approach is illustrated in a scenario from healthcare industry. A combination between modelling and STS practices proved powerful for holistic IT modernisation, future work discussed toward the end of the paper.
Keywords: Enterprise Modelling; Socio-technical Systems; Enterprise Integrated Model; Conceptual Modelling; Healthcare System

Alberto Anguita, Miguel García-Remesal, Norbert Graf, Victor Maojo,
A method and software framework for enriching private biomedical sources with data from public online repositories,
Journal of Biomedical Informatics,
Volume 60,
2016,
Pages 177-186,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2016.02.004.
(https://www.sciencedirect.com/science/article/pii/S1532046416000265)
Abstract: Modern biomedical research relies on the semantic integration of heterogeneous data sources to find data correlations. Researchers access multiple datasets of disparate origin, and identify elements—e.g. genes, compounds, pathways—that lead to interesting correlations. Normally, they must refer to additional public databases in order to enrich the information about the identified entities—e.g. scientific literature, published clinical trial results, etc. While semantic integration techniques have traditionally focused on providing homogeneous access to private datasets—thus helping automate the first part of the research, and there exist different solutions for browsing public data, there is still a need for tools that facilitate merging public repositories with private datasets. This paper presents a framework that automatically locates public data of interest to the researcher and semantically integrates it with existing private datasets. The framework has been designed as an extension of traditional data integration systems, and has been validated with an existing data integration platform from a European research project by integrating a private biological dataset with data from the National Center for Biotechnology Information (NCBI).
Keywords: Semantic integration; RDF; Public databases

Gen Li,
Generalized Co-clustering Analysis via Regularized Alternating Least Squares,
Computational Statistics & Data Analysis,
Volume 150,
2020,
106989,
ISSN 0167-9473,
https://doi.org/10.1016/j.csda.2020.106989.
(https://www.sciencedirect.com/science/article/pii/S0167947320300803)
Abstract: Biclustering is an important exploratory analysis tool that simultaneously clusters rows (e.g., samples) and columns (e.g., variables) of a data matrix. Checkerboard-like biclusters reveal intrinsic associations between rows and columns. However, most existing methods rely on Gaussian assumptions and only apply to matrix data. In practice, non-Gaussian and/or multi-way tensor data are frequently encountered. A new CO-clustering method via Regularized Alternating Least Squares (CORALS) is proposed, which generalizes biclustering to non-Gaussian data and multi-way tensor arrays. Non-Gaussian data are modeled with single-parameter exponential family distributions and co-clusters are identified in the natural parameter space via sparse CANDECOMP/PARAFAC tensor decomposition. A regularized alternating (iteratively reweighted) least squares algorithm is devised for model fitting and a deflation procedure is exploited to automatically determine the number of co-clusters. Comprehensive simulation studies and three real data examples demonstrate the efficacy of the proposed method. The data and code are publicly available at https://github.com/reagan0323/CORALS.
Keywords: Exponential family; Biclustering; Generalized Linear Model; Parafac/Candecomp; Tensor

Oyawale Adetunji Moses, Wei Chen, Mukhtar Lawan Adam, Zhuo Wang, Kaili Liu, Junming Shao, Zhengsheng Li, Wentao Li, Chensu Wang, Haitao Zhao, Cheng Heng Pang, Zongyou Yin, Xuefeng Yu,
Integration of data-intensive, machine learning and robotic experimental approaches for accelerated discovery of catalysts in renewable energy-related reactions,
Materials Reports: Energy,
Volume 1, Issue 3,
2021,
100049,
ISSN 2666-9358,
https://doi.org/10.1016/j.matre.2021.100049.
(https://www.sciencedirect.com/science/article/pii/S2666935821000847)
Abstract: Technological advancements in recent decades have greatly transformed the field of material chemistry. Juxtaposing the accentuating energy demand with the pollution associated, urgent measures are required to ensure energy maximization, while reducing the extended experimental time cycle involved in energy production. In lieu of this, the prominence of catalysts in chemical reactions, particularly energy related reactions cannot be undermined, and thus it is critical to discover and design catalyst, towards the optimization of chemical processes and generation of sustainable energy. Most recently, artificial intelligence (AI) has been incorporated into several fields, particularly in advancing catalytic processes. The integration of intensive data set, machine learning models and robotics, provides a very powerful tool in modifying material synthesis and optimization by generating multifarious dataset amenable with machine learning techniques. The employment of robots automates the process of dataset and machine learning models integration in screening intermetallic surfaces of catalyst, with extreme accuracy and swiftness comparable to a number of human researchers. Although, the utilization of robots in catalyst discovery is still in its infancy, in this review we summarize current sway of artificial intelligence in catalyst discovery, briefly describe the application of databases, machine learning models and robots in this field, with emphasis on the consolidation of these monomeric units into a tripartite flow process. We point out current trends of machine learning and hybrid models of first principle calculations (DFT) for generating dataset, which is integrable into autonomous flow process of catalyst discovery. Also, we discuss catalyst discovery for renewable energy related reactions using this tripartite flow process with predetermined descriptors.
Keywords: Material chemistry; Sustainable energy; Artificial intelligence; Machine learning models; Robots; Catalyst discovery; Intensive dataset

Alan De Renzis, Martin Garriga, Andres Flores, Alejandra Cechich, Cristian Mateos, Alejandro Zunino,
A domain independent readability metric for web service descriptions,
Computer Standards & Interfaces,
Volume 50,
2017,
Pages 124-141,
ISSN 0920-5489,
https://doi.org/10.1016/j.csi.2016.09.005.
(https://www.sciencedirect.com/science/article/pii/S0920548916300782)
Abstract: Web Services are influencing most IT-based industries as the basic building block of business infrastructures. A Web Service has an interface described in a machine-processable format (specifically WSDL). Service providers expose their services by publishing the corresponding WSDL documents. Service consumers can learn about service capability and how to interact with the services. Service descriptions (WSDL documents) should be ideally understood easily by service stakeholders so that the process of consuming services is simplified. In this work we present a practical metric to quantify readability in WSDL documents – attending to their semantics by using WordNet as the underlying concept hierarchy. In addition, we propose a set of best practices to be used during the development of WSDL documents to improve their readability. To validate our proposals, we performed both qualitative and quantitative experiments. A controlled survey with a group of (human) service consumers showed that software engineers required less time and effort to analyze WSDL documents with higher readability values. Other experiment compares readability values of a dataset of real-life WSDL documents from the industry before and after modifying them to adhere to the readability best practices proposed in this paper. We detected a significant readability improvement for WSDL documents written according to the best practices. In another experiment, we applied existing readability metrics for natural language texts detecting their unsuitability to the Web Service context. Lastly, we analyzed the readability best practices identifying their useful applicability to the industry.
Keywords: Readability; Web service descriptions; WordNet; Domain independent

Carlos Coutinho, Adina Cretan, Ricardo Jardim-Goncalves,
Sustainable interoperability on space mission feasibility studies,
Computers in Industry,
Volume 64, Issue 8,
2013,
Pages 925-937,
ISSN 0166-3615,
https://doi.org/10.1016/j.compind.2013.06.016.
(https://www.sciencedirect.com/science/article/pii/S0166361513001309)
Abstract: The evolution of ICT towards fast and robust data exchange promoted the blooming of successful technical and business concepts like cloud computing and virtualisation of enterprise assets. The escalation of service providers enabled the specialisation of enterprises (particularly SMEs) and the building of provider networks. This move from enterprise-concentric to service-dispersed strategies is leading to concerns about reaching and maintaining the interoperability between customer–provider pairs and their associated business networks. This is particularly true in the aerospace industry: a highly-competitive and demanding business supported by numerous applications, which may be general-purposed, specific, proprietary or open-sourced, all needing to be interoperable with the lowest impact on the business itself. This paper addresses the need for improving the sustainability of enterprise interoperability via the application of negotiations, with the objective of reducing the impact of changes and achieving the best solutions in the interoperability between the enterprises and their surrounding environment. It proposes a framework that features a negotiation mechanism for the management of changes towards the sustainability of the seamless business-to-business interactions, and describes its application on the real business case of the ESA-CDF space mission feasibility studies.
Keywords: Sustainable; Enterprise; Interoperability; Negotiation; Ontology; Framework

Fernando Romero, Emilio M. Sanfilippo, Pedro Rosado, Stefano Borgo, Sergio Benavent,
Feature in product engineering with single and variant design approaches. A comparative review,
Procedia Manufacturing,
Volume 41,
2019,
Pages 328-335,
ISSN 2351-9789,
https://doi.org/10.1016/j.promfg.2019.09.016.
(https://www.sciencedirect.com/science/article/pii/S2351978919311035)
Abstract: The paper contributes to the study of feature concept by analyzing its use in techniques and models of both classical product engineering and product engineering with variants. In particular, the latter field broadens the classical modeling problem via the introduction of concepts like product family and product line. Until today the literature dedicated to classical product engineering has focused on the study of the so-called engineering features, e.g. form or functional features, and has ignored the larger class of features used in techniques related to the analysis and design of products with variants. This work intends to overcome this deficit by providing a broader vision of the problem and by pointing out the need for a renewed analysis and classification of feature definitions based on their originating design context. The work is part of a research project which aims to provide a conceptual framework to unify multiple feature notions and develop feature-based methodologies and applications to support product design.
Keywords: product variant modelling; feature models; feature definitons; classical engineering; domain engineering

Mariana Neves, Ulf Leser,
Question answering for Biology,
Methods,
Volume 74,
2015,
Pages 36-46,
ISSN 1046-2023,
https://doi.org/10.1016/j.ymeth.2014.10.023.
(https://www.sciencedirect.com/science/article/pii/S1046202314003491)
Abstract: Biologists often pose queries to search engines and biological databases to obtain answers related to ongoing experiments. This is known to be a time consuming, and sometimes frustrating, task in which more than one query is posed and many databases are consulted to come to possible answers for a single fact. Question answering comes as an alternative to this process by allowing queries to be posed as questions, by integrating various resources of different nature and by returning an exact answer to the user. We have surveyed the current solutions on question answering for Biology, present an overview on the methods which are usually employed and give insights on how to boost performance of systems in this domain.
Keywords: Question answering; Biomedicine; Natural language processing; Data integration

Yoan Gutiérrez, Sonia Vázquez, Andrés Montoyo,
A semantic framework for textual data enrichment,
Expert Systems with Applications,
Volume 57,
2016,
Pages 248-269,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2016.03.048.
(https://www.sciencedirect.com/science/article/pii/S0957417416301439)
Abstract: In this work we present a semantic framework suitable of being used as support tool for recommender systems. Our purpose is to use the semantic information provided by a set of integrated resources to enrich texts by conducting different NLP tasks: WSD, domain classification, semantic similarities and sentiment analysis. After obtaining the textual semantic enrichment we would be able to recommend similar content or even to rate texts according to different dimensions. First of all, we describe the main characteristics of the semantic integrated resources with an exhaustive evaluation. Next, we demonstrate the usefulness of our resource in different NLP tasks and campaigns. Moreover, we present a combination of different NLP approaches that provide enough knowledge for being used as support tool for recommender systems. Finally, we illustrate a case of study with information related to movies and TV series to demonstrate that our framework works properly.
Keywords: Recommender systems; Framework; Integrated semantic resources; Sentiment analysis; Word Sense Disambiguation; Content categorisation

Fabio Gasparetti, Carlo De Medio, Carla Limongelli, Filippo Sciarrone, Marco Temperini,
Prerequisites between learning objects: Automatic extraction based on a machine learning approach,
Telematics and Informatics,
Volume 35, Issue 3,
2018,
Pages 595-610,
ISSN 0736-5853,
https://doi.org/10.1016/j.tele.2017.05.007.
(https://www.sciencedirect.com/science/article/pii/S0736585316304890)
Abstract: One standing problem in the area of web-based e-learning is how to support instructional designers to effectively and efficiently retrieve learning materials, appropriate for their educational purposes. Learning materials can be retrieved from structured repositories, such as repositories of Learning Objects and Massive Open Online Courses; they could also come from unstructured sources, such as web hypertext pages. Platforms for distance education often implement algorithms for recommending specific educational resources and personalized learning paths to students. But choosing and sequencing the adequate learning materials to build adaptive courses may reveal to be quite a challenging task. In particular, establishing the prerequisite relationships among learning objects, in terms of prior requirements needed to understand and complete before making use of the subsequent contents, is a crucial step for faculty, instructional designers or automated systems whose goal is to adapt existing learning objects to delivery in new distance courses. Nevertheless, this information is often missing. In this paper, an innovative machine learning-based approach for the identification of prerequisites between text-based resources is proposed. A feature selection methodology allows us to consider the attributes that are most relevant to the predictive modeling problem. These features are extracted from both the input material and weak-taxonomies available on the web. Input data undergoes a Natural language process that makes finding patterns of interest more easy for the applied automated analysis. Finally, the prerequisite identification is cast to a binary statistical classification task. The accuracy of the approach is validated by means of experimental evaluations on real online coursers covering different subjects.
Keywords: Curriculum sequencing; E-learning; Learning object; Machine learning; Prerequisite

Zhihui Luo, Riccardo Miotto, Chunhua Weng,
A human–computer collaborative approach to identifying common data elements in clinical trial eligibility criteria,
Journal of Biomedical Informatics,
Volume 46, Issue 1,
2013,
Pages 33-39,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2012.07.006.
(https://www.sciencedirect.com/science/article/pii/S1532046412001037)
Abstract: Objective
To identify Common Data Elements (CDEs) in eligibility criteria of multiple clinical trials studying the same disease using a human–computer collaborative approach.
Design
A set of free-text eligibility criteria from clinical trials on two representative diseases, breast cancer and cardiovascular diseases, was sampled to identify disease-specific eligibility criteria CDEs. In this proposed approach, a semantic annotator is used to recognize Unified Medical Language Systems (UMLSs) terms within the eligibility criteria text. The Apriori algorithm is applied to mine frequent disease-specific UMLS terms, which are then filtered by a list of preferred UMLS semantic types, grouped by similarity based on the Dice coefficient, and, finally, manually reviewed.
Measurements
Standard precision, recall, and F-score of the CDEs recommended by the proposed approach were measured with respect to manually identified CDEs.
Results
Average precision and recall of the recommended CDEs for the two diseases were 0.823 and 0.797, respectively, leading to an average F-score of 0.810. In addition, the machine-powered CDEs covered 80% of the cardiovascular CDEs published by The American Heart Association and assigned by human experts.
Conclusion
It is feasible and effort saving to use a human–computer collaborative approach to augment domain experts for identifying disease-specific CDEs from free-text clinical trial eligibility criteria.
Keywords: Clinical research informatics; Clinical trial eligibility criteria; Common data elements; Knowledge management; Human–computer collaboration; Text mining

Marieke M.M. Peeters, Maaike Harbers, Mark A. Neerincx,
Designing a personal music assistant that enhances the social, cognitive, and affective experiences of people with dementia,
Computers in Human Behavior,
Volume 63,
2016,
Pages 727-737,
ISSN 0747-5632,
https://doi.org/10.1016/j.chb.2016.06.003.
(https://www.sciencedirect.com/science/article/pii/S0747563216304319)
Abstract: Research shows that music with a strong personal meaning can enhance the social, cognitive, and affective experiences of both people with dementia (PwD) and their social environment. We applied a human-centred design method, called situated Cognitive Engineering, to develop the conceptual design and design rationale of the Music ePartner. The design rationale specifies the general knowledge-base (ontology), context (use cases), and expected effects (claims) of the ePartner support. Three functionalities were developed through rapid prototyping: (1) annotated play lists, (2) a music and picture album, and (3) a picture slide show. Accompanied by a close relative, five PwD participated in a formative evaluation of the prototype at their regular day care centres. All participants interacted with all three functionalities of the prototype as they would in their natural setting. The researchers observed participants’ responses to the prototype using observational scoring forms, and interviewed participants about their experiences using semi-structured interviews. Results showed that the music stimulated PwD to tell life stories related to the songs. Furthermore, music evoked positive individual and group experiences. Specific constraints, additional user needs, and interaction requirements for the Music ePartner resulted in a refinement of both the requirements baseline and the design rationale.
Keywords: Dementia; Music; Personalisation; Design; Experience sharing; ePartner

Anna Masłowska-Górnicz, Melanie R.M. van den Bosch, Edoardo Saccenti, Maria Suarez-Diez,
A large-scale analysis of codon usage bias in 4868 bacterial genomes shows association of codon adaptation index with GC content, protein functional domains and bacterial phenotypes,
Biochimica et Biophysica Acta (BBA) - Gene Regulatory Mechanisms,
Volume 1865, Issue 6,
2022,
194826,
ISSN 1874-9399,
https://doi.org/10.1016/j.bbagrm.2022.194826.
(https://www.sciencedirect.com/science/article/pii/S1874939922000414)
Abstract: Multiple synonymous codons code for the same amino acid, resulting in the degeneracy of the genetic code and in the preferred used of some codons called codon bias usage (CBU). We performed a large-scale analysis of codon usage bias analysing the distribution of the codon adaptation index (CAI) and the codon relative adaptiveness index (RA) in 4868 bacterial genomes. We found that CAI values differ significantly between protein functional domains and part of the protein outside domains and show how CAI, GC content and preferred usage of polymerase III alpha subunits are related. Additionally, we give evidence of the association between CAI and bacterial phenotypes.
Keywords: Bacterial genome; Codon adaptiveness; Evolution

Muhammad Arslan, Christophe Cruz, Dominique Ginhac,
Semantic Enrichment of Spatio-temporal Trajectories for Worker Safety on Construction Sites,
Procedia Computer Science,
Volume 130,
2018,
Pages 271-278,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2018.04.039.
(https://www.sciencedirect.com/science/article/pii/S1877050918303909)
Abstract: Existing literature reveals that major reasons of fatalities on construction sites are linked to mobility related issues such as unsafe human behaviors, difficult site conditions and workers striking against or being struck by the moving objects. In an attempt to reduce such fatalities, a system is proposed to enrich spatio-temporal trajectories of construction workers. Three semantic enrichment techniques are used to construct semantically enriched trajectories which are: (1) enrichment with semantic points that maps site location identification to trajectory points; (2) enrichment with semantic lines that relies on the speed based segmentation approach infers modes of transportation used in trajectory‘s episodes; and (3) enrichment with semantic region for mapping a complete trajectory on an actual construction site zone. Constructed semantic trajectories will help Health and Safety (H&S) managers in making improved decisions for monitoring and controlling site activities by understanding workers behaviors that ultimately attempts to contribute in reducing fatal accidents occurring on construction sites each year.
Keywords: : Safety; workers; construction; spatio-temporal data; fatalities

Juan J. Lastra-Díaz, Ana García-Serrano,
A novel family of IC-based similarity measures with a detailed experimental survey on WordNet,
Engineering Applications of Artificial Intelligence,
Volume 46, Part A,
2015,
Pages 140-153,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2015.09.006.
(https://www.sciencedirect.com/science/article/pii/S0952197615002067)
Abstract: This paper introduces a novel family of ontology-based similarity measures based on the Information Content (IC) theory, a detailed state of the art, a large experimental survey into ontology-based similarity measures on WordNet, and a new comparison between intrinsic and corpus-based IC models. Our experiments are based on our implementation of a large set of similarity measures, intrinsic and corpus-based IC models, which are evaluated on two known datasets and three different WordNet versions. The new measures are called weighted Jiang–Conrath distance (wJ&Cdist) and similarity (wJ&Csim), cosine-normalized Jiang–Conrath similarity (cosJ&Csim) and cosine-normalized weighted Jiang–Conrath similarity (coswJ&Csim). Two of our similarity measures outperform the state-of-the-art measures on the RG65 dataset, and one of them obtains the third overall score on all the datasets and evaluated WordNet versions. The cosine-normalized similarity measures are a non-linear normalization of the classic Jiang–Conrath (J&C) distance and the new wJ&C distance. On the other hand, the wJ&C distance is a generalization of the classic J&C distance which is based on the length of the shortest path between concepts within an IC-based weighted graph. Our measures are based on two not previously considered notions: (1) a generalization of the classic J&C distance to any type of taxonomy, based on an IC-based weighted graph derived from the conditional probabilities between child and parent concepts, and (2) a non-linear normalization function that converts the ontology-based semantic distances into similarity functions. Finally, the corpus-based IC models based on the Resnik method obtain rivaling results as regards the state-of-the-art intrinsic IC models, when they are used with some unexplored WordNet-based frequency files. Therefore, this latter fact allows us to reconsider some previous conclusions about the outperformance of the intrinsic IC models over the corpus-based ones.
Keywords: Ontology-based semantic similarity measures; IC-based measures; Semantic similarity; Intrinsic and corpus-based information content models; Jiang–Conrath distance; Semantic similarity on WordNet survey

T B Lalitha, P S Sreeja,
Personalised Self-Directed Learning Recommendation System,
Procedia Computer Science,
Volume 171,
2020,
Pages 583-592,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2020.04.063.
(https://www.sciencedirect.com/science/article/pii/S1877050920310309)
Abstract: Modern educational systems have changed drastically bringing in knowledge anywhere as needed by the learner with the evolution of Internet. Availability of knowledge in public domain, capability of exchanging large amount of information and filtering relevant information quickly has enabled disruption to conventional educational system. Thus, future trends are looking towards E-Learning (Electronic Learning) and M-Learning (Mobile Learning) technologies over the Internet for their vast knowledge acquisition. In this paper, the work gives an elaborate context of learning strategies prevailing and emerging with the classification of e-learning Techniques. It majorly focuses on the features and variety of aspects with the e-learning and the choice of learning method involved and facilitate the adoption of new ways for personalized selection on learning resources for SDL (Self-Directed Learning) from the unstructured, large web-based environment. Thereby, proposes a Personalised Self-Directed Learning Recommendation System (PSDLR) based on the personal specifications of the SDL learner. The result offers insight into the perspectives and challenges of Self-Directed Learning based on cognitive and constructive characteristics which majorly incorporates web-based learning and gives path in finding appropriate solutions using machine learning techniques and ontology for the open problems in the respective fields with personalised recommendations and guidelines for future research.
Keywords: e-Learning; PSDLR; Recommendation System; SDL; Self-Directed Learning

Rusne Sileryte, Arnout Sabbe, Vasileios Bouzas, Kozmo Meister, Alexander Wandl, Arjan van Timmeren,
European Waste Statistics data for a Circular Economy Monitor: Opportunities and limitations from the Amsterdam Metropolitan Region,
Journal of Cleaner Production,
Volume 358,
2022,
131767,
ISSN 0959-6526,
https://doi.org/10.1016/j.jclepro.2022.131767.
(https://www.sciencedirect.com/science/article/pii/S0959652622013786)
Abstract: As appointed in the EU Circular Economy Action Plan, cities and regions in EU member countries start accompanying their circular economy strategies by monitoring frameworks, often called Circular Economy Monitors (CEM). Having the task to assess the performance towards the achievement of set targets and to steer decision-making, CEMs need to rely on a multitude of statistics and datasets. Waste statistics play an important role in circular economy monitoring as they provide insights into the remaining linear part of the economy. The collection of waste statistics is mandated by the European Commission which provides general guidelines on data collection and processing. The Netherlands has one of the most detailed waste registries among the EU countries. The country’s largest metropolitan region, Amsterdam, is currently building a CEM which tracks progress over time towards the set goals, highlights which areas need improvement and estimates target feasibility. This paper uses the Amsterdam CEM as a case-study to explore how the existing system of waste registration in the Netherlands is able to support decision-making. The data is explored with the help of four queries that relate to the CEM’s goals and require data mapping to be answered. The data mapping and analysis process has revealed several limitations present in the waste data collection and a number of gaps present in current circular economy research and data analysis. At the same time, the available data already supports significant insights into the status quo of the current waste system and provides opportunities for circular economy monitoring.
Keywords: Circular Economy Monitor; European Waste Statistics; Amsterdam Metropolitan Region; Circular Economy Action Plan; Waste mapping

Achille Souili, Denis Cavallucci, François Rousselot,
Identifying and Reformulating Knowledge Items to Fit with the Inventive Design Method (IDM) Model for a Semantically-based Patent Mining,
Procedia Engineering,
Volume 131,
2015,
Pages 1130-1139,
ISSN 1877-7058,
https://doi.org/10.1016/j.proeng.2015.12.432.
(https://www.sciencedirect.com/science/article/pii/S1877705815043246)
Abstract: The analysis of initial situation is necessary when dealing with the complex engineering situation. Furthermore, innovation is the underlying foundation of technological advances and today's competitive economy. Patents contain important research and it is also possible to find within them a kind of history of the evolution of an artifact. Used effectively, patents can help to provide businesses and individuals work with innovative ideas. In this context, the engineers may very often need to analyze them in order to benefit from the knowledge contained therein to organize their inventive task. However, patents are lengthy and rich in technical terminology; analyzing patents may require a lot of human effort. Thus, automating the process is very timely. To facilitate this, several patent analysis tools have been created. There are also patent analysis tools dedicated to highlighting various values of tools, but very few of them are designed to extract information or knowledge contained in patents. This paper presents an ongoing research on knowledge extraction for the Inventive Design Method (IDM), which extends from TRIZ, the theory of inventive problem solving.
Keywords: Knowledge discovery; Patent mining; NLP; Text-mining; IDM; TRIZ

Qingyuan Zhou, Zheng Xu, Neil Y. Yen,
User sentiment analysis based on social network information and its application in consumer reconstruction intention,
Computers in Human Behavior,
Volume 100,
2019,
Pages 177-183,
ISSN 0747-5632,
https://doi.org/10.1016/j.chb.2018.07.006.
(https://www.sciencedirect.com/science/article/pii/S0747563218303285)
Abstract: Due to the increasingly fierce competition in the consumer goods market, customer retention strategies are of great importance for enterprises to maintain their dominance and long-term and stable earnings. Understanding customer repurchase intention and re-patronage is the prerequisite and foundation for business and retailers. Online reviews include ratings and emotional information from many customers on brands and online stores. Consumer repurchase intention can be measured by mining the attitudes and emotions of consumers in the reviews. Based on online reviews, this work used textual sentiment calculation and fuzzy mathematics to study the online repurchase intention of online consumers. Taking satisfaction, trust and promotion efforts as the antecedents, and consumer repurchase intention as the consequent, a model was established based on emotional computing and fuzzy reasoning. Through the satisfaction, trust and promotion effort of five sportswear brands in Taobao, we verified the reasoning for determining consumer repurchase intention of products. Meanwhile, the relationship between the initial purchase intention and repurchase intention of consumers was compared, thus providing the basis for online stores to formulate their marketing strategy and brand segmentation.
Keywords: Social network information; Emotion calculation; Network consumption decision; Fuzzy reasoning

Cheng Zhang, Chao Fan, Wenlin Yao, Xia Hu, Ali Mostafavi,
Social media for intelligent public information and warning in disasters: An interdisciplinary review,
International Journal of Information Management,
Volume 49,
2019,
Pages 190-207,
ISSN 0268-4012,
https://doi.org/10.1016/j.ijinfomgt.2019.04.004.
(https://www.sciencedirect.com/science/article/pii/S0268401218310995)
Abstract: Social media offers participatory and collaborative structure and collective knowledge building capacity to the public information and warning approaches. Therefore, the author envisions the intelligent public information and warning in disaster based on social media, which has three functions: (1) efficiently and effectively acquiring disaster situational awareness information, (2) supporting self-organized peer-to-peer help activities, and (3) enabling the disaster management agencies to hear from the public. To achieve this vision, authors of this study examined 304 studies conducted 2008 through 2018 to systemically evaluate the current literature in understanding the phenomena of communication on social media and the state-of-the-art studies on social media informatics in disasters. This review then identified the challenges of existing studies and proposed a research roadmap to address the challenges of achieving the vision. This review could serve as a bridge for researchers working on social media in disasters to understand the state-of-the-art of this problem in other related domains. The findings of this review highlight the value of certain research areas, e.g., (1) a fine-grained disaster social media ontology with semantic interoperability, (2) network pattern of trending information and emerging influential users, (3) fine-grained assessment of societal impacts due to infrastructure disruptions, and (4) best practices for social media usage during disasters.
Keywords: Social media; Disaster; Information dissemination; Social sensing

Juan Ye, Graeme Stevenson, Simon Dobson,
KCAR: A knowledge-driven approach for concurrent activity recognition,
Pervasive and Mobile Computing,
Volume 19,
2015,
Pages 47-70,
ISSN 1574-1192,
https://doi.org/10.1016/j.pmcj.2014.02.003.
(https://www.sciencedirect.com/science/article/pii/S1574119214000297)
Abstract: Recognising human activities from sensors embedded in an environment or worn on bodies is an important and challenging research topic in pervasive computing. Existing work on activity recognition is mainly concerned with identifying single user sequential activities from well-scripted or pre-segmented sequences of sensor events. However a real-world environment often contains multiple users, with each performing activities simultaneously, in their own way and with no explicit instructions to follow. Recognising multi-user concurrent activities is challenging, but essential for designing applications for real environments. This paper presents a novel Knowledge-driven approach for Concurrent Activity Recognition (KCAR). Within KCAR, we explore the semantics underlying each sensor event and use semantic dissimilarity to segment a continuous sensor sequence into fragments, each of which corresponds to one ongoing activity. We exploit the Pyramid Match Kernel, with a strength in approximate matching on hierarchical concepts, to recognise activities of varying grained constraints from a potentially noisy sensor sequence. We conduct an empirical evaluation on a large-scale real-world data set that is collected over one year and consists of 2.8 millions of sensor events. Our results demonstrate that KCAR achieves an average recognition accuracy of 91%.
Keywords: Ontologies; Smart home; Concurrent activity recognition; Semantics; Domain knowledge; Pyramid match kernel

Rune Rasmussen, Alistair Barros, Fuguo Wei,
A likelihood-free Bayesian derivation method for service variants,
Journal of Systems and Software,
Volume 143,
2018,
Pages 87-99,
ISSN 0164-1212,
https://doi.org/10.1016/j.jss.2018.05.011.
(https://www.sciencedirect.com/science/article/pii/S0164121218300943)
Abstract: Application programming interfaces (API), allowing systems to be accessed by the services they expose, have proliferated on the Internet and gained strategic interest in the IT industry. However, integration opportunities for larger, enterprise systems are hampered by complex and overloaded operations of their interfaces, having hundreds of parameters and multiple levels of nesting, corresponding to multiple business entities. Static (code) analysis techniques have been proposed to analyse service interfaces of enterprise systems. They support the derivation of business entities and relationships from the parameters of interface operations, allowing the restructure of operations, based on individual entities. In this paper, we extend the repertoire of static interface analysis to derive service variants, whereby subsets of operation parameters correspond to multiple nested business entity subtypes of variants. Specifically, we apply a Monte Carlo sampling method, based on likelihood-free Bayesian sampling, to traverse large parameter spaces, based on higher probabilistic tree search, to efficiently find subsets of parameters related to prospective subtypes. The results demonstrate a method with significant success rates in massive search spaces, as applied to the FedEx Shipment interface whose operations have in excess of 1000 parameters.
Keywords: Service synthesis; Variant derivation; Likelihood-free Bayesian methods

Hend S. Al-Khalifa,
Introduction to the special issue on Arabic NLP: Current state and future challenges,
Journal of King Saud University - Computer and Information Sciences,
Volume 26, Issue 4,
2014,
Pages 355-356,
ISSN 1319-1578,
https://doi.org/10.1016/j.jksuci.2014.10.001.
(https://www.sciencedirect.com/science/article/pii/S1319157814000457)

Javad Chamanara, Birgitta König-Ries,
A conceptual model for data management in the field of ecology,
Ecological Informatics,
Volume 24,
2014,
Pages 261-272,
ISSN 1574-9541,
https://doi.org/10.1016/j.ecoinf.2013.12.003.
(https://www.sciencedirect.com/science/article/pii/S1574954113001246)
Abstract: Conceptual models play an important role in identifying the domain under study and establishing an interoperability framework between different scientific groups and tools working on the same or neighboring domains. The importance comes from the fact that the conceptual models describe the target domain in a technology agnostic manner, using domain terminology, considerations, and rules. In this paper we introduce a highly flexible data and metadata structure for biodiversity (and related fields) information management. The model incorporates important concepts needed to develop a proper domain model for managing biodiversity data, e.g., data, data structure, metadata, metadata structure, and semantic descriptions of model elements. The model is designed in UML using the object oriented analysis paradigms. The data management teams of several large collaborative projects as well as those of two research institutes were actively cooperating in the design of the model, thus ensuring that all aspects relevant for these very different projects and institutions are considered and that a high acceptance of the model will ensue. The model supports and encourages reuse and sharing of different elements, making the cross dataset syntheses, comparison, merging and searches easier. The incorporated semantic package helps to annotate dataset's variables and metadata attributes by means of ontologies, taxonomies or thesauri. These annotations can be used for standardization, localization and also for managing the variety of meanings of same or similar variables among community members. The model is currently undergoing its implementation phase and will replace the model used in the current version of BExIS, a data management platform for biodiversity research, when finished.
Keywords: Conceptual model; Scientific data model; Biodiversity domain model; Standardization of ecological data models; Ecological data management

Halima Ramdani, Armelle Brun, Eric Bonjour, Davy Monticolo,
DEEP, a methodology for entity extraction using organizational patterns: Application to job offers,
Knowledge-Based Systems,
Volume 258,
2022,
109573,
ISSN 0950-7051,
https://doi.org/10.1016/j.knosys.2022.109573.
(https://www.sciencedirect.com/science/article/pii/S0950705122007936)
Abstract: Plain texts written in natural language may have several specific features, such as organizational patterns and an ambiguous and evolving vocabulary. From the literature, entity extraction approaches are not sufficient to consider these specific features jointly. To address this issue, we propose DEEP, a methodology that improves the quality of entity extraction by using organizational patterns through a sequence labelling technique. To this end, DEEP creates a high-quality corpus and relies on an appropriate learning algorithm. DEEP is validated on a real corpus of job offers. Experiments show that (1) considering organizational patterns improves the quality of entity extraction, (2) vocabulary evolution is taken into consideration and ambiguity in vocabulary is reduced, (3) DEEP provides clear guidelines for the creation of a high-quality corpus for entity extraction, (4) the Bidirectional Long Short-Term Memory + Conditional Random Field architecture for sequence labelling is the one that takes the most advantage of the organizational patterns.
Keywords: Entity extraction; Organizational patterns; Methodology; Sequence labelling; Recurrent neural network; Job offer entity extraction

Wei Zhai, Xueyin Bai, Zhong-ren Peng, Chaolin Gu,
From edit distance to augmented space-time-weighted edit distance: Detecting and clustering patterns of human activities in Puget Sound region,
Journal of Transport Geography,
Volume 78,
2019,
Pages 41-55,
ISSN 0966-6923,
https://doi.org/10.1016/j.jtrangeo.2019.05.003.
(https://www.sciencedirect.com/science/article/pii/S0966692318305465)
Abstract: Considering and measuring the similarity of human activities remains challenging. Existing studies of similarity measures based on traditional edit distance (ED), specifically on activity patterns, do not reflect the spatiotemporal characteristics in the measurement model. Additionally, interdependence between activities is ignored in existing multidimensional sequence alignment methods. To address the gap, we initially extend the traditional edit distance to a space-time-weighted edit distance (STW-ED). Specifically, differences in distance and time between activities are considered cost functions in the operation cost calculation (insertion, deletion, and substitution). We advance STW-ED to an augmented space-time-weighted edit distance method (ASTW-ED) that integrates an optimum-trajectory-based multidimensional sequence alignment method (OT-MDSAM) with STW-ED, treating the nonspatiotemporal dimensions as augment factors. In addition, ontology is considered for the similarity measure for nonspatiotemporal dimensions. To show the feasibility of our proposed approach, we conduct an empirical study based on an activity-based travel survey in the Puget Sound Region. Eight clusters (homemakers, regular workers with a colorful life, regular workers with a monotonous life, part-time workers, recreation travelers, senior travelers, no-job travelers, and night owl adventurers) are identified based on ASTW-ED and ontology. To cluster the similarity matrix derived from the introduced methods, the affinity propagation (AP) clustering method is employed because it is free of prior knowledge for clustering and can produce exemplars of the clusters. The empirical study indicates that, relative to existing methods for multidimensional activity similarity measurement and clustering, ASTW-ED performs better in terms of within-group homogeneity and between-group heterogeneity of clusters. In addition, the results reveal that ontology can improve clustering performance if it is considered for nonspatiotemporal dimensions provide better understanding of human behavior for urban governance..
Keywords: Augmented space-time-weighted edit distance; Multidimensional activities; Puget sound region

Alexandros Kanterakis, Nikos Karacapilidis, Lefteris Koumakis, George Potamias,
On the development of an open and collaborative bioinformatics research environment,
Procedia Computer Science,
Volume 126,
2018,
Pages 1062-1071,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2018.08.043.
(https://www.sciencedirect.com/science/article/pii/S1877050918313218)
Abstract: This paper reports on the development of a self-sustaining and community-responsive platform that streamlines the wealth of available open Bioinformatics resources to accelerate multi-disciplinary collaboration and boost innovation in post-genomics biomedical research. Our approach adopts the principles of reproducible, reusable and remixable computer-aided research, and builds on top of state-of-the-art concepts and converging technologies for simple, fast and scalable specification and execution of scientific workflows. The proposed platform enables innovative networking and community building among researchers, facilitates knowledge sharing and co-creation, assures better-informed collaboration, and expedites gaining of insights. Paying particular attention to the issues of data and research provenance and attribution, the platform integrates a set of innovative services for the management of research resources and competences. The overall approach ensures the interoperability of the abovementioned resources and services from a technical, conceptual and user interface point of view.
Keywords: Bioinformatics; Collaborative Systems; Open-Science; Data Analytics

Roberto Pérez-Rodríguez, Luis Anido-Rifón, Miguel Gómez-Carballa, Marcos Mouriño-García,
Architecture of a concept-based information retrieval system for educational resources,
Science of Computer Programming,
Volume 129,
2016,
Pages 72-91,
ISSN 0167-6423,
https://doi.org/10.1016/j.scico.2016.05.005.
(https://www.sciencedirect.com/science/article/pii/S0167642316300314)
Abstract: Internet searches that occur in learning contexts are very different in nature from traditional “lookup” or “known item” searches: students usually perform searches to gather information about or master a certain topic, and the search engine is used as an aid in the exploration of a domain of knowledge. This paper presents SDE (Search Discover Explore), an exploratory search engine for educational resources that was built on top of the knowledge provided by Wikipedia: the set of its articles provides the search space (the set of topics that users can investigate), and the relationships between Wikipedia articles inform the suggestions that the search engine provides to students to go deeper in the exploration of a certain domain of knowledge. SDE indexes several hundreds of thousands of educational resources from high-quality Web sources, such as Project Gutenberg and Open Education Europe, among many others. This paper also reports the results of the evaluation of SDE by experts in Technology Enhanced Learning in several workshops that took place across Europe in the context of the European FP7 project iTEC. These results enable us to conclude that the exploratory search paradigm, making use of knowledge mined from Wikipedia, is a very promising approach for building information retrieval systems to be used in learning contexts.
Keywords: Exploratory search; Information retrieval; Bag-of-concepts (BoC) representation; Software architecture

Rodrigo Costa dos Santos, Maria Augusta Soares Machado,
FIRDoR - Fuzzy information retrieval for document recommendation,
Procedia Computer Science,
Volume 139,
2018,
Pages 56-63,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2018.10.217.
(https://www.sciencedirect.com/science/article/pii/S1877050918318866)
Abstract: This paper presents FIRDoR, which is a recommended methodology for document retrieval using the Fuzzy Inference System, with the goal of converging the area of interest of the user with that of the documents recovered. With the increasing number of documents available today on the internet and in databases, the task of assisting users in finding relevant information becomes very complex using conventional methods due to the difficulty in retrieving and ranking results. This study performs an experiment in order to identify the needs of the user, by their areas of interest, and the fuzzy classification approach in terms of a semantic document in order to make a recommendation. For the experiment, a base of 918 papers and 8 researchers were used and the results were gratifying, with a high degree of certainty in the recommendation of papers of importance for the user.
Keywords: fuzzy systems; information retrieval; semantic recommendation

Somayeh Soltani, Seyed Amin Hosseini Seno,
Detecting the software usage on a compromised system: A triage solution for digital forensics,
Forensic Science International: Digital Investigation,
Volume 44,
2023,
301484,
ISSN 2666-2817,
https://doi.org/10.1016/j.fsidi.2022.301484.
(https://www.sciencedirect.com/science/article/pii/S2666281722001652)
Abstract: One of the challenges of digital forensics is the high volume of investigative cases. To address this problem, researchers have proposed various triage methods. Detecting the applications that have run on the compromised system under inspection can be an excellent triage method that gives the investigator an overview of the system. In this paper, we construct the signature of software usage on a system using file path artifacts. We propose a software signature detection engine (SSDE) to identify the usage of the software on the system under investigation. The SSDE consists of two subsystems: the signature construction subsystem, which builds the software signature using the TF-IDF weighting scheme, and the signature detection subsystem, which identifies the executed set of software on the target system. We consider several parameters with different values in the design of SSDEs, leading to more than 500 SSDE models. We test the SSDE models against 14 pseudo-real systems from the M57 Patents scenario and evaluate their performance. The experimental results show that about 38% of SSDE models achieve near-perfect Precision, and about 18% of them achieve near-perfect Recall. We introduce the top models and determine which parameter values lead to the superior models. Besides, we compare the SSDE models with some doc2vec-based models. The results show that SSDE models have higher average Precision, slightly lower average Recall, and much less computational time.
Keywords: Digital forensics; Triage process; Software signature; TF-IDF; Forensic differential analysis

Malik Ghallab, Dana Nau, Paolo Traverso,
The actorʼs view of automated planning and acting: A position paper,
Artificial Intelligence,
Volume 208,
2014,
Pages 1-17,
ISSN 0004-3702,
https://doi.org/10.1016/j.artint.2013.11.002.
(https://www.sciencedirect.com/science/article/pii/S0004370213001173)
Abstract: Planning is motivated by acting. Most of the existing work on automated planning underestimates the reasoning and deliberation needed for acting; it is instead biased towards path-finding methods in a compactly specified state-transition system. Researchers in this AI field have developed many planners, but very few actors. We believe this is one of the main causes of the relatively low deployment of automated planning applications. In this paper, we advocate a change in focus to actors as the primary topic of investigation. Actors are not mere plan executors: they may use planning and other deliberation tools, before and during acting. This change in focus entails two interconnected principles: a hierarchical structure to integrate the actorʼs deliberation functions, and continual online planning and reasoning throughout the acting process. In the paper, we discuss open problems and research directions toward that objective in knowledge representations, model acquisition and verification, synthesis and refinement, monitoring, goal reasoning, and integration.
Keywords: Automated planning and acting

Matthias König,
Executable Simulation Model of the Liver,
Editor(s): Olaf Wolkenhauer,
Systems Medicine,
Academic Press,
2021,
Pages 413-422,
ISBN 9780128160787,
https://doi.org/10.1016/B978-0-12-801238-3.11682-4.
(https://www.sciencedirect.com/science/article/pii/B9780128012383116824)
Abstract: To address the issue of reproducibility in computational modeling we developed the concept of an executable simulation model (EXSIMO). An EXSIMO combines model, data and code with the execution environment to run the computational analysis in an automated manner using tools from software engineering. Key components are (i) models, data and code for the computational analysis; (ii) tests for models, data and code; and (iii) an automation layer to run tests and execute the analysis. An EXSIMO combines version control, model, data, units, annotations, analysis, reports, execution environment, testing, continuous integration and release. We applied the concept to perform a replication study of a computational analysis of hepatic glucose metabolism in the liver. The corresponding EXSIMO is available from https://github.com/matthiaskoenig/exsimo.
Keywords: COMBINE; Glucose; Liver; Metabolism; Replication; Reproducibility; SBML; SED-ML

Abdaladhem Albreshne, Jacques Pasquier,
A Domain Specific Language for High-level Process Control Programming in Smart Buildings,
Procedia Computer Science,
Volume 63,
2015,
Pages 65-73,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2015.08.313.
(https://www.sciencedirect.com/science/article/pii/S1877050915024412)
Abstract: Web services composition is a recurring idea in the field of smart residential environments (SRE), since it helps to solve complex problems such as energy saving, security control or health care by combining and orchestrating the available basic services. Smart environments are composed of networked devices (sensors and actuators) embedded within web services, which contain well-defined programming interfaces allowing them to share data, capture events and create composed control applications. There is still, however, a lack of domain specific languages (DSL) supporting a high degree of abstraction and transparency and allowing users to define control scenarios in a compact and comprehensible way. To satisfy these needs, the present paper aims to propose a DSL for describing scenarios to control SRE, with considerable gains in transparency, abstraction, expressiveness and simplicity.
Keywords: Smart Residential Environments; Smart Objects; Sensors; Actuators; Web Services Composition and Orchestration; Ontology; Domain Specific Language.

Mohammad Ebrahimi, Elaheh ShafieiBavani, Raymond K. Wong, Simon Fong, Jinan Fiaidhi,
An adaptive meta-heuristic search for the internet of things,
Future Generation Computer Systems,
Volume 76,
2017,
Pages 486-494,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2015.12.006.
(https://www.sciencedirect.com/science/article/pii/S0167739X15003908)
Abstract: The number of sensors deployed around the world is growing at a rapid pace when we are moving towards the Internet of Things (IoT). The widespread deployment of these sensors represents significant financial investment and technical achievement. These sensors continuously generate enormous amounts of data which is capable of supporting an almost unlimited set of high value proposition applications for users. Given that, effectively and efficiently searching and selecting the most related sensors of a user’s interest has recently become a crucial challenge. In this paper, inspired by ant clustering algorithm, we propose an effective context-aware method to cluster sensors in the form of Sensor Semantic Overlay Networks (SSONs) in which sensors with similar context information are gathered into one cluster. Firstly, sensors are grouped based on their types to create SSONs. Then, our meta-heuristic algorithm called AntClust has been performed to cluster sensors using their context information. Furthermore, useful adjustments have been applied to reduce the cost of sensor search process and an adaptive strategy is proposed to maintain the performance against dynamicity in the IoT environment. Experiments show the scalability and adaptability of AntClust in clustering sensors. It is significantly faster on sensor search when compared with other approaches.
Keywords: Internet of things; Context-aware sensor search; Ant-based clustering

Sergey Kosikov, Larisa Ismailova, Viacheslav Wolfengagen, Alexander Marenkov, Mikhail Ermak, Igor Slieptsov, Vladislav Zaytsev, Andrey Shedko,
Indexical structures to enable knowledge mining tasks,
Procedia Computer Science,
Volume 169,
2020,
Pages 284-290,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2020.02.180.
(https://www.sciencedirect.com/science/article/pii/S1877050920303033)
Abstract: In this paper, it is shown that semantic modeling technologies rely on data indexing systems. The semantic information theory known today is based on the method of using the principle of index expressions, using the division into real, possible and virtual individuals/data objects. Organization of a search on the Web for the concept of conceptual modeling of information processes based on variable domains that is being developed in the work is based on the natural requirements imposed on data organization. The model structure implements the separation of individuals, which manifests itself in the interaction of data organization, search and indexing. The identity of the data and the information processes designated by them is achieved in this model structure. A skeleton view of the organization of the search is being developed, leading to the maintenance of a network of variable domains and the possibility of deploying on them a conceptual modeling of interacting information processes.
Keywords: semantic information processing; computational model; variable domains

Nikola Dobrić,
Three-factor prototypicality evaluation and the verb look,
Language Sciences,
Volume 50,
2015,
Pages 1-11,
ISSN 0388-0001,
https://doi.org/10.1016/j.langsci.2014.12.005.
(https://www.sciencedirect.com/science/article/pii/S0388000115000030)
Abstract: Various linguistic and extra-linguistic criteria have been put forward as useful for evaluating the semantic prototypicality of polysemous lexemes. It has been suggested that most of these criteria are linked to frequency of occurrence of senses. However, the provision of additional quantitative linguistic data can shed more light on this complex lexical issue. The present paper embarks on a corpus-based investigation of whether a three-factor measurement of prototypicality based on a) frequency of occurrence of a sense (as the central feature), b) contextual saliency, and c) inter-category similarity produces significant results, particularly when applied to a highly polysemous lexeme – in this case the verb look. Besides investigating the quantitative linguistic background of the central (prototypical) member of a semantic category, the paper briefly scrutinizes whether the same combination of quantitative data can be applied to gauging the level of prototypicality of senses other than the prototype. The findings strongly support the application of the proposed three-factor methodology and point to the need for further work on the identification of suitable criteria for evaluating attested levels of category membership of multiple senses of a lexeme.
Keywords: Prototypicality; Polysemy; Inter-category similarity; Contextual salience; Category membership; Corpus-based semantics

Matthias Filter, Tasja Buschhardt, Fernanda Dórea, Estibaliz Lopez de Abechuco, Taras Günther, Esther M. Sundermann, Jörn Gethmann, Johanna Dups-Bergmann, Karin Lagesen, Johanne Ellis-Iversen,
One Health Surveillance Codex: promoting the adoption of One Health solutions within and across European countries,
One Health,
Volume 12,
2021,
100233,
ISSN 2352-7714,
https://doi.org/10.1016/j.onehlt.2021.100233.
(https://www.sciencedirect.com/science/article/pii/S2352771421000239)
Abstract: Cross-sector communication, collaboration and knowledge exchange are still significant challenges for practical adoption of the One Health paradigm. To address these needs the “One Health Surveillance Codex” (OHS Codex) was established to provide a framework for the One Health community to continuously share practical solutions (e.g. tools, technical resources, guidance documents and experiences) applicable for national and international stakeholders from different One Health Surveillance sectors. Currently, the OHS Codex provides a number of resources that support the adoption of the OH paradigm in areas linked to the harmonization and interpretation of surveillance data. The OHS Codex framework comprises four high-level “action” principles, which respectively support collaboration, knowledge exchange, data interoperability, and dissemination. These principles match well with priority areas identified in the “Tripartite Guide to Addressing Zoonotic Diseases in Countries” published by WHO, FAO and OIE. Within each of the four principles, the OHS Codex provides a collection of useful resources as well as pointers to success stories for the application of these resources. As the OHS Codex is designed as an open community framework, it will continuously evolve and adapt to the needs of the OH community in the future.
Keywords: Knowledge base; Collaboration; Dissemination; Mutual understanding; Data interoperability

Charlotte Siefridt, Julien Grosjean, Tatiana Lefebvre, Laetitia Rollin, Stefan Darmoni, Matthieu Schuers,
Evaluation of automatic annotation by a multi-terminological concepts extractor within a corpus of data from family medicine consultations,
International Journal of Medical Informatics,
Volume 133,
2020,
104009,
ISSN 1386-5056,
https://doi.org/10.1016/j.ijmedinf.2019.104009.
(https://www.sciencedirect.com/science/article/pii/S1386505619308184)
Abstract: Introduction
Research in family medicine is necessary to improve the quality of care. The number of publications in general medicine remains low. Databases from Electronic Medical Records can increase the number of these publications. These data must be coded to be used pertinently. The objective of this study was to assess the quality of semantic annotation by a multi-terminological concept extractor within a corpus of family medicine consultations.
Method
Consultation data in French from 25 general practitioners were automatically annotated using 28 different terminologies. The data extracted were classified into three groups: reasons for consulting, observations and consultation results. The first evaluation led to a correction phase of the tool which led to a second evaluation. For each evaluation, the precision, recall and F-measure were quantified. Then, the inter- and intra-terminological coverage of each terminology was assessed.
Results
Nearly 15,000 automatic annotations were manually evaluated. The mean values for the second evaluation of precision, recall and F-measure were 0.85, 0.83 and 0.84 respectively. The most common terminologies used were SNOMED CT, SNOMED 3.5 and NClt. The terminologies with the best intra-terminological coverage were ICPC-2, DRC and CISMeF Meta-Terms.
Conclusion
A multi-terminological concepts extractor can be used for the automatic annotation of consultation data in family medicine. Integrating such a tool into general practitioners’ business software would be a solution to the lack of routine coding. Developing the use of a single terminology specific to family medicine could improve coding, facilitate semantic interoperability and the communication of relevant information.
Keywords: Automatic annotation; Databases; Family medicine; Clinical coding; Electronic medical records

François Pinet, Sandro Bimonte, André Miralles, Frédéric Flouvat,
Preface,
Ecological Informatics,
Volume 26, Part 2,
2015,
Pages 117-118,
ISSN 1574-9541,
https://doi.org/10.1016/j.ecoinf.2014.08.003.
(https://www.sciencedirect.com/science/article/pii/S1574954114001101)

Isabelle Comyn-Wattiau, Il-Yeol Song, Katsumi Tanaka, Motoshi Saeki, Shuichiro Yamamoto,
Guest editorial – Special issue on conceptual modeling — 35th International Conference on Conceptual Modeling (ER2016),
Data & Knowledge Engineering,
Volume 117,
2018,
Pages 336-338,
ISSN 0169-023X,
https://doi.org/10.1016/j.datak.2018.04.010.
(https://www.sciencedirect.com/science/article/pii/S0169023X18301745)

Mohamed Daoud, Asmae El Mezouari, Noura Faci, Djamal Benslimane, Zakaria Maamar, Aziz El Fazziki,
A multi-model based microservices identification approach,
Journal of Systems Architecture,
Volume 118,
2021,
102200,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102200.
(https://www.sciencedirect.com/science/article/pii/S1383762121001442)
Abstract: Microservices are hailed for their capabilities to tackle the challenge of breaking monolithic business systems down into small, cohesive, and loosely-coupled services. Indeed, these systems are neither easy to maintain nor to replace undermining organizations’ efforts to cope with user’s changing needs and governments’ complex regulations. Microservices constitute an architectural style for developing a new generation of systems as a suite of services that, although they are separate, engage in collaborative execution and communication sessions. However, microservices success depends, among many other things, on the existence of an approach that would automatically identify the necessary microservices according to organizations’ requirements. In this paper, we present such an approach and demonstrate its technical doability in the context of a case study, Bicing, for renting bikes. Some salient features of this approach are business processes as input for the identification needs, three models known as control, data, and semantic to capture dependencies between these processes’ activities, and, finally, a collaborative clustering technique that recommends potential microservices. Conducted experiments in the context of Bicing clearly indicate that our approach outperforms similar ones for microservices identification and reinforce the important role of business processes in this identification. The approach constitutes a major milestone towards a better architectural style for future microservices systems.
Keywords: Business process; Control/Data/Semantic dependency; Clustering; Microservice

Nenad Petrović, Đorđe Kocić,
11 - Smart technologies for COVID-19 indoor monitoring,
Editor(s): Fernando Pacheco-Torgal, Volodymyr Ivanov, Joseph O. Falkinham,
In Woodhead Publishing Series in Civil and Structural Engineering,
Viruses, Bacteria and Fungi in the Built Environment,
Woodhead Publishing,
2022,
Pages 251-272,
ISBN 9780323852067,
https://doi.org/10.1016/B978-0-323-85206-7.00012-5.
(https://www.sciencedirect.com/science/article/pii/B9780323852067000125)
Abstract: COVID-19 pandemic has affected every aspect of our life significantly, from work and education to healthcare and entertainment. In order to reduce its spread, safety guidelines have to be followed both indoors and outdoors. In this chapter, the focus is on scenarios related to indoor safety monitoring. The following aspects relevant to indoor COVID-19 protection will be considered: (1) checking whether visitor wears protective mask, (2) body temperature check, (3) social distancing, (4) limited person number indoors, (5) automatic touch-free hand sanitization, and (6) tracing contacts involving infected persons. As research outcomes, several case studies developed at the University of Niš, Faculty of Electronic Engineering in Serbia, will be presented, relying on state-of-the-art smart technology: IoT devices, smartphones, computer vision, semantic knowledge representation, augmented reality, robotics, and blockchain.
Keywords: Augmented reality; COVID-19; Coronavirus; Computer vision; Microcontrollers; Mobile apps; Robotics

Bangchao Wang, Rong Peng, Yuanbang Li, Han Lai, Zhuo Wang,
Requirements traceability technologies and technology transfer decision support: A systematic review,
Journal of Systems and Software,
Volume 146,
2018,
Pages 59-79,
ISSN 0164-1212,
https://doi.org/10.1016/j.jss.2018.09.001.
(https://www.sciencedirect.com/science/article/pii/S0164121218301754)
Abstract: Requirements traceability (RT) is a core activity in Requirements Engineering. Various types of RT technologies have been extensively studied for decades. In this paper, we present a systematic literature review from 114 papers between 2006 and 2016 on RT techniques. We summarized 10 major challenges in current RT activities, and categorized existing RT techniques into 6 groups and 25 sub-groups. Moreover, we built mapping relations between these challenges and techniques, and identified 7 potential future research directions. Based on 83 empirical studies, the evaluations for technology transfer are conducted. The main conclusions are: (1) The “trustworthy” and “automated” challenges are the most widely investigated ones, while “scalable”, “coordinated”, “dynamic” and “lightweight” challenges receive much less attention; (2) “Trace link generation”, especially information retrieval-based (IR-based) methods, are the most studied techniques; (3) IR-based methods have the most potential to be adopted by industry, as they have been validated from multiple viewpoints; (4) Seven promising future research directions are identified, which include developing scalable, dynamic and lightweight tracing techniques, introducing new approaches in other disciplines to meet the RT challenges, improving the express ability of trace links, promoting the industry adoption of RT technologies and developing new techniques to support developers’ coordination.
Keywords: Requirements traceability technology; Technology transfer; Systematic literature review; Requirements traceability challenges; Quality assessment

Emanuel Demetrescu, Bruno Fanini,
A white-box framework to oversee archaeological virtual reconstructions in space and time: Methods and tools,
Journal of Archaeological Science: Reports,
Volume 14,
2017,
Pages 500-514,
ISSN 2352-409X,
https://doi.org/10.1016/j.jasrep.2017.06.034.
(https://www.sciencedirect.com/science/article/pii/S2352409X17301475)
Abstract: The goal of this paper is to present original methods and visual tools able to formally document the scientific processes behind an archaeological virtual reconstruction, namely a new version of the Extended Matrix (EM 1.1) and the Extended Matrix Framework (EMF 1.1). The proposed approach aims to improve the EM as well as methods and tools for 3D query, visualization, and inspection of extended matrices in order to solve current bottlenecks and issues with the integration of 3D virtual environments and rich semantic descriptions (EMF). A real case scenario is provided to present the steps involved in a reconstruction project using EM/EMF: the Great Temple of the ancient Roman town Colonia Dacica Sarmizegetusa.
Keywords: 3D semantic annotation; 3D virtual reconstruction; Extended Matrix; Virtual Stratigraphic Unit; Scenegraph

María Herrero-Zazo, Isabel Segura-Bedmar, Paloma Martínez,
Annotation Issues in Pharmacological Texts,
Procedia - Social and Behavioral Sciences,
Volume 95,
2013,
Pages 211-219,
ISSN 1877-0428,
https://doi.org/10.1016/j.sbspro.2013.10.641.
(https://www.sciencedirect.com/science/article/pii/S187704281304161X)
Abstract: Natural language processing of pharmacological texts includes recognition of drug names and extraction of relationships between them. To this purpose, pharmacological annotated corpora are required. These corpora are usually semantically annotated by domain experts. However, other linguistic aspects should be considered to ensure the quality and consistency of the annotation. This paper introduces several linguistic phenomena affecting the annotation of both drug named entities and drug-drug interaction relationships that arose during the annotation process of the DDI corpus. The detailed documentation of these issues and the decisions on them will improve the quality of the annotated corpus and its usefulness for other researchers or users.
Keywords: linguistics; biomedical corpora; drug nomenclature; drug interactions; annotation.

Giuseppe Becchi, Marco Bertini, Alberto Del Bimbo, Andrea Ferracani, Daniele Pezzatini,
A Distributed System for Multimedia Monitoring, Publishing and Retrieval,
Procedia Computer Science,
Volume 38,
2014,
Pages 100-107,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2014.10.017.
(https://www.sciencedirect.com/science/article/pii/S1877050914013775)
Abstract: In this paper we present a distributed and interactive multi-user system which provides a flexible approach to collect, manage, annotate and publish collections of images, videos and textual documents. The system is based on a Service Oriented Architecture that allows to combine and orchestrate a large set of web services for automatic and manual annotation, retrieval, browsing, ingestion and authoring of different multimedia sources. These tools can been used to create several publicly available vertical applications, addressing different use cases. Positive results of usability test evaluations have shown that the system can be effectively used to create video retrieval systems.
Keywords: semantic multimedia annotation; SOA; multimedia retrieval

K. Bontcheva, L. Derczynski,
Chapter 6 - Extracting Information from Social Media with GATE,
Editor(s): Emma L. Tonkin, Gregory J.L. Tourte,
In Chandos Information Professional Series,
Working with Text,
Chandos Publishing,
2016,
Pages 133-158,
ISBN 9781843347491,
https://doi.org/10.1016/B978-1-84334-749-1.00006-8.
(https://www.sciencedirect.com/science/article/pii/B9781843347491000068)
Abstract: Information extraction from social media content has only recently become an active research topic, following early experiments that showed this genre to be extremely challenging for state-of-the-art algorithms. Unlike carefully authored news text and other longer content, social media content poses a number of new challenges, due to shortness, noise, strong contextual anchoring, and highly dynamic nature. This chapter provides a thorough analysis of the problems and describes the most recent GATE algorithms, specifically developed for extracting information from social media content. Comparisons against other state-of-the-art research on this topic are also made. These new GATE components have now been bundled together, to form the new TwitIE information extraction pipeline, distributed as a GATE plugin.
Keywords: Information extraction; GATE; Social media

Michael Abramovici, Philip Gebus, Jens Christian Göbel, Philipp Savarino,
Semantic Quality Assurance of Heterogeneous Unstructured Repair Reports,
Procedia CIRP,
Volume 73,
2018,
Pages 265-270,
ISSN 2212-8271,
https://doi.org/10.1016/j.procir.2018.03.334.
(https://www.sciencedirect.com/science/article/pii/S2212827118305158)
Abstract: Service technicians spend a considerable amount of their working hours in order to search for information regarding a current work order. In case of an IPS² malfunction for example, they can either search for potential failure causes within previously documented repair reports that describe a similar malfunction or manually inspect the IPS². On the one hand the former IT-related information procurement is caused by a large amount of irrelevant search results rendered by current state of the art information retrieval approaches implemented in maintenance-related IT systems. On the other hand many repair reports suffer from missing or ambiguous information and a holistically low data quality, which makes it difficult for the service technicians to derive task-related information from a particular repair report, although this report basically describes the same malfunction. These current difficulties will be amplified once service partners get access to maintenance-related information documented by other service partners (companies) within the IPS² network as the amount of available data will increase drastically and new problems will raise concerning the heterogeneity of the data. The paper on hand presents a semantic quality assurance concept for heterogeneous unstructured repair reports that addresses the low data quality problem by utilizing natural language processing and machine learning methods to automatically analyze the service technician’s inputs during the repair report creation process and by notifying him of potential losses in data quality. The concept’s feasibility has been shown by performing a case study with a prototype that utilizes the developed methods.
Keywords: Quality Assurance; Product-Service Systems; Semantic Technologies; Knowledge Management

Niloofar Shahidi, Michael Pan, Kenneth Tran, Edmund J. Crampin, David P. Nickerson,
SBML to bond graphs: From conversion to composition,
Mathematical Biosciences,
Volume 352,
2022,
108901,
ISSN 0025-5564,
https://doi.org/10.1016/j.mbs.2022.108901.
(https://www.sciencedirect.com/science/article/pii/S0025556422000906)
Abstract: The Systems Biology Markup Language (SBML) is a popular software-independent XML-based format for describing models of biological phenomena. The BioModels Database is the largest online repository of SBML models. Several tools and platforms are available to support the reuse and composition of SBML models. However, these tools do not explicitly assess whether models are physically plausible or thermodynamically consistent. This often leads to ill-posed models that are physically impossible, impeding the development of realistic complex models in biology. Here, we present a framework that can automatically convert SBML models into bond graphs, which imposes energy conservation laws on these models. The new bond graph models are easily mergeable, resulting in physically plausible coupled models. We illustrate this by automatically converting and coupling a model of pyruvate distribution to a model of the pentose phosphate pathway.
Keywords: SBML; BioModels; Bond graphs; Automatic conversion; Glycolysis; Pentose phosphate pathway

J.L. Allones, M. Taboada, D. Martinez, R. Lozano, M.J. Sobrido,
SNOMED CT module-driven clinical archetype management,
Journal of Biomedical Informatics,
Volume 46, Issue 3,
2013,
Pages 388-400,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2013.01.003.
(https://www.sciencedirect.com/science/article/pii/S1532046413000051)
Abstract: Objective
To explore semantic search to improve management and user navigation in clinical archetype repositories.
Methods
In order to support semantic searches across archetypes, an automated method based on SNOMED CT modularization is implemented to transform clinical archetypes into SNOMED CT extracts. Concurrently, query terms are converted into SNOMED CT concepts using the search engine Lucene. Retrieval is then carried out by matching query concepts with the corresponding SNOMED CT segments.
Results
A test collection of the 16 clinical archetypes, including over 250 terms, and a subset of 55 clinical terms from two medical dictionaries, MediLexicon and MedlinePlus, were used to test our method. The keyword-based service supported by the OpenEHR repository offered us a benchmark to evaluate the enhancement of performance. In total, our approach reached 97.4% precision and 69.1% recall, providing a substantial improvement of recall (more than 70%) compared to the benchmark.
Conclusions
Exploiting medical domain knowledge from ontologies such as SNOMED CT may overcome some limitations of the keyword-based systems and thus improve the search experience of repository users. An automated approach based on ontology segmentation is an efficient and feasible way for supporting modeling, management and user navigation in clinical archetype repositories.
Keywords: openEHR; SNOMED CT; Clinical archetypes; Semantic interoperability; Modularization; Semantic search

Kun Liang, Cong Wang, Yiying Zhang, Weifu Zou,
Knowledge Aggregation and Intelligent Guidance for Fragmented Learning,
Procedia Computer Science,
Volume 131,
2018,
Pages 656-664,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2018.04.309.
(https://www.sciencedirect.com/science/article/pii/S1877050918306896)
Abstract: This paper analyzes the characteristics of Fragmented Learning behavior, and then reorganizes the knowledge in online education according to the learner’s individual learning needs. It helps and guides the learners to make full use of the fragmented time to obtain accurate and meaningful knowledge content. This paper provides a new theoretical support for online learning. It has a theoretical and guiding significance to carry out an effective Fragmented Learning model.
Keywords: Fragmented Learning; Knowledge Aggregation; intelligent guidance; User Profile; Knowledge Elements

Abdelhadi Belfadel, Sebastian Hörl, Rodrigo Javier Tapia, Dimitra Politaki, Ibad Kureshi, Lorant Tavasszy, Jakob Puchinger,
A conceptual digital twin framework for city logistics,
Computers, Environment and Urban Systems,
Volume 103,
2023,
101989,
ISSN 0198-9715,
https://doi.org/10.1016/j.compenvurbsys.2023.101989.
(https://www.sciencedirect.com/science/article/pii/S0198971523000522)
Abstract: Urban logistics is one of the key elements of urban mobility planning. The use of real-time information systems in logistics operations generates an enormous amount of data, nowadays used mainly for the purpose of monitoring and control of large flows of goods. At the same time, urban planners, business stakeholders, and city administrators are in need of adaptive, data-driven decision support solutions to address today's urban logistics problems. Recently, digital twins have received a lot of attention to support advanced experimentation, simulation and decision-making for on-demand logistics operations. Questions still remain on how to realize these for urban logistics management in a mixed public-private stakeholder context. We argue that this lack of a specific framework for city logistics with a model library for data mergers, linking physical and virtual data exchange, can compromise the timely adoption of digital twin technology. We contribute to filling this gap by presenting a systematic review of the literature, proposing a conceptual framework for digital twin applications in urban logistics, and providing use case scenarios for their demonstration. Together, these should advance the technical implementation of digital twins in a sustainable city logistics context.
Keywords: Digital twin; Urban logistics; City logistics; On-demand logistics; System architecture

Gerhard Mayer, Christian Stephan, Helmut E. Meyer, Michael Kohl, Katrin Marcus, Martin Eisenacher,
ProCon — PROteomics CONversion tool,
Journal of Proteomics,
Volume 129,
2015,
Pages 56-62,
ISSN 1874-3919,
https://doi.org/10.1016/j.jprot.2015.06.015.
(https://www.sciencedirect.com/science/article/pii/S1874391915300531)
Abstract: With the growing amount of experimental data produced in proteomics experiments and the requirements/recommendations of journals in the proteomics field to publicly make available data described in papers, a need for long-term storage of proteomics data in public repositories arises. For such an upload one needs proteomics data in a standardized format. Therefore, it is desirable, that the proprietary vendor's software will integrate in the future such an export functionality using the standard formats for proteomics results defined by the HUPO-PSI group. Currently not all search engines and analysis tools support these standard formats. In the meantime there is a need to provide user-friendly free-to-use conversion tools that can convert the data into such standard formats in order to support wet-lab scientists in creating proteomics data files ready for upload into the public repositories. ProCon is such a conversion tool written in Java for conversion of proteomics identification data into standard formats mzIdentML and Pride XML. It allows the conversion of Sequest™/Comet .out files, of search results from the popular and often used ProteomeDiscoverer® 1.x (x=versions 1.1 to1.4) software and search results stored in the LIMS systems ProteinScape® 1.3 and 2.1 into mzIdentML and PRIDE XML. This article is part of a Special Issue entitled: Computational Proteomics.
Keywords: Proteomics; Conversion tool; ProCon; mzIdentML; PRIDE; ProteomeXchange

Daniel S. Hain, Roman Jurowetzki, Tobias Buchmann, Patrick Wolf,
A text-embedding-based approach to measuring patent-to-patent technological similarity,
Technological Forecasting and Social Change,
Volume 177,
2022,
121559,
ISSN 0040-1625,
https://doi.org/10.1016/j.techfore.2022.121559.
(https://www.sciencedirect.com/science/article/pii/S0040162522000919)
Abstract: This paper describes an efficiently scaleable approach to measuring technological similarity between patents by combining embedding techniques from natural language processing with nearest-neighbor approximation. Using this methodology, we are able to compute similarities between all existing patents, which in turn enables us to represent the whole patent universe as a technological network. We validate both technological signature and similarity in various ways and, using the case of electric vehicle technologies, demonstrate their usefulness in measuring knowledge flows, mapping technological change, and creating patent quality indicators. This paper contributes to the growing literature on text-based indicators for patent analysis. We provide thorough documentation of our methods, including all code, and indicators at https://github.com/AI-Growth-Lab/patent_p2p_similarity_w2v).
Keywords: Technological similarity; Patent data; Natural-language processing; Technology network; Patent landscaping; Patent quality

Yuanfeng He, Yuanxi Li, Jiajia Lei, C.H.C Leung,
A framework of query expansion for image retrieval based on knowledge base and concept similarity,
Neurocomputing,
Volume 204,
2016,
Pages 26-32,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2015.11.102.
(https://www.sciencedirect.com/science/article/pii/S0925231216300972)
Abstract: We study several semantic concept-based query expansion and re-ranking scheme and compare different ontology-based expansion methods in image search and retrieval. To improve the query expansion efficiency and accuracy, we employ the CYC knowledge base to generate the expansion candidate concepts, while filter and rank the expansion results by calculating concept similarities using the Semantic Relatedness Metrics. Using our knowledge-based query expansion in image retrieval, the efficiency and accuracy has been improved.
Keywords: Image retrieval; Knowledge base; Query expansion

Xiran Zhou, Zhenfeng Shao, Wei Zeng, Jun Liu,
Semantic graph construction for 3D geospatial data of multi-versions,
Optik,
Volume 125, Issue 6,
2014,
Pages 1730-1734,
ISSN 0030-4026,
https://doi.org/10.1016/j.ijleo.2013.09.057.
(https://www.sciencedirect.com/science/article/pii/S0030402613013351)
Abstract: 3D geospatial data holds rich information that causing significant tough workload in 3D geospatial semantic building. In order to avoid those difficulties in building high precise semantic, this paper focuses on creating a semantic graph for 3D geospatial data via the novel approach called semantic graph. Firstly, all data related to geo-spatial are organized through semantic conceptualization processing. Its result is divided into conceptual description and formal pattern involving all features belong to certain 3D object or scene. Then from perspective of spatial, thematic and temporal domains, multi-versions semantic relations are created depend on relational rules and semantic mapping mechanism. On the basic of semantic conceptualization processing, all conceptual and formal patterns are controlled as semantic integrated result. Since the result covers spatial, thematic and temporal semantic information of 3D geospatial field, approach proposed by this paper can generate 3D geospatial data semantic graph based on semantic conceptualization and accomplish the transform from 3D geospatial data to 3D geospatial semantic effectively.
Keywords: Semantic conceptualization; Multi-versions data; 3D geospatial data; Semantic graph

Haishuo Wang, Ke Chen, Hongmei Zheng, Guojun Zhang, Rui Wu, Xiaopeng Yu,
Knowledge transfer methods for expressing product design information and organization,
Journal of Manufacturing Systems,
Volume 58, Part A,
2021,
Pages 1-15,
ISSN 0278-6125,
https://doi.org/10.1016/j.jmsy.2020.11.009.
(https://www.sciencedirect.com/science/article/pii/S0278612520301965)
Abstract: Product design information represents not only the carrier of design but also the significant digital assets of businesses. At present, manufacturing is facing an environment with mass, fragmented, real-time, and multi-scene digital information in the process of product design. To improve the availability of information resources and the efficiency of information reuse as well as to achieve the sharing of production means, the expression of product design information should be optimized in information storage. In addition, a new ecosystem should be built for information increments, making the participants of every link in the process of product design become the contributors of information. Considering the unique aspects of design group individuals, this paper builds a knowledge transfer model that capitalizes on hypercycle theory and proposes the concept of modularizing information carriers and information processing methods. It comprehensively analyses the expression methodology and organizational attributes of product design information and proposes a knowledge transfer carrier model constructed via discretized fragmented semantic information, which is concretely implemented as informative product file labelling. By combining personnel, information carriers and information dissemination networks, this paper provides the functionality and architecture needed to build an information processing platform using social networking software (SNS). Some application scenarios are described by using this processing method for production information; the development process for an automotive air filter is shown as an example. The results of this study suggest that the proposed method is conducive to improving the expressions of product design information and the interactions among participants. The simplicity of the labelling process and the intuitive label content greatly reduce the usability threshold and the losses caused by information gaps, creating a precondition for many types of people to fully participate in the product design information knowledge transfer cycle.
Keywords: Product design; Information gap; Hypercycle theory; Information processing; Information carrier

Mario Almagro, Raquel Martínez, Soto Montalvo, Víctor Fresno,
A cross-lingual approach to automatic ICD-10 coding of death certificates by exploring machine translation,
Journal of Biomedical Informatics,
Volume 94,
2019,
103207,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2019.103207.
(https://www.sciencedirect.com/science/article/pii/S153204641930125X)
Abstract: Automatic ICD-10 coding is an unresolved challenge in terms of Machine Learning tasks. Despite hospitals generating an enormous amount of clinical documents, data is considerably sparse, associated with a very skewed and unbalanced code distribution, what entails reduced interoperability. In addition, in some languages the availability of coded documents is very limited. This paper proposes a cross-lingual approach based on Machine Translation methods to code death certificates with ICD-10 using supervised learning. The aim of this approach is to increase the availability of coded documents by combining collections of different languages, which may also contribute to reduce their possible bias in the ICD distribution, i.e. to avoid the promotion of a subset of codes due to service or environmental factors. A significant improvement in system performance is achieved for those labels with few occurrences.
Keywords: ICD-10 coding; Cross-lingual approach; Text mining; Electronic health records; Machine translation

Valentina Dragos,
Detection of contradictions by relation matching and uncertainty assessment,
Procedia Computer Science,
Volume 112,
2017,
Pages 71-80,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2017.08.028.
(https://www.sciencedirect.com/science/article/pii/S1877050917313674)
Abstract: Contradiction detection is a difficult task in the field of natural language processing given the variety of ways contradictions occur across texts. If blunt negations, antonyms and numerical mismatches are obvious features to convey contradictions, they also arise from inconsistent domain knowledge, uncertain co-references or differences in the structures of assertions. In this paper, we investigate the problem of contradictions detection for uncertain statements when the author provides not only factual information but also clues about its plausibility. The problem is of particular interest for application fields relying on reported information, when decision makers receive information from various sources. Along with hints as to the derivation of the content, authors often embed clues as to how strong they support reported facts, in the form on confidence, skepticism, doubt or strong conviction. For such assertions, contradictions highlight not only impossible versions of reported events and actions but also discrepancies in the assessment of their plausibility. After analyzing various types of contradictions in subjective statements, we describe a model to detect contradictions thanks to a joint analysis of functional relations and uncertainty assessments.
Keywords: contradiction identification; relation extraction; uncertainty assessment

Imon Banerjee, Giuseppe Patané, Michela Spagnuolo,
Combination of visual and symbolic knowledge: A survey in anatomy,
Computers in Biology and Medicine,
Volume 80,
2017,
Pages 148-157,
ISSN 0010-4825,
https://doi.org/10.1016/j.compbiomed.2016.11.018.
(https://www.sciencedirect.com/science/article/pii/S0010482516303122)
Abstract: In medicine, anatomy is considered as the most discussed field and results in a huge amount of knowledge, which is heterogeneous and covers aspects that are mostly independent in nature. Visual and symbolic modalities are mainly adopted for exemplifying knowledge about human anatomy and are crucial for the evolution of computational anatomy. In particular, a tight integration of visual and symbolic modalities is beneficial to support knowledge-driven methods for biomedical investigation. In this paper, we review previous work on the presentation and sharing of anatomical knowledge, and the development of advanced methods for computational anatomy, also focusing on the key research challenges for harmonizing symbolic knowledge and spatial 3D data.
Keywords: Computational anatomy; Computer aided diagnosis systems; Knowledge in medicine; Information systems; Medical informatics; IT applications in health care

Luigi Atzori, Davide Carboni, Antonio Iera,
Smart things in the social loop: Paradigms, technologies, and potentials,
Ad Hoc Networks,
Volume 18,
2014,
Pages 121-132,
ISSN 1570-8705,
https://doi.org/10.1016/j.adhoc.2013.03.012.
(https://www.sciencedirect.com/science/article/pii/S1570870513000590)
Abstract: Information about human social activities and relationships are exploited by an ever increasing number of proposed applications and protocols in several scenarios, given the consequent increase in the system performance. Examples are data transmission over delay tolerant networks, content recommendation in search engines, and advertisement of products and services. An emerging field where social networks are being exploited is the Internet of Things, where smart objects connect to the network to bring the real world into the virtual dimension. Objects capable to communicate on social network sites are able to enter into their owners’ social loop so as to automatically publish information of interest for selected communities of people and to perform some related automatic actions. In so doing, not only can objects be part of the human social networks but they can also build their own social network. As a consequence, interactions among them can be fostered towards the development of complex services for the direct benefit of people. Accordingly, objects mimic the human behavior towards a scalable and effective service discovery and composition as well as trustworthiness management. On the basis of the importance achieved by this trend in the last couple of years, in this paper we intend to review the adopted approaches towards the exploitation of social network concepts by the Internet of Things, the technologies behind these, and the potentialities.
Keywords: Internet of Things; Social networks; Smart objects

Zhixin Li, Zhongzhi Shi, Weizhong Zhao, Zhiqing Li, Zhenjun Tang,
Learning semantic concepts from image database with hybrid generative/discriminative approach,
Engineering Applications of Artificial Intelligence,
Volume 26, Issue 9,
2013,
Pages 2143-2152,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2013.07.004.
(https://www.sciencedirect.com/science/article/pii/S0952197613001322)
Abstract: Semantic gap has become a bottleneck of content-based image retrieval in recent years. In order to bridge the gap and improve the retrieval performance, automatic image annotation has emerged as a crucial problem. In this paper, a hybrid approach is proposed to learn the semantic concepts of images automatically. Firstly, we present continuous probabilistic latent semantic analysis (PLSA) and derive its corresponding Expectation–Maximization (EM) algorithm. Continuous PLSA assumes that elements are sampled from a multivariate Gaussian distribution given a latent aspect, instead of a multinomial one in traditional PLSA. Furthermore, we propose a hybrid framework which employs continuous PLSA to model visual features of images in generative learning stage and uses ensembles of classifier chains to classify the multi-label data in discriminative learning stage. Therefore, the framework can learn the correlations between features as well as the correlations between words. Since the hybrid approach combines the advantages of generative and discriminative learning, it can predict semantic annotation precisely for unseen images. Finally, we conduct the experiments on three baseline datasets and the results show that our approach outperforms many state-of-the-art approaches.
Keywords: Automatic image annotation; Continuous PLSA; Hybrid framework; Classifier chain; Semantic gap; Image retrieval

Liang Tian, Yang Zhiping, Hu Zhengyin,
The Large Aperture Optical Elements patent search system based on Domain Knowledge Organization System,
World Patent Information,
Volume 35, Issue 3,
2013,
Pages 209-213,
ISSN 0172-2190,
https://doi.org/10.1016/j.wpi.2013.04.007.
(https://www.sciencedirect.com/science/article/pii/S0172219013000641)
Abstract: Effective use of patent information provides a multiplier effect in product design and new technology development. This paper reports research on using an open-source extraction tool—General architecture for text engineering (GATE) and word split software—from the Institute of Computing Technology and Chinese lexical analysis system (ICTCLAS) to assist the expert in acquiring and marking of feature word groups from the abstracts and claims of the patents on Large Aperture Optical Elements. Then, it used the experts feature word groups, which were formalized by INSPEC control words, to construct a Domain Knowledge Organization System (DKOS). Base on the DKOS, a retrieval module of patent information is constructed, which has practical significance for designers to design products and develop new technology. The system is a visual application system, for example, which can filter patent documents by topics and retrieve relevant topics via sample text.
Keywords: Semiautomatic data capture; Knowledge system; Patent search; Semantic search; Control words; Word groups; Optical instruments; Domain knowledge

G. Ignisha Rajathi, R. Johny Elton, M. Beena Mol, L. Prinza, G. Jerald Prasath, J. Mohanalin Rajarathnam,
2 - Exploration of various Internet of Medical Things techniques for monitoring and guiding the Covid-19 infected and uninfected cases,
Editor(s): Rajkumar Buyya, Muhammad Imran Tariq, Valentina Emilia Balas, Guojun Wang, Radu Prodan,
Security and Privacy Issues in Internet of Medical Things,
Academic Press,
2023,
Pages 33-45,
ISBN 9780323898720,
https://doi.org/10.1016/B978-0-323-89872-0.00006-X.
(https://www.sciencedirect.com/science/article/pii/B978032389872000006X)
Abstract: The pandemic of COVID-19, which has been the current global challenge, seemed to have shattered all the trust and hope in the current technologies. Millions have been affected and yet the technologist couldn’t defend the onslaught mostly due to the massiveness and quickness of the spread. Even though the vaccination is in eyesight, still, survival is a tedious task due to poor clarity about the whereabouts of the infected and poor monitoring systems. Internet of Things (IoT) has revolutionized the health care system and can be useful for proper monitoring of COVID-19 patients, by employing an interconnected network to invigorate the Internet of Medical things. In this study, we explore the possibilities of the current IoT techniques in monitoring and guiding the patients. This in turn can help the uninfected to be informed and guided to keep tracking their own movement with respect to the infected. This will help the readers in identifying varieties of IoMT applications that can be extended in monitoring the COVID-19 infected patients. Hence, this article will motivate the thriving researchers to innovate their ideas over it, ultimately helping the society in fighting the pandemic situation.
Keywords: Health care; Pandemic; Drone; Smartphone; Cloud storage; Network delay

A. F Zakaria, S. C Johnson Lim,
Data Analytics Skill Development for Design Education: A Case Study in Optimal Product-Service Bundle Design,
Thinking Skills and Creativity,
Volume 46,
2022,
101191,
ISSN 1871-1871,
https://doi.org/10.1016/j.tsc.2022.101191.
(https://www.sciencedirect.com/science/article/pii/S1871187122001924)
Abstract: Design education (DE) is a growing field that focuses on the educational perspective of domains such as technology and engineering to support the design study. It is aimed to equip engineers with the essential knowledge and skills for future needs. Previously, there exist a number of related studies on skills development that had applied information and communication technology (ICT) tools to improve teaching and learning experience. Nevertheless, data analytics skills development in the DE field, particularly on how data is organized and readily accessible during analysis and decision-making process, is less emphasized. In this paper, we have presented an implementation of design skill module using a data mining software and an optimization software package to train design engineers in data-driven design decision-making. A case study in optimal product service bundle (PSB) design that illustrates the usefulness of our proposed skill training modules is presented to showcase the level of trainees’ achievement, usability and user experience. In overall, our case study has produced promising outcomes which indicate the merits of our approach in data analytics skill development.
Keywords: Design education; Skill training; Decision making; Data-driven design; Product-service bundle

Sebastian Pütz, Thomas Wiemann, Joachim Hertzberg,
The Mesh Tools Package – Introducing Annotated 3D Triangle Maps in ROS,
Robotics and Autonomous Systems,
Volume 138,
2021,
103688,
ISSN 0921-8890,
https://doi.org/10.1016/j.robot.2020.103688.
(https://www.sciencedirect.com/science/article/pii/S0921889020305285)
Abstract: Triangle mesh maps for robotic applications are becoming increasingly popular, but are not yet effectively supported in the Robot Operating System (ROS). We introduce the Mesh Tools package consisting of message definitions, RViz plugins and tools, as well as a persistence layer. These tools make annotated triangle maps available in ROS and allow to publish, edit and inspect such maps within the existing ROS software stack. The persistence layer efficiently loads and stores large mesh maps. The proposed plugins and tools enable the visualization and validation of the complete layered map and associated properties to allow fluid interaction. We demonstrate the seamless integration of our tools in two application areas as a proof-of-concept: Labeling of triangle clusters for semantic mapping and robot navigation on triangle meshes in rough terrain outdoor environments by integrating our tools into an existing navigation stack.
Keywords: ROS; Triangle meshes; RViz; 3D mesh navigation

Lara Quijano-Sánchez, Iván Cantador, María E. Cortés-Cediel, Olga Gil,
Recommender systems for smart cities,
Information Systems,
Volume 92,
2020,
101545,
ISSN 0306-4379,
https://doi.org/10.1016/j.is.2020.101545.
(https://www.sciencedirect.com/science/article/pii/S0306437920300478)
Abstract: Among other conceptualizations, smart cities have been defined as functional urban areas articulated by the use of Information and Communication Technologies (ICT) and modern infrastructures to face city problems in efficient and sustainable ways. Within ICT, recommender systems are strong tools that filter relevant information, upgrading the relations between stakeholders in the polity and civil society, and assisting in decision making tasks through technological platforms. There are scientific articles covering recommendation approaches in smart city applications, and there are recommendation solutions implemented in real world smart city initiatives. However, to the best of our knowledge, there is not a comprehensive review of the state of the art on recommender systems for smart cities. For this reason, in this paper we present a taxonomy of smart city features, dimensions, actions and goals, and, according to these variables, we survey the existing literature on recommender systems. As a result of our survey, we do not only identify and analyze main research trends, but also show current opportunities and challenges where personalized recommendations could be exploited as solutions for citizens, firms and public administrations.
Keywords: Recommender systems; Smart cities; Urban computing; Smart sensors; Internet of Things; Open data

José-Luis Sierra,
Special issue on eLearning Software Architectures,
Science of Computer Programming,
Volume 129,
2016,
Pages 1-2,
ISSN 0167-6423,
https://doi.org/10.1016/j.scico.2016.08.001.
(https://www.sciencedirect.com/science/article/pii/S0167642316300910)

Carol Friedman, Thomas C. Rindflesch, Milton Corn,
Natural language processing: State of the art and prospects for significant progress, a workshop sponsored by the National Library of Medicine,
Journal of Biomedical Informatics,
Volume 46, Issue 5,
2013,
Pages 765-773,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2013.06.004.
(https://www.sciencedirect.com/science/article/pii/S1532046413000798)
Abstract: Natural language processing (NLP) is crucial for advancing healthcare because it is needed to transform relevant information locked in text into structured data that can be used by computer processes aimed at improving patient care and advancing medicine. In light of the importance of NLP to health, the National Library of Medicine (NLM) recently sponsored a workshop to review the state of the art in NLP focusing on text in English, both in biomedicine and in the general language domain. Specific goals of the NLM-sponsored workshop were to identify the current state of the art, grand challenges and specific roadblocks, and to identify effective use and best practices. This paper reports on the main outcomes of the workshop, including an overview of the state of the art, strategies for advancing the field, and obstacles that need to be addressed, resulting in recommendations for a research agenda intended to advance the field.
Keywords: Natural language processing; Biomedical language processing

E. Parimbelli, S. Wilk, R. Cornet, P. Sniatala, K. Sniatala, S.L.C. Glaser, I. Fraterman, A.H Boekhout, M. Ottaviano, M. Peleg,
A review of AI and Data Science support for cancer management,
Artificial Intelligence in Medicine,
Volume 117,
2021,
102111,
ISSN 0933-3657,
https://doi.org/10.1016/j.artmed.2021.102111.
(https://www.sciencedirect.com/science/article/pii/S0933365721001044)
Abstract: Introduction
Thanks to improvement of care, cancer has become a chronic condition. But due to the toxicity of treatment, the importance of supporting the quality of life (QoL) of cancer patients increases. Monitoring and managing QoL relies on data collected by the patient in his/her home environment, its integration, and its analysis, which supports personalization of cancer management recommendations. We review the state-of-the-art of computerized systems that employ AI and Data Science methods to monitor the health status and provide support to cancer patients managed at home.
Objective
Our main objective is to analyze the literature to identify open research challenges that a novel decision support system for cancer patients and clinicians will need to address, point to potential solutions, and provide a list of established best-practices to adopt.
Methods
We designed a review study, in compliance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, analyzing studies retrieved from PubMed related to monitoring cancer patients in their home environments via sensors and self-reporting: what data is collected, what are the techniques used to collect data, semantically integrate it, infer the patient’s state from it and deliver coaching/behavior change interventions.
Results
Starting from an initial corpus of 819 unique articles, a total of 180 papers were considered in the full-text analysis and 109 were finally included in the review. Our findings are organized and presented in four main sub-topics consisting of data collection, data integration, predictive modeling and patient coaching.
Conclusion
Development of modern decision support systems for cancer needs to utilize best practices like the use of validated electronic questionnaires for quality-of-life assessment, adoption of appropriate information modeling standards supplemented by terminologies/ontologies, adherence to FAIR data principles, external validation, stratification of patients in subgroups for better predictive modeling, and adoption of formal behavior change theories. Open research challenges include supporting emotional and social dimensions of well-being, including PROs in predictive modeling, and providing better customization of behavioral interventions for the specific population of cancer patients.
Keywords: Cancer; Decision support system; Data science; Data integration; Patient reported outcomes; Quality of life; Artificial intelligence; Predictive modeling; Patient coaching

Qian Zhu, Robert R. Freimuth, Jyotishman Pathak, Matthew J. Durski, Christopher G. Chute,
Disambiguation of PharmGKB drug–disease relations with NDF-RT and SPL,
Journal of Biomedical Informatics,
Volume 46, Issue 4,
2013,
Pages 690-696,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2013.05.005.
(https://www.sciencedirect.com/science/article/pii/S1532046413000713)
Abstract: PharmGKB is a leading resource of high quality pharmacogenomics data that provides information about how genetic variations modulate an individual’s response to drugs. PharmGKB contains information about genetic variations, pharmacokinetic and pharmacodynamic pathways, and the effect of variations on drug-related phenotypes. These relationships are represented using very general terms, however, and the precise semantic relationships among drugs, and diseases are not often captured. In this paper we develop a protocol to detect and disambiguate general clinical associations between drugs and diseases using more precise annotation terms from other data sources. PharmGKB provides very detailed clinical associations between genetic variants and drug response, including genotype-specific drug dosing guidelines, and this procedure will armGKB. The availability of more detailed data will help investigators to conduct more precise queries, such as finding particular diseases caused or treated by a specific drug. We first mapped drugs extracted from PharmGKB drug–disease relationships to those in the National Drug File Reference Terminology (NDF-RT) and to Structured Product Labels (SPLs). Specifically, we retrieved drug and disease role relationships describing and defining concepts according to their relationships with other concepts from NDF-RT. We also used the NCBO (National Center for Biomedical Ontology) annotator to annotate disease terms from the free text extracted from five SPL sections (indication, contraindication, ADE, precaution, and warning). Finally, we used the detailed drug and disease relationship information from NDF-RT and the SPLs to annotate and disambiguate the more general PharmGKB drug and disease associations.
Keywords: Pharmacogenomics; Clinical associations; PharmGKB; NDF-RT; SPL

Jemma L. König, Annika Hinze, Judy Bowen,
Workload categorization for hazardous industries: The semantic modelling of multi-modal physiological data,
Future Generation Computer Systems,
Volume 141,
2023,
Pages 369-381,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.11.019.
(https://www.sciencedirect.com/science/article/pii/S0167739X22003806)
Abstract: The forestry industry is one of the most hazardous industries in New Zealand, and the physical and cognitive fatigue of forestry workers has been shown to contribute to this. Physical and cognitive fatigue can be exacerbated by prolonged physical and cognitive workload. As such, we propose that the identification and mitigation of fatigue factors could reduce the risk of incident and injury in hazardous work environments. This paper introduces a semantic model for workload categorization. The model takes as input, a set of multi-modal physiological measurements, and uses parallel processing, complex event processing, and rule-based modelling to categorize a series of workloads (resting, cognitive workload, and physical workload). The model has undergone a set of evaluations, including categorization accuracy, and performance. The model has been tested under three scenarios: when a participant is resting and refraining from any physically or mentally demanding tasks; when a participant is undertaking a cognitively intensive task; and when a participant is walking, jogging, and running. The study has been conducted with participants between the ages of 22 and 39 and has shown an average accuracy of 89% for resting workload, 76% for cognitive workload, and 97% for physical workload. Finally, in this paper we discuss the application and extension of this model to predict fatigue in hazardous industries. The work described in this paper contributes to a larger research project centered on investigating technology uses in hazardous work environments.
Keywords: Semantic modelling; Complex event processing; Parallel processing; Rule-based modelling; Multi-modal data; Physiological data

Wei Ding, Peng Liang, Antony Tang, Hans van Vliet,
Knowledge-based approaches in software documentation: A systematic literature review,
Information and Software Technology,
Volume 56, Issue 6,
2014,
Pages 545-567,
ISSN 0950-5849,
https://doi.org/10.1016/j.infsof.2014.01.008.
(https://www.sciencedirect.com/science/article/pii/S0950584914000196)
Abstract: Context
Software documents are core artifacts produced and consumed in documentation activity in the software lifecycle. Meanwhile, knowledge-based approaches have been extensively used in software development for decades, however, the software engineering community lacks a comprehensive understanding on how knowledge-based approaches are used in software documentation, especially documentation of software architecture design.
Objective
The objective of this work is to explore how knowledge-based approaches are employed in software documentation, their influences to the quality of software documentation, and the costs and benefits of using these approaches.
Method
We use a systematic literature review method to identify the primary studies on knowledge-based approaches in software documentation, following a pre-defined review protocol.
Results
Sixty studies are finally selected, in which twelve quality attributes of software documents, four cost categories, and nine benefit categories of using knowledge-based approaches in software documentation are identified. Architecture understanding is the top benefit of using knowledge-based approaches in software documentation. The cost of retrieving information from documents is the major concern when using knowledge-based approaches in software documentation.
Conclusions
The findings of this review suggest several future research directions that are critical and promising but underexplored in current research and practice: (1) there is a need to use knowledge-based approaches to improve the quality attributes of software documents that receive less attention, especially credibility, conciseness, and unambiguity; (2) using knowledge-based approaches with the knowledge content in software documents which gets less attention in current applications of knowledge-based approaches in software documentation, to further improve the practice of software documentation activity; (3) putting more focus on the application of software documents using the knowledge-based approaches (knowledge reuse, retrieval, reasoning, and sharing) in order to make the most use of software documents; and (4) evaluating the costs and benefits of using knowledge-based approaches in software documentation qualitatively and quantitatively.
Keywords: Knowledge-based approach; Software documentation; Systematic literature review; Knowledge activity; Software architecture design

Liu Zheng, Zhang Caiming, Chen Caixian,
MMDF-LDA: An improved Multi-Modal Latent Dirichlet Allocation model for social image annotation,
Expert Systems with Applications,
Volume 104,
2018,
Pages 168-184,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2018.03.014.
(https://www.sciencedirect.com/science/article/pii/S0957417418301544)
Abstract: Social image annotation, which aims at inferring a set of semantic concepts for a social image, is an effective and straightforward way to facilitate social image search. Conventional approaches mainly demonstrated on adopting the visual features and tags, without considering other types of metadata. How to enhance the accuracy of social image annotation by fully exploiting multi-modal features is still an opening and challenging problem. In this paper, we propose an improved Multi-Modal Data Fusion based Latent Dirichlet Allocation (LDA) topic model (MMDF-LDA) to annotate social images via fusing visual content, user-supplied tags, user comments, and geographic information. When MMDF-LDA samples annotations for one data modality, all the other data modalities are exploited. In MMDF-LDA, geographical topics are generated from GPS locations of social images, and annotations have different probability to be used in different geographical regions. A social image is divided into several patches in advance, and then MMDF-LDA assigns annotations for the patches of social images by estimating the probability of annotation-patch assignment. Through experiments in social image annotation and retrieval on several datasets, we demonstrate the effectiveness of the proposed MMDF-LDA model in comparison with state-of-the-art methods.
Keywords: Social image; Multi-modal data fusion; LDA model; Semantic annotation; Geographical topic

Irena Spasić, Jacqueline Livsey, John A. Keane, Goran Nenadić,
Text mining of cancer-related information: Review of current status and future directions,
International Journal of Medical Informatics,
Volume 83, Issue 9,
2014,
Pages 605-623,
ISSN 1386-5056,
https://doi.org/10.1016/j.ijmedinf.2014.06.009.
(https://www.sciencedirect.com/science/article/pii/S1386505614001105)
Abstract: Purpose
This paper reviews the research literature on text mining (TM) with the aim to find out (1) which cancer domains have been the subject of TM efforts, (2) which knowledge resources can support TM of cancer-related information and (3) to what extent systems that rely on knowledge and computational methods can convert text data into useful clinical information. These questions were used to determine the current state of the art in this particular strand of TM and suggest future directions in TM development to support cancer research.
Methods
A review of the research on TM of cancer-related information was carried out. A literature search was conducted on the Medline database as well as IEEE Xplore and ACM digital libraries to address the interdisciplinary nature of such research. The search results were supplemented with the literature identified through Google Scholar.
Results
A range of studies have proven the feasibility of TM for extracting structured information from clinical narratives such as those found in pathology or radiology reports. In this article, we provide a critical overview of the current state of the art for TM related to cancer. The review highlighted a strong bias towards symbolic methods, e.g. named entity recognition (NER) based on dictionary lookup and information extraction (IE) relying on pattern matching. The F-measure of NER ranges between 80% and 90%, while that of IE for simple tasks is in the high 90s. To further improve the performance, TM approaches need to deal effectively with idiosyncrasies of the clinical sublanguage such as non-standard abbreviations as well as a high degree of spelling and grammatical errors. This requires a shift from rule-based methods to machine learning following the success of similar trends in biological applications of TM. Machine learning approaches require large training datasets, but clinical narratives are not readily available for TM research due to privacy and confidentiality concerns. This issue remains the main bottleneck for progress in this area. In addition, there is a need for a comprehensive cancer ontology that would enable semantic representation of textual information found in narrative reports.
Keywords: Cancer; Natural language processing; Data mining; Electronic medical records

Georg Weichhart, Hervé Panetto, Arturo Molina,
Interoperability in the cyber-physical manufacturing enterprise,
Annual Reviews in Control,
Volume 51,
2021,
Pages 346-356,
ISSN 1367-5788,
https://doi.org/10.1016/j.arcontrol.2021.03.006.
(https://www.sciencedirect.com/science/article/pii/S1367578821000146)
Abstract: New technologies supporting cyber-physical enterprise systems with respect to online decision-making based on up-to-date data, require networked sensor and actor systems in place. Interoperability is a key factor when supporting systems in a system-of-systems. In this paper, we survey approaches on Enterprise Interoperability with special attention to the Cyber-Physical Manufacturing Enterprise. The paper identifies the need for interoperability in system-of-systems in contrast to integration in a single system. Also identified are issues due to insufficient support for physical aspects of systems. An application scenario from the manufacturing domain will serve to underpin the developed approach.
Keywords: Systems interoperability; Enterprise integration; Internet-of-Things and sensing enterprise; Cyber-physical systems; Enterprise interoperability

Sivadi Balakrishna, M. Thirumaran,
Chapter 7 - Semantic interoperability in IoT and big data for health care: a collaborative approach,
Editor(s): Valentina Emilia Balas, Vijender Kumar Solanki, Raghvendra Kumar, Manju Khari,
Handbook of Data Science Approaches for Biomedical Engineering,
Academic Press,
2020,
Pages 185-220,
ISBN 9780128183182,
https://doi.org/10.1016/B978-0-12-818318-2.00007-6.
(https://www.sciencedirect.com/science/article/pii/B9780128183182000076)
Abstract: SI is used to exchange the information from one place to another place in an efficient and meaningful way. The data is generated from various heterogeneous devices, communication protocols, and data formats that are enormous in nature. This is a significant problem for Internet of things (IoT) application developers to make the IoT generated data interoperable. In the existing approaches there is lack of well-defined standards and established tools to solve semantic interoperability (SI) problem in IoT and big data applications. This chapter proposes a collaborative approach to address the SI in IoT and big data for health care applications. In the health care domain, the physicians and patients may interoperate with each other effectively and conveniently. Both IoT and big data are dominant technologies for health care applications. This chapter mainly deals with two use cases, namely (1) IoT in health care systems and (2) big data analytics in health care systems. Gruff and Tableau tools were used for performing experiments and analysis on health care data. The obtained results are convincing and support both patients’ and physicians’ health care data as semantically interoperable. This chapter summarizes, with supporting SI, the tools and developing methodologies in both IoT and big data analytics technologies for health care applications.
Keywords: Big data; Health care; IoT; semantic interoperability; Use case

Janne Parkkila, Jouni Ikonen, Jari Porras,
Where is the research on connecting game ​ worlds?—A systematic mapping study,
Computer Science Review,
Volume 18,
2015,
Pages 46-58,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2015.10.002.
(https://www.sciencedirect.com/science/article/pii/S1574013715300289)
Keywords: Connecting video games; Connecting virtual worlds; Interoperability; Mapping study; Systematic literature review

Nicolas Fiorini, Sébastien Harispe, Sylvie Ranwez, Jacky Montmain, Vincent Ranwez,
Fast and reliable inference of semantic clusters,
Knowledge-Based Systems,
Volume 111,
2016,
Pages 133-143,
ISSN 0950-7051,
https://doi.org/10.1016/j.knosys.2016.08.008.
(https://www.sciencedirect.com/science/article/pii/S0950705116302684)
Abstract: Document Indexing is but not limited to summarizing document contents with a small set of keywords or concepts of a knowledge base. Such a compact representation of document contents eases their use in numerous processes such as content-based information retrieval, corpus-mining and classification. An important effort has been devoted in recent years to (partly) automate semantic indexing, i.e. associating concepts to documents, leading to the availability of large corpora of semantically indexed documents. In this paper we introduce a method that hierarchically clusters documents based on their semantic indices while providing the proposed clusters with semantic labels. Our approach follows a neighbor joining strategy. Starting from a distance matrix reflecting the semantic similarity of documents, it iteratively selects the two closest clusters to merge them in a larger one. The similarity matrix is then updated. This is usually done by combining similarity of the two merged clusters, e.g. using the average similarity. We propose in this paper an alternative approach where the new cluster is first semantically annotated and the similarity matrix is then updated using the semantic similarity of this new annotation with those of the remaining clusters. The hierarchical clustering so obtained is a binary tree with branch lengths that convey semantic distances of clusters. It is then post-processed by using the branch lengths to keep only the most relevant clusters. Such a tool has numerous practical applications as it automates the organization of documents in meaningful clusters (e.g. papers indexed by MeSH terms, bookmarks or pictures indexed by WordNet) which is a tedious everyday task for many people. We assess the quality of the proposed methods using a specific benchmark of annotated clusters of bookmarks that were built manually. Each dataset of this benchmark has been clustered independently by several users. Remarkably, the clusters automatically built by our method are congruent with the clusters proposed by experts. All resources of this work, including source code, jar file, benchmark files and results are available at this address: http://sc.nicolasfiorini.info.
Keywords: Clustering; Cluster labeling; Semantic indexing; Neighbor joining; Complexity analysis

V. Curcin, S. Miles, R. Danger, Y. Chen, R. Bache, A. Taweel,
Implementing interoperable provenance in biomedical research,
Future Generation Computer Systems,
Volume 34,
2014,
Pages 1-16,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2013.12.001.
(https://www.sciencedirect.com/science/article/pii/S0167739X13002653)
Abstract: The provenance of a piece of data refers to knowledge about its origin, in terms of the entities and actors involved in its creation, e.g. data sources used, operations carried out on them, and users enacting those operations. Provenance is used to better understand the data and the context of its production, and to assess its reliability, by asserting whether correct procedures were followed. Providing evidence for validating research is of particular importance in the biomedical domain, where the strength of the results depends on the data sources and processes used. In recent times, previously manual processes have become fully or semi-automated, e.g. clinical trial recruitment, epidemiological studies, diagnosis making. The latter is typically achieved through interactions of heterogeneous software systems in multiple settings (hospitals, clinics, academic and industrial research organisations). Provenance traces of these software need to be integrated in a consistent and meaningful manner, but since these software systems rarely share a common platform, the provenance interoperability between them has to be achieved on the level of conceptual models. It is a non-trivial matter to determine where to start in making a biomedical software system provenance-aware. In this paper, we specify recommendations to developers on how to approach provenance modelling, capture, security, storage and querying, based on our experiences with two large-scale biomedical research projects: Translational Research and Patient Safety in Europe (TRANSFoRm) and Electronic Health Records for Clinical Research (EHR4CR). While illustrated with concrete issues encountered, the recommendations are of a sufficiently high level so as to be reusable across the biomedical domain.
Keywords: Provenance; Biomedical informatics

Yuri Ferretti, Newton Shydeo Brandão Miyoshi, Wilson Araújo Silva, Joaquim Cezar Felipe,
BioBankWarden: A web-based system to support translational cancer research by managing clinical and biomaterial data,
Computers in Biology and Medicine,
Volume 84,
2017,
Pages 254-261,
ISSN 0010-4825,
https://doi.org/10.1016/j.compbiomed.2015.04.008.
(https://www.sciencedirect.com/science/article/pii/S0010482515001262)
Abstract: Background
Researchers of translational medicine face numerous challenges in attempting to bring research results to the bedside. This field of research covers a wide range of resources, including blood and tissue samples, which are processed for isolation of RNA and DNA to study cancer omics data (genomics, proteomics and metabolomics). Clinical information about patients׳ habits, family history, physical examinations, remissions, etc., is also important to underpin studies aimed at identifying patterns that lead to the development of cancer and to its successful treatment.
Purpose
Development of a web-based computer system—BioBankWarden—to manage, consolidate and integrate these diversified data, enabling cancer research groups to retrieve and analyze clinical and biomolecular data within an integrative environment. The system has a three-tier architecture comprising database, logic and user-interface layers.
Results
The system׳s integrated database and user-friendly interface allow for the control of patient records, biomaterial storage, research groups, research projects, users and biomaterial exchange.
Conclusions
BioBankWarden can be used to store and retrieve specific information from different clinical fields linked to biomaterials collected from patients, providing the functionalities required to support translational research in the field of cancer.
Keywords: Translational medicine; Biological databases; Oncogenomics; Biomaterial management; Clinical data management

Paul Johannesson, Mong Li Lee, Stephen Liddle, Andreas L. Opdahl, Oscar Pastor,
Special issue on conceptual modeling – 34th International Conference on Conceptual Modeling (ER 2015),
Data & Knowledge Engineering,
Volume 109,
2017,
Pages 1-2,
ISSN 0169-023X,
https://doi.org/10.1016/j.datak.2017.03.001.
(https://www.sciencedirect.com/science/article/pii/S0169023X17301040)

Kehua Guo, Yayuan Tang, Peiyun Zhang,
CSF: Crowdsourcing semantic fusion for heterogeneous media big data in the internet of things,
Information Fusion,
Volume 37,
2017,
Pages 77-85,
ISSN 1566-2535,
https://doi.org/10.1016/j.inffus.2017.01.008.
(https://www.sciencedirect.com/science/article/pii/S1566253517300702)
Abstract: With the rising popularity of social media in the context of environments based on the Internet of things (IoT), semantic information has emerged as an important bridge to connect human intelligence with heterogeneous media big data. As a critical tool to improve media big data retrieval, semantic fusion encounters a number of challenges: the manual method is inefficient, and the automatic approach is inaccurate. To address these challenges, this paper proposes a solution called CSF (Crowdsourcing Semantic Fusion) that makes full use of the collective wisdom of social users and introduces crowdsourcing computing to semantic fusion. First, the correlation of cross-modal semantics is mined and the semantic objects are normalized for fusion. Second, we employ the dimension reduction and relevance feedback approaches to reduce non-principal components and noise. Finally, we research the storage and distribution mechanism. Experiment results highlight the efficiency and accuracy of the proposed approach. The proposed method is an effective and practical cross-modal semantic fusion and distribution mechanism for heterogeneous social media, provides a novel idea for social media semantic processing, and uses an interactive visualization framework for social media knowledge mining and retrieval to improve semantic knowledge and the effect of representation.
Keywords: Crowdsourcing computing; Semantic fusion; Social media; Big data; Internet of things


Contents,
Procedia Computer Science,
Volume 35,
2014,
Pages iii-ix,
ISSN 1877-0509,
https://doi.org/10.1016/S1877-0509(14)01235-6.
(https://www.sciencedirect.com/science/article/pii/S1877050914012356)

Jesus Arias Fisteus, Norberto Fernández García, Luis Sánchez Fernández, Damaris Fuentes-Lorenzo,
Ztreamy: A middleware for publishing semantic streams on the Web,
Journal of Web Semantics,
Volume 25,
2014,
Pages 16-23,
ISSN 1570-8268,
https://doi.org/10.1016/j.websem.2013.11.002.
(https://www.sciencedirect.com/science/article/pii/S1570826813000772)
Abstract: In order to make the semantic sensor Web a reality, middleware for efficiently publishing semantically-annotated data streams on the Web is needed. Such middleware should be designed to allow third parties to reuse and mash-up data coming from streams. These third parties should even be able to publish their own value-added streams derived from other streams and static data. In this work we present Ztreamy, a scalable middleware platform for the distribution of semantic data streams through HTTP. The platform provides an API for both publishing and consuming streams, as well as built-in filtering services based on data semantics. A key contribution of our proposal with respect to other related systems in the state of the art is its scalability. Our experiments with Ztreamy show that a single server is able, in some configurations, to publish a real-time stream to up to 40000 simultaneous clients with delivery delays of just a few seconds, largely outperforming other systems in the state of the art.
Keywords: Semantic sensor web; Stream distribution middleware; Semantic stream

Alberto G. Jácome, Florentino Fdez-Riverola, Anália Lourenço,
BIOMedical Search Engine Framework: Lightweight and customized implementation of domain-specific biomedical search engines,
Computer Methods and Programs in Biomedicine,
Volume 131,
2016,
Pages 63-77,
ISSN 0169-2607,
https://doi.org/10.1016/j.cmpb.2016.03.030.
(https://www.sciencedirect.com/science/article/pii/S0169260715300560)
Abstract: Background and objectives
Text mining and semantic analysis approaches can be applied to the construction of biomedical domain-specific search engines and provide an attractive alternative to create personalized and enhanced search experiences. Therefore, this work introduces the new open-source BIOMedical Search Engine Framework for the fast and lightweight development of domain-specific search engines. The rationale behind this framework is to incorporate core features typically available in search engine frameworks with flexible and extensible technologies to retrieve biomedical documents, annotate meaningful domain concepts, and develop highly customized Web search interfaces.
Methods
The BIOMedical Search Engine Framework integrates taggers for major biomedical concepts, such as diseases, drugs, genes, proteins, compounds and organisms, and enables the use of domain-specific controlled vocabulary. Technologies from the Typesafe Reactive Platform, the AngularJS JavaScript framework and the Bootstrap HTML/CSS framework support the customization of the domain-oriented search application. Moreover, the RESTful API of the BIOMedical Search Engine Framework allows the integration of the search engine into existing systems or a complete web interface personalization.
Results
The construction of the Smart Drug Search is described as proof-of-concept of the BIOMedical Search Engine Framework. This public search engine catalogs scientific literature about antimicrobial resistance, microbial virulence and topics alike. The keyword-based queries of the users are transformed into concepts and search results are presented and ranked accordingly. The semantic graph view portraits all the concepts found in the results, and the researcher may look into the relevance of different concepts, the strength of direct relations, and non-trivial, indirect relations. The number of occurrences of the concept shows its importance to the query, and the frequency of concept co-occurrence is indicative of biological relations meaningful to that particular scope of research. Conversely, indirect concept associations, i.e. concepts related by other intermediary concepts, can be useful to integrate information from different studies and look into non-trivial relations.
Conclusions
The BIOMedical Search Engine Framework supports the development of domain-specific search engines. The key strengths of the framework are modularity and extensibilityin terms of software design, the use of open-source consolidated Web technologies, and the ability to integrate any number of biomedical text mining tools and information resources. Currently, the Smart Drug Search keeps over 1,186,000 documents, containing more than 11,854,000 annotations for 77,200 different concepts. The Smart Drug Search is publicly accessible at http://sing.ei.uvigo.es/sds/. The BIOMedical Search Engine Framework is freely available for non-commercial use at https://github.com/agjacome/biomsef.
Keywords: Search engine framework; Biomedical literature; Vertical engine; Text mining; Web application

Marcos Nieto, Orti Senderos, Oihana Otaegui,
Boosting AI applications: Labeling format for complex datasets,
SoftwareX,
Volume 13,
2021,
100653,
ISSN 2352-7110,
https://doi.org/10.1016/j.softx.2020.100653.
(https://www.sciencedirect.com/science/article/pii/S2352711020303666)
Abstract: Data labeling has become a major problem in industries aiming to create and use ground truth labels from massive multi-sensor archives to feed into Artificial Intelligence (AI) applications. Annotation of multi-sensor set-ups with multiple cameras and LIDAR is now particularly relevant for the automotive industry aiming to build Autonomous Driving (AD) functions. In this paper, we present the Video Content Description (VCD), as the first open source metadata structure and set of tools, able to structure annotations for such complex scenes, including unprecedented flexibility to label 2D and 3D objects, pixel-wise labels, actions, events, contexts, semantic relations, odometry, and calibration. Several example cases are reported to demonstrate the flexibility of the VCD.
Keywords: Annotation; Dataset; Multi-sensor; Automotive

Mario José Diván, María Laura Sánchez-Reynoso,
1 - The impact of Internet of Things and data semantics on decision making for outpatient monitoring,
Editor(s): Jitendra Kumar Verma, Sudip Paul, Prashant Johri,
Computational Intelligence and Its Applications in Healthcare,
Academic Press,
2020,
Pages 1-15,
ISBN 9780128206041,
https://doi.org/10.1016/B978-0-12-820604-1.00001-7.
(https://www.sciencedirect.com/science/article/pii/B9780128206041000017)
Abstract: The Internet of Things (IoT) is useful for data collection due to its portability, low cost, and the wide range of heterogeneous devices available for field deployment. Outpatients are a natural group to be monitored using IoT devices. Data semantics is a key topic in this regard, related mainly to risk prevention and support for data-driven decision making. In this chapter, an application of a measurement framework with the support of states and scenarios is introduced as a mean of homogenizing data semantics independently of the collector device. The application is focused on monitoring of outpatients engaged in physical activities outdoor. The project definition is detailed, along with the devices used in the implementation. The main contribution of this work is the integration of heterogeneous IoT devices through a measurement framework with the support of multiple scenarios and states, with data semantics used to guide the real-time data processing.
Keywords: Internet of Things; Data; Decision making; Real-time; Monitoring

Philip R.O. Payne,
Chapter 2 - From data to knowledge: an introduction to biomedical informatics,
Editor(s): Geoffrey S. Ginsburg, Huntington F. Willard, John H. Strickler, Matthew S. McKinney,
Genomic and Precision Medicine (Third Edition),
Academic Press,
2022,
Pages 13-28,
ISBN 9780128006849,
https://doi.org/10.1016/B978-0-12-800684-9.00010-1.
(https://www.sciencedirect.com/science/article/pii/B9780128006849000101)
Abstract: The field of Biomedical Informatics (BMI) is concerned with multimethod approaches to generating contextualized information and actionable knowledge from a variety of biological and healthcare-relevant data types. In doing so, BMI practioners adopt and adapt a number of methods drawn from the computational, quantitative, and qualitative sciences. In this chapter, we provide an overview of such methods and a rationale for how they can be applied to address driving biological and clinical problems. Such use cases span a range from the bio-molecular characterization of disease states to the comprehensive phenotyping of patients to the promotion of population health. In doing so, we hope to equip readers with the ability to critically understand and evaluate the use of multimethod approaches as are commonly encountered in the aforementioned application areas.
Keywords: Biomedical informatics; research design; evaluation; computer science; data science; qualitative research

Han van der Aa, Henrik Leopold, Adela del-Río-Ortega, Manuel Resinas, Hajo A. Reijers,
Transforming unstructured natural language descriptions into measurable process performance indicators using Hidden Markov Models,
Information Systems,
Volume 71,
2017,
Pages 27-39,
ISSN 0306-4379,
https://doi.org/10.1016/j.is.2017.06.005.
(https://www.sciencedirect.com/science/article/pii/S0306437916304008)
Abstract: Monitoring process performance is an important means for organizations to identify opportunities to improve their operations. The definition of suitable Process Performance Indicators (PPIs) is a crucial task in this regard. Because PPIs need to be in line with strategic business objectives, the formulation of PPIs is a managerial concern. Managers typically start out to provide relevant indicators in the form of natural language PPI descriptions. Therefore, considerable time and effort have to be invested to transform these descriptions into PPI definitions that can actually be monitored. This work presents an approach that automates this task. The presented approach transforms an unstructured natural language PPI description into a structured notation that is aligned with the implementation underlying a business process. To do so, we combine Hidden Markov Models and semantic matching techniques. A quantitative evaluation on the basis of a data collection obtained from practice demonstrates that our approach works accurately. Therefore, it represents a viable automated alternative to an otherwise laborious manual endeavor.
Keywords: Performance measurement; Process Performance Indicators; Natural language processing; Hidden Markov Models; Model alignment

Perla Velasco-Elizondo, Rosario Marín-Piña, Sodel Vazquez-Reyes, Arturo Mora-Soto, Jezreel Mejia,
Knowledge representation and information extraction for analysing architectural patterns,
Science of Computer Programming,
Volume 121,
2016,
Pages 176-189,
ISSN 0167-6423,
https://doi.org/10.1016/j.scico.2015.12.007.
(https://www.sciencedirect.com/science/article/pii/S0167642316000101)
Abstract: Today, many software architecture design methods consider the use of architectural patterns as a fundamental design concept. When making an effective pattern selection, software architects must consider, among other aspects, its impact on promoting or inhibiting quality attributes. However, for inexperienced architects, this task often requires significant time and effort. Some reasons of the former include: the number of existing patterns, the emergence of new patterns, the heterogeneity in the natural language descriptions used to define them and the lack of tools for automatic pattern analysis. In this paper we describe an approach, based on knowledge representation and information extraction, for analysing architectural pattern descriptions with respect to specific quality attributes. The approach is automated by computable model that works as a prototype tool. We focus on the performance quality attribute and, by performing experiments on a corpus of patterns with forty-five architects of varying levels of experience, demonstrate that the proposed approach increases recall and reduces analysis time compared to manual analysis.
Keywords: Architectural design; Architectural patterns; Quality attribute; Ontology; Information extraction

A. Reyana, Sandeep Kautish, Yogita Gupta,
Chapter 6 - Emergence of decision support systems in healthcare,
Editor(s): Pradeep N, Sandeep Kautish, Sheng-Lung Peng,
Demystifying Big Data, Machine Learning, and Deep Learning for Healthcare Analytics,
Academic Press,
2021,
Pages 109-128,
ISBN 9780128216330,
https://doi.org/10.1016/B978-0-12-821633-0.00004-0.
(https://www.sciencedirect.com/science/article/pii/B9780128216330000040)
Abstract: Clinical decision support (CDS) presents knowledge and person-specific information to enhance decision-making, including patient data reports, diagnostic support, and guidelines, in the clinical workflow. The emergence of CDS improves the safety, quality, and effectiveness in health IT. CDS combines knowledge and data to filter, organize, and present clinical data. Healthcare IT integrates CDS applications as components of comprehensive EHR systems, though standalone CDS systems are available. It uses the hidden insights of big data to sift a huge amount of health information to diagnose and provide alerts on dangerous medication interactions. A study by JAMA Internal Medicine found that in 2016, clinical centers received around 76.9 notifications daily, including lab results and pharmacies adding CDS alerts. Today, health IT provides many aids for decision-making, including reminders and alerts to patients, through leveraging an organization’s big data assets. Following the implementation of surveillance, healthcare in countries like Alabama lowered sepsis mortality rates by 53%. Patients learned about their risk-reducing cost-saving of $4300 per capital by testing interactions on drug genes. The other benefits from CDS tools include the calculation of the drug-dosage specific illness severity index, the electronic health record (EHR) input conditions identified and reported, the educational resource materials, etc. An agency says that “CDS implementation and adoption cannot begin until the organization has (its) own clinical goals and priorities.” The objective of this chapter is to bring insight into CDS and its potential benefits despite the integration challenges as well as evaluating patient care quality including patient safety with the use of CDSS and minimizing errors to enhance human support. Today, healthcare professionals face stress in improving the quality of care while low-cost techniques for the adoption of CDS are discussed for their effective deployment.
Keywords: Clinical decision support; Diagnosis; Health infrastructure; Information technology

Jianlei Gu, Jiawei Dai, Hui Lu, Hongyu Zhao,
Comprehensive Analysis of Ubiquitously Expressed Genes in Humans from A Data-driven Perspective,
Genomics, Proteomics & Bioinformatics,
Volume 21, Issue 1,
2023,
Pages 164-176,
ISSN 1672-0229,
https://doi.org/10.1016/j.gpb.2021.08.017.
(https://www.sciencedirect.com/science/article/pii/S1672022922000420)
Abstract: Comprehensive characterization of spatial and temporal gene expression patterns in humans is critical for uncovering the regulatory codes of the human genome and understanding the molecular mechanisms of human diseases. Ubiquitously expressed genes (UEGs) refer to the genes expressed across a majority of, if not all, phenotypic and physiological conditions of an organism. It is known that many human genes are broadly expressed across tissues. However, most previous UEG studies have only focused on providing a list of UEGs without capturing their global expression patterns, thus limiting the potential use of UEG information. In this study, we proposed a novel data-driven framework to leverage the extensive collection of ∼ 40,000 human transcriptomes to derive a list of UEGs and their corresponding global expression patterns, which offers a valuable resource to further characterize human transcriptome. Our results suggest that about half (12,234; 49.01%) of the human genes are expressed in at least 80% of human transcriptomes, and the median size of the human transcriptome is 16,342 genes (65.44%). Through gene clustering, we identified a set of UEGs, named LoVarUEGs, which have stable expression across human transcriptomes and can be used as internal reference genes for expression measurement. To further demonstrate the usefulness of this resource, we evaluated the global expression patterns for 16 previously predicted disallowed genes in islet beta cells and found that seven of these genes showed relatively more varied expression patterns, suggesting that the repression of these genes may not be unique to islet beta cells.
Keywords: Ubiquitous expression; Housekeeping gene; Disallowed gene; Expression specificity; Expression variability

Isabel Segura-Bedmar, Paloma Martínez,
Pharmacovigilance through the development of text mining and natural language processing techniques,
Journal of Biomedical Informatics,
Volume 58,
2015,
Pages 288-291,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2015.11.001.
(https://www.sciencedirect.com/science/article/pii/S1532046415002385)


Binh Minh Nguyen, Huan Phan, Duong Quang Ha, Giang Nguyen,
An Information-centric Approach for Slice Monitoring from Edge Devices to Clouds,
Procedia Computer Science,
Volume 130,
2018,
Pages 326-335,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2018.04.046.
(https://www.sciencedirect.com/science/article/pii/S1877050918303995)
Abstract: Internet of Things (IoT) has enabled physical devices and virtual objects to be connected to share data, coordinate, and automatically make smart decisions to server people. Recently, many IoT resource slicing studies that allow managing devices, IoT platforms, network functions, and clouds under a single unified programming interface have been proposed. Although they helped IoT developers to create IoT services more easily, the efforts still have not dealt with the monitoring problem for the slice components. This could cause an issue: thing states could not be tracked continuously, and hence the effectiveness of controlling the IoT components would be decreased significantly because of updated information lack. In this paper, we introduce an information-centric approach for multiple sources monitoring issue in IoT. The proposed model thus is designed to provide generic and extensible data format for diverse IoT objects. Through this model, IoT developers can build smart services smoothly without worrying about the diversity as well as heterogeneity of collected data. We also propose an overall monitoring architecture for the information-centric model to deploy in IoT environment and its monitoring API prototype. This document also presents our experiments and evaluations to prove feasibility of the proposals in practice.
Keywords: Internet of Things; cloud computing; edge computing; information-centric; IoT slice monitoring; data sensing; resource utilization; activity log

F. Perez, J. Huguet, R. Aguilar, L. Lara, I. Larrabide, M.C. Villa-Uriol, J. López, J.M. Macho, A. Rigo, J. Rosselló, S. Vera, E. Vivas, J. Fernàndez, A. Arbona, A.F. Frangi, J. Herrero Jover, M.A. González Ballester,
RADStation3G: A platform for cardiovascular image analysis integrating PACS, 3D+t visualization and grid computing,
Computer Methods and Programs in Biomedicine,
Volume 110, Issue 3,
2013,
Pages 399-410,
ISSN 0169-2607,
https://doi.org/10.1016/j.cmpb.2012.12.002.
(https://www.sciencedirect.com/science/article/pii/S0169260712003112)
Abstract: RADStation3G is a software platform for cardiovascular image analysis and surgery planning. It provides image visualization and management in 2D, 3D and 3D+t; data storage (images or operational results) in a PACS (using DICOM); and exploitation of patients’ data such as images and pathologies. Further, it provides support for computationally expensive processes with grid technology. In this article we first introduce the platform and present a comparison with existing systems, according to the platform's modules (for cardiology, angiology, PACS archived enriched searching and grid computing), and then RADStation3G is described in detail.
Keywords: Segmentation; Cardiac function; Aneurysm; Diagnostics; Planning; Grid

Francesco Marino, Corrado Moiso, Matteo Petracca,
Automatic contract negotiation, service discovery and mutual authentication solutions: A survey on the enabling technologies of the forthcoming IoT ecosystems,
Computer Networks,
Volume 148,
2019,
Pages 176-195,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2018.11.011.
(https://www.sciencedirect.com/science/article/pii/S1389128618312167)
Abstract: The Internet-of-Things (IoT) paradigm envisions the full integration of several technologies in order to enable a new class of applications relying on an unprecedented sensing and actuating infrastructure. However, traditional IoT applications are static, closed, vertically integrated solutions which do not allow to fully exploit the potentialities of this new technological platform. For this reason IoT systems are now evolving towards more open and dynamic architectures and solutions in which dynamicity is considered an added value to build context-aware and self-adapting applications. In this new continuous changing scenario, solutions for automatic contract negotiation and management, service discovery and mutual authentication will play a central role, since they can be considered the main building blocks to create secure and dynamic applications. The paper surveys research activities in the these fields, pointing out the challenges and the open research issues that arise in such a new emerging scenario.
Keywords: Dynamic IoT; Distributed systems; Heterogeneous systems; Contract negotiation; Service discovery; Mutual authentication

Ales Frece, Matjaz B. Juric,
Complete and reusable description of message structural constraints in web service interfaces,
Computer Standards & Interfaces,
Volume 35, Issue 2,
2013,
Pages 218-230,
ISSN 0920-5489,
https://doi.org/10.1016/j.csi.2012.09.001.
(https://www.sciencedirect.com/science/article/pii/S0920548912000943)
Abstract: Existing specifications for describing message structure as a part of web service description do not support use case-specific definition of structural constraints. We propose a solution to describe a complete set of structural constraints for a particular business object in all its use cases. To implement our solution we use XML Schema (XSD), de facto standard for description of web service message structure. We propose XSD extensions that realize two distinct and complementary approaches. Measurements have shown that by using our extensions the average complexity of real world schemas (XSD documents) comparing to expressional equivalent alternatives is smaller by ~29%.
Keywords: Service description; XML Schema (XSD); Use case-specific structural constraints; Reuse

Jiangnan Qiu, Liwei Xu, Jie Zhai, Ling Luo,
Extracting Causal Relations from Emergency Cases Based on Conditional Random Fields,
Procedia Computer Science,
Volume 112,
2017,
Pages 1623-1632,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2017.08.252.
(https://www.sciencedirect.com/science/article/pii/S1877050917316617)
Abstract: As causality extraction from cases is essential for emergency causal learning, it serves as a foundation for follow-up emergency management. However, there remain barriers to break for applying the previous causality extraction methods to emergency management. The experience of emergency management inspires us that the cause of disasters should have existed in the time before the effect. Therefore, causality relations can be seen as distinct temporal relations. By utilizing the temporal characteristics of causality, this paper redefines the causality extraction as a special kind of temporality extraction and presents a method for extracting causality from emergency cases based on conditional random fields (CRFs). Then the task turns to be a sequence labeling process which can be solved by involving a CRFs model. Several typhoon-related emergency cases are chosen as the experimental dataset. To seek the impact of different features on the model performance, two feature templates are also chosen to train the model. The experimental results show that our approaches can not only deal with marked causal relations, but also work effectively on unmarked causal relations. Besides, the CRFs model can even extract causal relations between sentences.
Keywords: Causal Relations; Conditional Random Fields; Relations Extraction; Emergency Management


Table of Contents,
Procedia Computer Science,
Volume 130,
2018,
Pages iii-xi,
ISSN 1877-0509,
https://doi.org/10.1016/S1877-0509(18)30537-4.
(https://www.sciencedirect.com/science/article/pii/S1877050918305374)

Georgia Melagraki,
Reducing health & environmental impacts of chemical warfare agents: Computational chemistry contributions,
Chemosphere,
Volume 288, Part 2,
2022,
132564,
ISSN 0045-6535,
https://doi.org/10.1016/j.chemosphere.2021.132564.
(https://www.sciencedirect.com/science/article/pii/S0045653521030368)
Abstract: This review article summarizes advances in computational chemistry and cheminformatics methods and techniques that are used or have potential for use in reducing health and environmental impacts of Chemical Warfare Agents (CWA). These methods, include, but are not limited to, predictive modeling, data mining and virtual screening, similarity searching, molecular docking and dynamics and are briefly presented here. Applications of these in silico approaches, specifically for the protection of personnel and civilians against CWA, but also beyond, are discussed. CWA include toxic chemicals that can cause death, injury, or temporary incapacitation through their chemical action. CWA impose a significant worldwide threat and as such, destruction, remediation as well as protection measurements need to be carefully designed. Towards this goal computational chemistry and cheminformatics can play a key role specifically as far as decontamination, risk assessment and risk management are concerned. Among the wide range of in silico techniques applied for CWA, specific previously published paradigms are presented, including toxicity and property prediction, CWA simulant identification and CWA detoxification. Beyond CWA research, other applications with military interest are briefly presented and emerging trends of potential relevance noted.
Keywords: Computational chemistry; Cheminformatics; Predictive modeling; Quantitative structure activity relationships (QSAR); Molecular docking and dynamics; Molecular simulation; Chemical warfare agents; Decontamination; Risk assessment

Pablo Giménez, Benjamín Molina, Jaime Calvo-Gallego, Manuel Esteve, Carlos E. Palau,
I3WSN: Industrial Intelligent Wireless Sensor Networks for indoor environments,
Computers in Industry,
Volume 65, Issue 1,
2014,
Pages 187-199,
ISSN 0166-3615,
https://doi.org/10.1016/j.compind.2013.09.002.
(https://www.sciencedirect.com/science/article/pii/S0166361513001899)
Abstract: Sensor Web Enablement (SWE) technologies have been successfully applied to a great variety of outdoor scenarios but, in practical terms, little effort has been applied for indoor environments, and even less in the field of industrial applications. This article presents an intelligent SWE application for an indoor and industrial scenario, with the aim of improving and increasing the levels of human safety. The base low-level architecture is built on top of wireless sensor networks (WSN) connected to a Sensor Observation Service (SOS). Higher layers in the architecture include services that make real time decisions based on the collected data. Both simulation and experimental results are presented. The paper shows the viability of our approach in terms of performance, scalability, modularity and safety.
Keywords: Indoor environments; Industrial environments; Industrial safety; Sensor Observation Service (SOS); Sensor Web Enablement (SWE); Wireless Sensor Networks (WSNs)

Jason Dominiczak, Lara Khansa,
Principles of Automation for Patient Safety in Intensive Care: Learning From Aviation,
The Joint Commission Journal on Quality and Patient Safety,
Volume 44, Issue 6,
2018,
Pages 366-371,
ISSN 1553-7250,
https://doi.org/10.1016/j.jcjq.2017.11.008.
(https://www.sciencedirect.com/science/article/pii/S1553725017303422)
Abstract: Background
The transition away from written documentation and analog methods has opened up the possibility of leveraging data science and analytic techniques to improve health care. In the implementation of data science techniques and methodologies, high-acuity patients in the ICU can particularly benefit. The Principles of Automation for Patient Safety in Intensive Care (PASPIC) framework draws on Billings's principles of human-centered aviation (HCA) automation and helps in identifying the advantages, pitfalls, and unintended consequences of automation in health care.
The Framework and Its Key Characteristics
Billings's HCA principles are based on the premise that human operators must remain “in command,” so that they are continuously informed and actively involved in all aspects of system operations. In addition, automated systems need to be predictable, simple to train, to learn, and to operate, and must be able to monitor the human operators, and every intelligent system element must know the intent of other intelligent system elements. In applying Billings's HCA principles to the ICU setting, PAPSIC has three key characteristics: (1) integration and better interoperability, (2) multidimensional analysis, and (3) enhanced situation awareness.
Recommendations
PAPSIC suggests that health care professionals reduce overreliance on automation and implement “cooperative automation” and that vendors reduce mode errors and embrace interoperability.
Conclusion
Much can be learned from the aviation industry in automating the ICU. Because it combines “smart” technology with the necessary controls to withstand unintended consequences, PAPSIC could help ensure more informed decision making in the ICU and better patient care.

Na Hong, Andrew Wen, Daniel J. Stone, Shintaro Tsuji, Paul R. Kingsbury, Luke V. Rasmussen, Jennifer A. Pacheco, Prakash Adekkanattu, Fei Wang, Yuan Luo, Jyotishman Pathak, Hongfang Liu, Guoqian Jiang,
Developing a FHIR-based EHR phenotyping framework: A case study for identification of patients with obesity and multiple comorbidities from discharge summaries,
Journal of Biomedical Informatics,
Volume 99,
2019,
103310,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2019.103310.
(https://www.sciencedirect.com/science/article/pii/S1532046419302291)
Abstract: Background
Standards-based clinical data normalization has become a key component of effective data integration and accurate phenotyping for secondary use of electronic healthcare records (EHR) data. HL7 Fast Healthcare Interoperability Resources (FHIR) is an emerging clinical data standard for exchanging electronic healthcare data and has been used in modeling and integrating both structured and unstructured EHR data for a variety of clinical research applications. The overall objective of this study is to develop and evaluate a FHIR-based EHR phenotyping framework for identification of patients with obesity and its multiple comorbidities from semi-structured discharge summaries leveraging a FHIR-based clinical data normalization pipeline (known as NLP2FHIR).
Methods
We implemented a multi-class and multi-label classification system based on the i2b2 Obesity Challenge task to evaluate the FHIR-based EHR phenotyping framework. Two core parts of the framework are: (a) the conversion of discharge summaries into corresponding FHIR resources – Composition, Condition, MedicationStatement, Procedure and FamilyMemberHistory using the NLP2FHIR pipeline, and (b) the implementation of four machine learning algorithms (logistic regression, support vector machine, decision tree, and random forest) to train classifiers to predict disease state of obesity and 15 comorbidities using features extracted from standard FHIR resources and terminology expansions. We used the macro- and micro-averaged precision (P), recall (R), and F1 score (F1) measures to evaluate the classifier performance. We validated the framework using a second obesity dataset extracted from the MIMIC-III database.
Results
Using the NLP2FHIR pipeline, 1237 clinical discharge summaries from the 2008 i2b2 obesity challenge dataset were represented as the instances of the FHIR Composition resource consisting of 5677 records with 16 unique section types. After the NLP processing and FHIR modeling, a set of 244,438 FHIR clinical resource instances were generated. As the results of the four machine learning classifiers, the random forest algorithm performed the best with F1-micro(0.9466)/F1-macro(0.7887) and F1-micro(0.9536)/F1-macro(0.6524) for intuitive classification (reflecting medical professionals’ judgments) and textual classification (reflecting the judgments based on explicitly reported information of diseases), respectively. The MIMIC-III obesity dataset was successfully integrated for prediction with minimal configuration of the NLP2FHIR pipeline and machine learning models.
Conclusions
The study demonstrated that the FHIR-based EHR phenotyping approach could effectively identify the state of obesity and multiple comorbidities using semi-structured discharge summaries. Our FHIR-based phenotyping approach is a first concrete step towards improving the data aspect of phenotyping portability across EHR systems and enhancing interpretability of the machine learning-based phenotyping algorithms.
Keywords: Clinical phenotyping; HL7 Fast Healthcare Interoperability Resources (FHIR); Electronic Health Records (EHRs); Natural language processing; Algorithm portability

Constantin-Bala Zamfirescu, Ciprian Candea, Ciprian Radu,
A stigmergic approach for social interaction design in collaboration engineering,
Neurocomputing,
Volume 146,
2014,
Pages 151-163,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2014.04.061.
(https://www.sciencedirect.com/science/article/pii/S092523121400798X)
Abstract: The increasing number of available collaborative tools and their extensive use in many organizational activities has constantly raised the complexity of collaboration engineering. It presumes the design of group decision processes, supported by a wide-range of groupware tools, in an ill-structured, dynamic, and open environment. As many of these processes are recurring by nature, the development of a shared repository to store the collective knowledge and experiences of group decision process designs became a core research topic of collaboration engineering in last few years. The paper presents a human–computer interaction engineering approach to design a software prototype that provides personalized, contextual and actionable recommendations for this problem. The approach emphasizes the computational aspects of collective intelligence, inspired from the stigmergic system designs, to structure these recommendations based on the collective knowledge that reflects not only the design space per se, but the collective experience in exploiting it as well. It is demonstrated by (1) detailing the engineering issues of an implemented prototype for the group decision process design; and (2) explaining its functionalities through a representative set of interaction scenarios. The paper covers the methodological, theoretical and practical aspects of engineering the above mentioned issues.
Keywords: Design space exploration; Stigmergic systems; Human–computer interaction; Collective knowledge

Natallia Kokash, Stuart L. Moodie, Mike K. Smith, Nick Holford,
Implementing a Domain-specific Language for Model-based Drug Development,
Procedia Computer Science,
Volume 63,
2015,
Pages 308-316,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2015.08.348.
(https://www.sciencedirect.com/science/article/pii/S1877050915024837)
Abstract: In this paper, we present the implementation of a novel domain-specific language (DSL) for pharmacometric modeling called the Modelling Description Language (MDL). MDL is a modular, declarative language with block structures that allows users to abstract data, processes and mathematical models from auxiliary code, and hence, improves model readability, reusability and opportunities for collaborative research. The main aim of this DSL is interoperability between core software tools used in pharmacometrics. We describe the MDL-IDE, an integrated development environment for MDL, which assists users in writing MDL code. The paper focuses on language constructs and design decisions, briefly explains how models are validated and converted to a machine-readable format for processing by existing model simulation and estimation software tools.
Keywords: Model-based drug development; pharmacokinetics; domain-specific language; modeling; simulation ;

Rob Knapen, Sander Janssen, Onno Roosenschoon, Peter Verweij, Wim de Winter, Michel Uiterwijk, Jan-Erik Wien,
Evaluating OpenMI as a model integration platform across disciplines,
Environmental Modelling & Software,
Volume 39,
2013,
Pages 274-282,
ISSN 1364-8152,
https://doi.org/10.1016/j.envsoft.2012.06.011.
(https://www.sciencedirect.com/science/article/pii/S1364815212001946)
Abstract: For decision makers in the domains of agriculture and environment, for instance in government agencies, farmers, environmental NGOs and farmers' unions, it is beneficial to evaluate ex-post or to asses ex-ante the impacts of their choices. To research these interdisciplinary relationships, models developed by different scientific disciplines and often operating at different scales can be integrated into model chains that cover processes across disciplines. In order to assemble models into an operational model chain conceptual, semantic and technical levels of integration have to be taken into account. The main focus of this paper is on technical integration to ensure repeatability and reproducibility of model chain runs and to optimize use of computer hardware for model simulations. Technical integration itself can be achieved by different approaches (i.e. manual, scripting, building or using a proprietary framework, using an open framework based on standards). From the many available modelling frameworks (e.g. OMS, TIME, KEPLER, FRAMES, MODCOM, OpenMI) the emphasis will be on OpenMI, the Open Modelling Interface and its use and usefulness as a readily available, generally accepted and open standards based framework. OpenMI is an open source software standard for dynamically linking models at runtime, which can potentially be used in many domains, but is currently mainly applied in the water and environmental domains. This paper describes and evaluates the use of OpenMI in several multi-disciplinary large projects that worked on integrated models. These projects operated in the disciplines of agriculture, land use, nitrogen cycling, forestry, hydrology and economics. To this end two workshops were organized to acquire feedback from both software developers and modellers that contributed to the aforementioned projects on the use of OpenMI. Perceived advantages and disadvantages of OpenMI differed between modellers and software engineers, although both identified the lack of standard functionality as a major disadvantage and the prescription of a way of working through OpenMI as a standard as a major advantage. In conclusion, OpenMI can be used as a standard for technical model integration across disciplines, and it is not limited to one particular discipline.
Keywords: Modelling frameworks; Integration; Semantic; Disciplines; Simulation; OpenMI

Yiming Zhang, Ling Liu,
Distance-aware bloom filters: Enabling collaborative search for efficient resource discovery,
Future Generation Computer Systems,
Volume 29, Issue 6,
2013,
Pages 1621-1630,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2012.08.007.
(https://www.sciencedirect.com/science/article/pii/S0167739X1200163X)
Abstract: Resource discovery in large-scale Peer-to-Peer (P2P) networks is challenging due to lack of effective methods for guiding queries. Based on the observation that the effectiveness of P2P resource discovery is determined by the utilization of hints, i.e., a summary of where the resources are, scattered in the network, in this paper we propose the distance-aware bloom filters (DABF) that disseminate hint information to faraway nodes by decaying BFs with different deterministic masks. Based on DABF, we design a novel Collaborative P2P Search (CPS) mechanism, which supports intelligent message behaviors including suspend, resume, terminate, move, reside, dispatch, notify and order. The effectiveness of our proposals is demonstrated through theoretical analysis and extensive simulations, in which we observed a remarkable reduction in search latency over previous approaches.
Keywords: Peer-to-Peer; Distance-aware bloom filters (DABF); Collaborative search; Information aggregation; Resource discovery

P.J. Hunter, B. de Bono, David P. Nickerson,
Organism-Wide Physiological Systems,
Editor(s): Olaf Wolkenhauer,
Systems Medicine,
Academic Press,
2021,
Pages 214-231,
ISBN 9780128160787,
https://doi.org/10.1016/B978-0-12-801238-3.11595-8.
(https://www.sciencedirect.com/science/article/pii/B9780128012383115958)
Abstract: Multiscale computational physiology attempts to link molecular pathways, incorporated into biophysically and anatomically based cell, tissue and organ models, to organism-wide physiological responses that are relevant in a medical context. Here we provide the background for this challenging task and discuss the development of model encoding standards and model repositories, and a mathematical modeling framework based on energy conservation principles. Functional Tissue Units (FTUs) are introduced as modular units at the tissue level to bridge the gap between molecular systems biology and anatomically based whole organ physiology.
Keywords: Bond graphs; CellML; Computational physiology; Functional tissue unit; Model reduction; Model repository; Modeling standards; Multiscale modeling; Non-dimensional numbers; OpenCOR; Physiome project; Port-Hamiltonians

Ricky J. Sethi, Yolanda Gil,
Scientific workflows in data analysis: Bridging expertise across multiple domains,
Future Generation Computer Systems,
Volume 75,
2017,
Pages 256-270,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2017.01.001.
(https://www.sciencedirect.com/science/article/pii/S0167739X17300195)
Abstract: In this paper, we demonstrate the use of scientific workflows in bridging expertise across multiple domains by re-purposing workflow fragments in the areas of text analysis, image analysis, and analysis of activity in video. We highlight how the reuse of workflows allows scientists to link across disciplines and avail themselves of the benefits of inter-disciplinary research beyond their normal area of expertise. In addition, we present in-depth studies of various tasks, including tasks for text analysis, multimedia analysis involving both images and text, video activity analysis, and analysis of artistic style using deep learning. These tasks show how the re-use of workflow fragments can turn a pre-existing, rudimentary approach into an expert-grade analysis. We also examine how workflow fragments save time and effort while amalgamating expertise in multiple areas such as machine learning and computer vision.
Keywords: Workflows; Data analysis; Workflow fragments

K. Kristensen, J. Krogstie, D. Ahlers, M. Mehrpoor,
Chapter 5 - LEAP Collaboration System,
Editor(s): Dimitris Kiritsis,
Taking the LEAP,
Academic Press,
2016,
Pages 99-123,
ISBN 9780128052631,
https://doi.org/10.1016/B978-0-12-805263-1.00005-0.
(https://www.sciencedirect.com/science/article/pii/B9780128052631000050)
Abstract: The LEAP collaboration system is a compilation of models, concepts, elements, and technology components that—when combined and structured in a meaningful way to teams of end users—enable companies to execute split-location engineering projects in a way that represents a competitive advantage. Written specifically for split-location and multidisciplinary teams, this chapter contains a description of these elements in the context of split-location engineering. Furthermore, this chapter describes how to develop and operationalize such a system in a way that represents a holistic, meaningful, and value-adding collaborative working environment that enables engineers and other knowledge workers to make decisions, solve problems, and address multidisciplinary issues effectively and efficiently. Based on lean thinking, the LEAP collaboration system approach identifies ways of reducing waste in collaboration processes. An evaluation of knowledge sources is described, together with approaches supporting knowledge creation in lean engineering environments. This chapter concludes with a toolbox that companies can use to diagnose collaboration problems and challenges, and systematically improve collaboration in their teams.
Keywords: collaboration system; collaborative engineering; knowledge sources; knowledge creation; collaborative diagnostics; waste in collaboration

Fuming Sun, Haojie Li, Xueming Wang,
Photo 4W: Mobile photo management on what, where, who and when,
Neurocomputing,
Volume 119,
2013,
Pages 59-64,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2012.03.038.
(https://www.sciencedirect.com/science/article/pii/S0925231212009125)
Abstract: As smart phone features dramatical performance, including faster processor, GPS chip, and more advanced camera, etc., more and more people use it for photography. With more photos stored on personal devices, there is a growing need of photo tools that can empower users to organize and manage personal photographs. To target this goal, we devote our efforts to develop a photos management system used for cellphone, namely Photo 4W, offering the index of mobile photography related to what, where, who and when. The key technologies in this system mainly include face annotation and scene classification. Face annotation is implemented by using interactive method after face detection. To help user annotate faces, we provided a batch solution in which users are allowed to multi-select a group of faces and then assign one name from a predicted candidate list. Scene classification utilizes semi-automated method to classify photos based on their semantic content. Experiments conducted on a real family album demonstrate that the proposed approach is effective and efficient for mobile photo management.
Keywords: Image annotation; Face annotation; Scene classification

Amanda Calatrava, Hernán Asorey, Jan Astalos, Alberto Azevedo, Francesco Benincasa, Ignacio Blanquer, Martin Bobak, Francisco Brasileiro, Laia Codó, Laura del Cano, Borja Esteban, Meritxell Ferret, Josef Handl, Tobias Kerzenmacher, Valentin Kozlov, Aleš Křenek, Ricardo Martins, Manuel Pavesio, Antonio Juan Rubio-Montero, Juan Sánchez-Ferrero,
A survey of the European Open Science Cloud services for expanding the capacity and capabilities of multidisciplinary scientific applications,
Computer Science Review,
Volume 49,
2023,
100571,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100571.
(https://www.sciencedirect.com/science/article/pii/S1574013723000382)
Abstract: Open Science is a paradigm in which scientific data, procedures, tools and results are shared transparently and reused by society. The European Open Science Cloud (EOSC) initiative is an effort in Europe to provide an open, trusted, virtual and federated computing environment to execute scientific applications and store, share and reuse research data across borders and scientific disciplines. Additionally, scientific services are becoming increasingly data-intensive, not only in terms of computationally intensive tasks but also in terms of storage resources. To meet those resource demands, computing paradigms such as High-Performance Computing (HPC) and Cloud Computing are applied to e-science applications. However, adapting applications and services to these paradigms is a challenging task, commonly requiring a deep knowledge of the underlying technologies, which often constitutes a general barrier to its uptake by scientists. In this context, EOSC-Synergy, a collaborative project involving more than 20 institutions from eight European countries pooling their knowledge and experience to enhance EOSC’s capabilities and capacities, aims to bring EOSC closer to the scientific communities. This article provides a summary analysis of the adaptations made in the ten thematic services of EOSC-Synergy to embrace this paradigm. These services are grouped into four categories: Earth Observation, Environment, Biomedicine, and Astrophysics. The analysis will lead to the identification of commonalities, best practices and common requirements, regardless of the thematic area of the service. Experience gained from the thematic services can be transferred to new services for the adoption of the EOSC ecosystem framework. The article made several recommendations for the integration of thematic services in the EOSC ecosystem regarding Authentication and Authorization (federated regional or thematic solutions based on EduGAIN mainly), FAIR data and metadata preservation solutions (both at cataloguing and data preservation—such as EUDAT’s B2SHARE), cloud platform-agnostic resource management services (such as Infrastructure Manager) and workload management solutions.
Keywords: Open science; Cloud computing; Federated infrastructure; Multidisciplinary; EOSC

Jan Höller, Vlasios Tsiatsis, Catherine Mulligan, Stamatis Karnouskos, Stefan Avesand, David Boyle,
Chapter 4 - M2M to IoT – An Architectural Overview,
Editor(s): Jan Höller, Vlasios Tsiatsis, Catherine Mulligan, Stamatis Karnouskos, Stefan Avesand, David Boyle,
From Machine-To-Machine to the Internet of Things,
Academic Press,
2014,
Pages 61-77,
ISBN 9780124076846,
https://doi.org/10.1016/B978-0-12-407684-6.00004-8.
(https://www.sciencedirect.com/science/article/pii/B9780124076846000048)
Abstract: The overall design objective of an IoT architecture shall be to target a horizontal system of real-world services that are open, service-oriented, secure, and offer trust. Design principles include the reuse of deployed IoT resources across application domains, and the design for a set of support services that provide open service-oriented capabilities and can be used for application development and execution, something already evident in the ETSI M2M standardization. A further design principle is to allow for actors taking on different roles of providing and using services across different business domains and value chains. An architecture must consider different functional components ranging from devices, networks and communications, data and knowledge management, applications, and exposure and integration into different business systems, all while considering security and management. The role of standardization is spanning across industrial domains, ICT and non-ICT, as well as addressing both technology and system perspectives.
Keywords: Architecture; Design Principles; ETSI; Functions; IoT-A; Requirements; Standards

Nanxing Li, Qian Li, Yu-Shen Liu, Wenlong Lu, Wanqi Wang,
BIMSeek++: Retrieving BIM components using similarity measurement of attributes,
Computers in Industry,
Volume 116,
2020,
103186,
ISSN 0166-3615,
https://doi.org/10.1016/j.compind.2020.103186.
(https://www.sciencedirect.com/science/article/pii/S0166361519307146)
Abstract: Building information modeling (BIM) has played a central role in architecture, engineering, and construction (AEC) industry, which also becomes an active research direction in smart buildings and smart cities. With the rapid development and popularization of BIM technology, online BIM resource libraries have grown rapidly. Fast and effective retrieval of BIM components from such great amount of resources has become an urgent demand. Traditional methods such as catalog browsing, keyword matching and shape matching are not capable of delivering satisfactory results, since they cannot extract the domain-specific information carried by BIM components. To resolve the aforementioned issue, we propose a novel similarity measurement and a new retrieval method, and integrate them into the BIMSeek system. The main contributions of our work are as follows. Firstly, we propose a novel algorithm for measuring the similarity between two BIM components based on their attribute information and Tversky similarity. Our proposed algorithm yields the best result in terms of Precision–Recall, F-measure and DCG compared to the traditional Tversky similarity measure and geometry similarity algorithm. Secondly, based on our proposed similarity measurement algorithm, we propose a novel retrieval method of BIM components called query-by-model. We integrate both our proposed similarity measurement algorithm and retrieval method into the BIM retrieval system, named BIMSeek, to greatly improve its retrieving speed and accuracy. Furthermore, we combine the query-by-model and query-by-keyword methods to refine the retrieval results iteratively. Finally, we conduct extensive experiments that compare our proposed method against previous retrieval methods. Results show that our method outperforms previous methods.
Keywords: Information retrieval; Attribute similarity; Domain-specific retrieval; Building information modeling (BIM); Industry foundation classes (IFC)

Hanane Grissette, El Habib Nfaoui,
Semisupervised neural biomedical sense disambiguation approach for aspect-based sentiment analysis on social networks,
Journal of Biomedical Informatics,
Volume 135,
2022,
104229,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2022.104229.
(https://www.sciencedirect.com/science/article/pii/S1532046422002349)
Abstract: Patient narratives on social networks contain large amounts of objective information, such as the descriptions of examinations and interventions. Sentiment analysis (SA) models are mostly used to evaluate the conveyed sentiments by patients in these narratives to assess positive or negative clinical outcomes or to judge the impact of a drug or a medical condition. To date, many state-of-the-art SA models often result in false assessment coverage due to the natural medical entities recognition deficiency and ambiguity problem. In this work, we propose a semisupervised-based neural sense disambiguation approach that helps to substantially define ambiguities, their levels, and the relational mappings between biomedical targets and dependencies for accurate aspect-based sentiment prediction. Three main modules are proposed: (1) generate a sentiment value based on extracted concepts and their synsets, (2) encode the representations of the contextual senses and sentiment inputs, and (3) estimate an aspect-based sentiment weight based on the context-dependency sentiment units vs. the biomedical sense. Both intrinsic and extrinsic evaluation proved how the proposed method have succeeded in pruning contextual sense feature generation and showed a strong agreement for biomedical data property parameterization and ambiguity type extraction. Thus, the model offers a significant rate of discrimination of biomedical natural concept senses by critically analysing constraints from conjunctions of positive or negative contextual semantics. A total of 21% of the vocabulary is drug names, 11% is a multiword drug reaction expressions, 7% is disease symptoms, and 5% is disease-related concepts such as symptoms and related therapy terms. Furthermore, the experiments on a multisource data from Twitter and health-related forums have overshadowed sentiment assessment and achieved an accuracy of 0.91 regarding concepts-based biomedical aspects. These results provide fresh insights into how to investigate biomedical knowledge, e.g., Medical Subject Headings (MeSH) and PubMed, to clarify the correspondence of various biomedical descriptive entities, definitions, and data properties from shared medication-related content.
Keywords: Biomedical sense disambiguation; Social networks; Aspect-based sentiment analysis; Neural networks

Manonmani. M, Sarojini Balakrishnan,
Feature Selection Using Improved Teaching Learning Based Algorithm on Chronic Kidney Disease Dataset,
Procedia Computer Science,
Volume 171,
2020,
Pages 1660-1669,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2020.04.178.
(https://www.sciencedirect.com/science/article/pii/S1877050920311595)
Abstract: Feature selection plays an important role in almost any data mining application especially in medical data mining to solve the problem of ‘curse of dimensionality’ and provide early diagnosis with relevant features and high accuracy. Innumerable feature selection methods have been presented in state-of-arts literature to tackle the problems of high dimensional data. Many evolutionary and swarm intelligence algorithms find solutions based on algorithm-specific control parameters. However, it is a challenging task to identify the optimal feature subset using a feature selection algorithm that is not dependent on the controlling parameters of an algorithm that is specific to a particular problem in hand. Hence, the present research work is based on the working principle of the original TLBO algorithm which does not require any algorithm-specific parameters. The proposed research work is known as Improved Teacher Learner Based Optimization (ITLBO) algorithm which aims to select the best feature subset based on Chebyshev distance formula in the evaluation of the fitness function and common control parameters viz., population size and number of generations to find the optimal feature subset for early diagnosis of chronic diseases. The proposed feature selection technique was applied to Chronic Kidney Disease (CKD) dataset and has achieved a significant feature reduction of 36% compared to the feature reduction of 25 % obtained by applying the original TLBO algorithm. The derived optimal feature subset obtained from TLBO algorithm and feature subset obtained from ITLBO algorithm is validated by evaluating the accuracy of Support Vector Machine (SVM), Convolution Neural Networks (CNN) and Gradient Boosting classification algorithms. Experimental results reveal that there is an overall improvement of classification accuracy for the three algorithms for the derived feature subset from the proposed feature selection algorithm compared to the original TLBO algorithm.
Keywords: Teacher Learner Based Optimization; Improved Teacher Learner Based Optimization; Chronic Kidney Disease; Support Vector Machine; Convolution Neural Networks; Gradient Boosting; Optimization techniques

Jian Yu, Quan Z. Sheng, Joshua K.Y. Swee, Jun Han, Chengfei Liu, Talal H. Noor,
Model-driven development of adaptive web service processes with aspects and rules,
Journal of Computer and System Sciences,
Volume 81, Issue 3,
2015,
Pages 533-552,
ISSN 0022-0000,
https://doi.org/10.1016/j.jcss.2014.11.008.
(https://www.sciencedirect.com/science/article/pii/S0022000014001494)
Abstract: Modern software systems are frequently required to be adaptive in order to cope with constant changes. Unfortunately, service-oriented systems built with WS-BPEL are still too rigid. In this paper, we propose a novel model-driven approach to supporting the development of dynamically adaptive WS-BPEL based systems. We model the system functionality with two distinct but highly correlated parts: a stable part called the base model describing the flow logic aspect and a volatile part called the variable model describing the decision logic aspect. We develop an aspect-oriented method to weave the base model and the variable model together so that runtime changes can be applied to the variable model without affecting the base model. A model-driven platform has been implemented to support the development of adaptive WS-BPEL processes. In-lab experiments show that our approach has low performance overhead. A real-life case study also validates the applicability of our approach.
Keywords: Web services; Adaptive systems; Model-driven development; Aspect-oriented methodology; Design tools and techniques

Jinsongdi Yu, Peter Baumann, Dimitar Misev, Piero Campalani, Mirko Albani, Fulvio Marelli, David Giaretta, Shirley Crompton,
Point of view: Long-Term access to Earth Archives across Multiple Disciplines,
Computer Standards & Interfaces,
Volume 36, Issue 6,
2014,
Pages 909-917,
ISSN 0920-5489,
https://doi.org/10.1016/j.csi.2014.04.001.
(https://www.sciencedirect.com/science/article/pii/S0920548914000439)
Abstract: Without an approach accepted by the communities at large, domain disagreements will continue to thwart current global efforts to harmonize information models. The research presented here reviewed current standardization activities. A number of observations and possible solutions are proposed to address the topic of standardizing long term access to multi-discipline Earth System archives by considering the application of the knowledge base concept to facilitate data interpretation. Finally, we present a case study as an initial entry point for the further discussion about standardization.
Keywords: Interoperability; virtualization; coverage; multi-disciplinary; data preservation

Aysh Al-Hroob, Ayad Tareq Imam, Rawan Al-Heisa,
The use of artificial neural networks for extracting actions and actors from requirements document,
Information and Software Technology,
Volume 101,
2018,
Pages 1-15,
ISSN 0950-5849,
https://doi.org/10.1016/j.infsof.2018.04.010.
(https://www.sciencedirect.com/science/article/pii/S0950584918300752)
Abstract: Context
The automatic extraction of actors and actions (i.e., use cases) of a system from natural language-based requirement descriptions, is considered a common problem in requirements analysis. Numerous techniques have been used to resolve this problem. Examples include rule-based (e.g., inference), keywords, query (e.g., bi-grams), library maintenance, semantic business vocabularies, and rules. The question remains: can combination of natural language processing (NLP) and artificial neural networks (ANNs) perform this job successfully and effectively?
Objective
This paper proposes a new approach to automatically identify actors and actions in a natural language-based requirements’ description of a system. Included are descriptions of how NLP plays an important role in extracting actors and actions, and how ANNs can be used to provide definitive identification.
Method
We used an NLP parser with a general architecture for text engineering, producing lexicons, syntaxes, and semantic analyses. An ANN was developed using five different use cases, producing different results due to their complexity and linguistic formation.
Results
Binomial classification accuracy techniques were used to evaluate the effectiveness of this approach. Based on the five use cases, the results were 17–63% for precision, 5–6100% for recall, and 29–71% for F-measure.
Conclusion
We successfully used a combination of NLP and ANN artificial intelligence techniques to reveal specific domain semantics found in a software requirements specification. An Intelligent Technique for Requirements Engineering (IT4RE) was developed to provide a semi-automated approach, classified as Intelligent Computer Aided Software Engineering (I-CASE).
Keywords: NLP; ANN; I-CASE; Software requirements; GATE; MATLAB

Jeffery S. Horsburgh, David G. Tarboton, Richard P. Hooper, Ilya Zaslavsky,
Managing a community shared vocabulary for hydrologic observations,
Environmental Modelling & Software,
Volume 52,
2014,
Pages 62-73,
ISSN 1364-8152,
https://doi.org/10.1016/j.envsoft.2013.10.012.
(https://www.sciencedirect.com/science/article/pii/S1364815213002557)
Abstract: The ability to discover and integrate data from multiple sources, projects, and research efforts is critical as scientists continue to investigate complex hydrologic processes at expanding spatial and temporal scales. Until recently, syntactic and semantic heterogeneity in data from different sources made data discovery and integration difficult. The Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) Hydrologic Information System (HIS) was developed to improve access to hydrologic data. A major semantic challenge related to data sharing and publication arose in development of the HIS. No accepted vocabulary existed within the hydrology research community for describing hydrologic observations, making it difficult to discover and synthesize data from multiple research groups even if access to the data was not a barrier. Additionally, the hydrology research community relies heavily on data collected or assembled by government agencies such as USGS and USEPA, each of which has its own semantics for describing observations. This semantic heterogeneity across data sources was a challenge in developing tools that support data discovery and access across multiple hydrologic data sources by time, geographic region, measured variable, data collection method, etc. This paper describes a community shared vocabulary and its supporting management tools that can be used by data publishers to populate metadata describing hydrologic observations to ensure that data from multiple sources published within the CUAHSI HIS are semantically consistent. We also describe how the CUAHSI HIS mediates across terms in the community shared vocabulary and terms used by government agencies to support discovery and integration of datasets published by both academic researchers and government agencies.
Keywords: Controlled vocabulary; Data publication; CUAHSI HIS; Hydrologic observations

Saptarsi Goswami, Sanjay Chakraborty, Sanhita Ghosh, Amlan Chakrabarti, Basabi Chakraborty,
A review on application of data mining techniques to combat natural disasters,
Ain Shams Engineering Journal,
Volume 9, Issue 3,
2018,
Pages 365-378,
ISSN 2090-4479,
https://doi.org/10.1016/j.asej.2016.01.012.
(https://www.sciencedirect.com/science/article/pii/S2090447916000307)
Abstract: Thousands of human lives are lost every year around the globe, apart from significant damage on property, animal life, etc., due to natural disasters (e.g., earthquake, flood, tsunami, hurricane and other storms, landslides, cloudburst, heat wave, forest fire). In this paper, we focus on reviewing the application of data mining and analytical techniques designed so far for (i) prediction, (ii) detection, and (iii) development of appropriate disaster management strategy based on the collected data from disasters. A detailed description of availability of data from geological observatories (seismological, hydrological), satellites, remote sensing and newer sources like social networking sites as twitter is presented. An extensive and in-depth literature study on current techniques for disaster prediction, detection and management has been done and the results are summarized according to various types of disasters. Finally a framework for building a disaster management database for India hosted on open source Big Data platform like Hadoop in a phased manner has been proposed. The study has special focus on India which ranks among top five counties in terms of absolute number of the loss of human life.
Keywords: Natural disaster; Data mining; Twitter; India; Big Data

Katia Lupinetti, Jean-Philippe Pernot, Marina Monti, Franca Giannini,
Content-based CAD assembly model retrieval: Survey and future challenges,
Computer-Aided Design,
Volume 113,
2019,
Pages 62-81,
ISSN 0010-4485,
https://doi.org/10.1016/j.cad.2019.03.005.
(https://www.sciencedirect.com/science/article/pii/S0010448518305451)
Abstract: Currently, the content-based retrieval is a problem of major interest in several different fields and, focusing on mechanical engineering, many approaches exist to compare and retrieve single CAD parts, to evaluate shape similarity, to extract features and to segment models. However, most of the proposed approaches do not take into account all the key characteristics of an assembly model, such as the relationships between its components, and the different levels according to which two assembly models can be considered similar, i.e. either globally, partially, or locally. For these reasons, the retrieval of CAD assembly models still faces challenges to fully satisfy designers’ expectations. The aim of this paper is to review the state-of-the-art of works addressing the CAD assembly model retrieval and to identify future challenges and possible research directions. Firstly, the paper highlights the user requirements for CAD assembly model retrieval and proposes a set of criteria for analyzing the available methods grouped into the following macro-categories: objective, assembly characterization, assembly descriptor, query specification and type of similarity. Secondly, it describes and characterizes the available methods by organizing them according to the adopted criteria. Finally, it discusses the open issues and future challenges.
Keywords: Assembly retrieval; Assembly similarity evaluation; Assembly matching; Knowledge representation; Knowledge extraction


Contents,
Procedia Computer Science,
Volume 170,
2020,
Pages iii-xii,
ISSN 1877-0509,
https://doi.org/10.1016/S1877-0509(20)30954-6.
(https://www.sciencedirect.com/science/article/pii/S1877050920309546)

Matúš Sulír, Milan Nosáľ, Jaroslav Porubän,
Recording concerns in source code using annotations,
Computer Languages, Systems & Structures,
Volume 46,
2016,
Pages 44-65,
ISSN 1477-8424,
https://doi.org/10.1016/j.cl.2016.07.003.
(https://www.sciencedirect.com/science/article/pii/S147784241630015X)
Abstract: A concern can be characterized as a developer׳s intent behind a piece of code, often not explicitly captured in it. We discuss a technique of recording concerns using source code annotations (concern annotations). Using two studies and two controlled experiments, we seek to answer the following 3 research questions: (1) Do programmers׳ mental models overlap? (2) How do developers use shared concern annotations when they are available? (3) Does using annotations created by others improve program comprehension and maintenance correctness, time and confidence? The first study shows that developers׳ mental models, recorded using concern annotations, overlap and thus can be shared. The second study shows that shared concern annotations can be used during program comprehension for the following purposes: hypotheses confirmation, feature location, obtaining new knowledge, finding relationships and maintenance notes. The first controlled experiment with students showed that the presence of annotations significantly reduced program comprehension and maintenance time by 34%. The second controlled experiment was a differentiated replication of the first one, focused on industrial developers. It showed a 33% significant improvement in correctness. We conclude that concern annotations are a viable way to share developers׳ thoughts.
Keywords: Program comprehension; Concerns; Source code annotations; Empirical studies

Shan Cui, Tao Zhu, Xiao Zhang, Huansheng Ning,
MCLA: Research on cumulative learning of Markov Logic Network,
Knowledge-Based Systems,
Volume 242,
2022,
108352,
ISSN 0950-7051,
https://doi.org/10.1016/j.knosys.2022.108352.
(https://www.sciencedirect.com/science/article/pii/S0950705122001319)
Abstract: Compared with other probabilistic semantic learning algorithms, Markov Logic Network (MLN) learning can integrate existing knowledge fragments. However, the knowledge fragments increase as the MLN learning algorithm traverses candidate objects, making the execution time of the MLN learning algorithm long. To shorten the execution time, an MLN Cumulative Learning Algorithm (MCLA) that combines the MLN learning algorithm and cumulative learning is proposed. MCLA can unify the existing knowledge and the new–old knowledge, perform multi-task learning under appropriate circumstances, and directly call the existing knowledge to learn the new knowledge. This paper applies MCLA to single-resident indoor activity scenarios to verify the effectiveness of the learning algorithm. Experiments have proved that MCLA not only ensures the accuracy and improves the versatility of knowledge, but also dramatically decreases the learning time and effectively manages the knowledge memory of the MLN.
Keywords: Markov Logic Network; Cumulative learning; Unification of new–old knowledge; Activity recognition; Versatility

Ondřej Dvořák, Robert Pergl,
Tackling rapid technology changes by applying enterprise engineering theories,
Science of Computer Programming,
Volume 215,
2022,
102747,
ISSN 0167-6423,
https://doi.org/10.1016/j.scico.2021.102747.
(https://www.sciencedirect.com/science/article/pii/S0167642321001404)
Abstract: Moore's law states that the number of transistors on a chip will double every two years. A similar force appears to drive the progress of information technology (IT). IT companies tend to struggle to keep up with the latest technological developments, and software solutions are becoming increasingly outdated. The ability for software to change easily is defined as evolvability. One of the major fields researching evolvability is enterprise engineering (EE). The EE research paradigm applies theories from other fields to the evolvability of organisations. We argue that such theories can be applied to software engineering (SE) as well, which can contribute to the construction of software with a clear separation of dynamically changing technologies based on a relatively stable description of functions required for a specific user. EE theories introduce notions of function, construction, and affordance. We reify these concepts in terms of SE. Based on this reification, we propose affordance-driven assembling (ADA) as a software design approach that can aid in the construction of more evolvable software solutions. We exemplify the implementation of ADA in a case study on a commercial system and measure its effectiveness in terms of the impact of changes, as defined by the normalised systems theory.
Keywords: ADA; Component-based systems; Enterprise engineering; Technology acceleration; Evolvability

Asad Khattak, Muhammad Zubair Asghar, Anam Saeed, Ibrahim A. Hameed, Syed Asif Hassan, Shakeel Ahmad,
A survey on sentiment analysis in Urdu: A resource-poor language,
Egyptian Informatics Journal,
Volume 22, Issue 1,
2021,
Pages 53-74,
ISSN 1110-8665,
https://doi.org/10.1016/j.eij.2020.04.003.
(https://www.sciencedirect.com/science/article/pii/S1110866520301171)
Abstract: Background/introduction
The dawn of the internet opened the doors to the easy and widespread sharing of information on subject matters such as products, services, events and political opinions. While the volume of studies conducted on sentiment analysis is rapidly expanding, these studies mostly address English language concerns. The primary goal of this study is to present state-of-art survey for identifying the progress and shortcomings saddling Urdu sentiment analysis and propose rectifications.
Methods
We described the advancements made thus far in this area by categorising the studies along three dimensions, namely: text pre-processing lexical resources and sentiment classification. These pre-processing operations include word segmentation, text cleaning, spell checking and part-of-speech tagging. An evaluation of sophisticated lexical resources including corpuses and lexicons was carried out, and investigations were conducted on sentiment analysis constructs such as opinion words, modifiers, negations.
Results and conclusions
Performance is reported for each of the reviewed study. Based on experimental results and proposals forwarded through this paper provides the groundwork for further studies on Urdu sentiment analysis.
Keywords: Urdu sentiment analysis; Pre-processing; Sentiment lexicon; Datasets; Corpus; Urdu sentiment classification; Semantic orientation

Kostas Kolomvatsos, Stathes Hadjiefthymiades,
An extended Q-gram algorithm for calculating the relevance factor of products in electronic marketplaces,
Electronic Commerce Research and Applications,
Volume 12, Issue 6,
2013,
Pages 397-411,
ISSN 1567-4223,
https://doi.org/10.1016/j.elerap.2012.12.005.
(https://www.sciencedirect.com/science/article/pii/S1567422313000033)
Abstract: Intelligent agents offer a number of advantages when used in electronic markets. In such environments, intelligent agents can represent users acting as buyers or sellers. On the buyer’s side, an intelligent agent can undertake the responsibility of finding and purchasing products that meet the owner’s needs. In this process, the agent should decide if a product, offered by a seller, is relevant to the owner’s preferences. We propose an algorithm for calculating the relevance factor of a product based on the product description, constraints defined by the buyer and the product’s quality of service characteristics, such as the delivery time or the seller trust level. The proposed algorithm is based on widely known similarity assessment techniques. However, we also propose a new similarity assessment scheme based on the Q-grams technique. We describe the proposed solution and evaluate our methodology. The results show that the algorithm is an efficient way for the relevance factor calculation and quality of service characteristics play an important role in the calculation process. Quality of service factor calculation provides an additional level of intelligence in the proposed methodology.
Keywords: Electronic markets; Intelligent agents; Product matching; Similarity

Daniel Sonntag, Hans-Jürgen Profitlich,
An architecture of open-source tools to combine textual information extraction, faceted search and information visualisation,
Artificial Intelligence in Medicine,
Volume 93,
2019,
Pages 13-28,
ISSN 0933-3657,
https://doi.org/10.1016/j.artmed.2018.08.003.
(https://www.sciencedirect.com/science/article/pii/S0933365717306656)
Abstract: This article presents our steps to integrate complex and partly unstructured medical data into a clinical research database with subsequent decision support. Our main application is an integrated faceted search tool, accompanied by the visualisation of results of automatic information extraction from textual documents. We describe the details of our technical architecture (open-source tools), to be replicated at other universities, research institutes, or hospitals. Our exemplary use cases are nephrology and mammography. The software was first developed in the nephrology domain and then adapted to the mammography use case. We report on these case studies, illustrating how the application can be used by a clinician and which questions can be answered. We show that our architecture and the employed software modules are suitable for both areas of application with a limited amount of adaptations. For example, in nephrology we try to answer questions about the temporal characteristics of event sequences to gain significant insight from the data for cohort selection. We present a versatile time-line tool that enables the user to explore relations between a multitude of diagnosis and laboratory values.
Keywords: Clinical decision support; Information extraction; Natural language processing; Medical data analysis; Data management; Faceted search; Human-computer interaction; Visualisation; Electronic health record (EHR)

Emanuel Demetrescu,
Archaeological stratigraphy as a formal language for virtual reconstruction. Theory and practice,
Journal of Archaeological Science,
Volume 57,
2015,
Pages 42-55,
ISSN 0305-4403,
https://doi.org/10.1016/j.jas.2015.02.004.
(https://www.sciencedirect.com/science/article/pii/S0305440315000382)
Abstract: In recent years there has been a growing interest in 3D acquisition techniques in the field of cultural heritage, yet, at the same time, only a small percentage of case studies have been conducted on the virtual reconstruction of archaeological sites that are no longer in existence. Such reconstructions are, at times, considered “artistic” or “aesthetic” endeavors, as the complete list of sources used is not necessarily provided as a reference along with the 3D representation. One of the reasons for this is likely the lack of a shared language in which to store and communicate the steps in the reconstruction process. This paper proposes the use of a formal language with which to keep track of the entire virtual reconstruction process. The proposal is based on the stratigraphic reading approach and aims to create a common framework connecting archaeological documentation and virtual reconstruction in the earliest stages of the survey. To this end, some of the tools and standards used in archaeological research have been “extended” to taxonomically annotate both the validation of the hypothesis and the sources involved.
Keywords: 3D virtual reconstruction; Archaeological stratigraphy; 3D reconstruction methodology; 3D modeling in archeology; Virtual Stratigraphic Unit; Extended Matrix

Imon Banerjee, Camille Kurtz, Alon Edward Devorah, Bao Do, Daniel L. Rubin, Christopher F. Beaulieu,
Relevance feedback for enhancing content based image retrieval and automatic prediction of semantic image features: Application to bone tumor radiographs,
Journal of Biomedical Informatics,
Volume 84,
2018,
Pages 123-135,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2018.07.002.
(https://www.sciencedirect.com/science/article/pii/S153204641830128X)
Abstract: Background
The majority of current medical CBIR systems perform retrieval based only on “imaging signatures” generated by extracting pixel-level quantitative features, and only rarely has a feedback mechanism been incorporated to improve retrieval performance. In addition, current medical CBIR approaches do not routinely incorporate semantic terms that model the user’s high-level expectations, and this can limit CBIR performance.
Method
We propose a retrieval framework that exploits a hybrid feature space (HFS) that is built by integrating low-level image features and high-level semantic terms, through rounds of relevance feedback (RF) and performs similarity-based retrieval to support semi-automatic image interpretation. The novelty of the proposed system is that it can impute the semantic features of the query image by reformulating the query vector representation in the HFS via user feedback. We implemented our framework as a prototype that performs the retrieval over a database of 811 radiographic images that contains 69 unique types of bone tumors.
Results
We evaluated the system performance by conducting independent reading sessions with two subspecialist musculoskeletal radiologists. For the test set, the proposed retrieval system at fourth RF iteration of the sessions conducted with both the radiologists achieved mean average precision (MAP) value ∼0.90 where the initial MAP with baseline CBIR was 0.20. In addition, we also achieved high prediction accuracy (>0.8) for the majority of the semantic features automatically predicted by the system.
Conclusion
Our proposed framework addresses some limitations of existing CBIR systems by incorporating user feedback and simultaneously predicting the semantic features of the query image. This obviates the need for the user to provide those terms and makes CBIR search more efficient for inexperience users/trainees. Encouraging results achieved in the current study highlight possible new directions in radiological image interpretation employing semantic CBIR combined with relevance feedback of visual similarity.
Keywords: Content based image retrieval; Pixel-level features; Radiomics; Semantic features; Relevance feedback; Bone tumors; Radiography

Zhoupeng Han, Rong Mo, Li Hao,
Clustering and retrieval of mechanical CAD assembly models based on multi-source attributes information,
Robotics and Computer-Integrated Manufacturing,
Volume 58,
2019,
Pages 220-229,
ISSN 0736-5845,
https://doi.org/10.1016/j.rcim.2019.01.003.
(https://www.sciencedirect.com/science/article/pii/S0736584518301947)
Abstract: Content-based CAD assembly model retrieval focuses more on the similarity measure of geometry and topology information, which can hardly meet designers’ design requirements in the product conceptual design process. To search quickly and effectively the CAD assembly models related with product design requirements information for assembly model reuse, inspire designers’ design thought and product design innovation in the product conceptual design phase, a novel approach for CAD assembly model clustering and retrieval based on multi-source attributes information is proposed. First, the CAD assembly model is represented by attribute adjacency graph (AAG) with multiple attributes information, and the similarity between assembly models is evaluated comprehensively by considering part information and assembly relationship information. Then, a weighted graph is constructed for assembly clustering in model repository. Meanwhile, an improved spectral clustering algorithm is given to divide CAD assembly models into different assembly clusters. Subsequently, indexing model is established for each assembly cluster. After that, the process of assembly retrieval based on indexing model mechanism is given in detail. Finally, experiments on the prototype system verify effectiveness and feasibility of the proposed approach for assembly clustering and retrieval.
Keywords: Multi-source attributes information; Spectral clustering; Indexing model; Assembly clustering; Assembly retrieval

María Pérez, Rafael Berlanga,
Semantic transference for enriching multilingual biomedical knowledge resources,
Journal of Biomedical Informatics,
Volume 58,
2015,
Pages 1-10,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2015.08.026.
(https://www.sciencedirect.com/science/article/pii/S1532046415002051)
Abstract: Biomedical knowledge resources (KRs) are mainly expressed in English, and many applications using them suffer from the scarcity of knowledge in non-English languages. The goal of the present work is to take maximum profit from existing multilingual biomedical KRs lexicons to enrich their non-English counterparts. We propose to combine different automatic methods to generate pair-wise language alignments. More specifically, we use two well-known translation methods (GIZA++ and Moses), and we propose a new ad hoc method specially devised for multilingual KRs. Then, resulting alignments are used to transfer semantics between KRs across their languages. Transference quality is ensured by checking the semantic coherence of the generated alignments. Experiments have been carried out over the Spanish, French and German UMLS Metathesaurus counterparts. As a result, the enriched Spanish KR can grow up to 1,514,217 concepts (originally 286,659), the French KR up to 1,104,968 concepts (originally 83,119), and the German KR up to 1,136,020 concepts (originally 86,842).
Keywords: Semantic transference; Multilingual biomedical knowledge resources; Term alignment

Martí Cuquet, Anna Fensel,
The societal impact of big data: A research roadmap for Europe,
Technology in Society,
Volume 54,
2018,
Pages 74-86,
ISSN 0160-791X,
https://doi.org/10.1016/j.techsoc.2018.03.005.
(https://www.sciencedirect.com/science/article/pii/S0160791X17300131)
Abstract: With its rapid growth and increasing adoption, big data is producing a substantial impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap to capture the economic, social and ethical, legal and political benefits associated with the use of big data in Europe. The present roadmap considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics in order to deliver social impact, skills development and standardisation. Finally, it also identifies what sectors will be most benefited by each of the research efforts. The goal of the roadmap is to guide European research efforts to develop a socially responsible big data economy, and to allow stakeholders to identify and meet big data challenges and proceed with a shared understanding of the societal impact, positive and negative externalities and concrete problems worth investigating in future programmes.
Keywords: Big data; Research roadmap; Societal externalities; Skills development; Standardisation

Stefania Rubrichi, Andrea Battistotti, Silvana Quaglini,
Patients’ involvement in e-health services quality assessment: A system for the automatic interpretation of SMS-based patients’ feedback,
Journal of Biomedical Informatics,
Volume 51,
2014,
Pages 41-48,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2014.03.003.
(https://www.sciencedirect.com/science/article/pii/S1532046414000604)
Abstract: Purpose
Effective communication between patients and health services providers is a key aspect for optimizing and maintaining these services. This work describes a system for the automatic evaluation of users’ perception of the quality of SmsCup, a reminder system for outpatient visits based on short message service (SMS). The final purpose is the creation of a closed-loop control system for the outpatient service, where patients’ complaints and comments represent a feedback that can be used for a better implementation of the service itself.
Methods
SmsCup was adopted since about eight years by an Italian healthcare organization, with very good results in reducing the no-show (missing visits) phenomenon. During these years, a number of citizens, even if not required, sent a message back, with comments about the service. The automatic interpretation of the content of those SMS may be useful for monitoring and improving service performances.Yet, due to the complex nature of SMS language, their interpretation represents an ongoing challenge. The proposed system uses conditional random fields as the information extraction method for classifying messages into several semantic categories. The categories refer to appreciation of the service or complaints of various types. Then, the system analyzes the extracted content and provides feedback to the service providers, making them learning and acting on this basis.
Results
At each step, the content of the messages reveals the actual state of the service as well as the efficacy of corrective actions previously undertaken. Our evaluations showed that: (i) the SMS classification system has achieved good overall performance with an average F1-measure and an overall accuracy of about 92%; (ii) the notification of the patients’ feedbacks to service providers showed a positive impact on service functioning.
Conclusions
Our study proposed an interactive patient-centered system for continuous monitoring of the service quality. It has demonstrated the feasibility of a tool for the analysis and notification of the patients’ feedback on their service experiences, which would support a more regular access to the service.
Keywords: e-Health; Patients’ feedback; Health service assessment; SMS; Information extraction; Conditional random fields

Chengcheng Wan, Yanmin Zhu, Jiadi Yu, Yanyan Shen,
SMOPAT: Mining semantic mobility patterns from trajectories of private vehicles,
Information Sciences,
Volume 429,
2018,
Pages 12-25,
ISSN 0020-0255,
https://doi.org/10.1016/j.ins.2017.10.043.
(https://www.sciencedirect.com/science/article/pii/S0020025517302323)
Abstract: With the increasing use of private vehicles with positioning services, GPS trajectory data of vehicles has become one of the major sources of big data about urban life. Existing studies on mobility pattern mining from trajectories share a common limitation, i.e., they fail to capture the semantics of trajectories. Automatic derivation of semantic information for every trajectory is a challenging task. In this paper, we propose an approach, called SMOPAT (Semantic MObility PATterns), for mining spatial-temporal semantic mobility patterns from trajectories of private vehicles. We design a probabilistic generative model with latent variables to characterize the semantic mobility of vehicles. Based on the model, SMOPAT labels each location in a trajectory with a visit purpose by using a polynomial-time dynamic programming algorithm. It then employs an efficient algorithm to find the most frequent semantic mobility patterns. We evaluate our approach on a large data set of real trajectories of private vehicles spanning a time duration of over ten months with 114 million records in Shanghai, China. The experimental results show that our approach produces meaningful patterns and outperforms the two competing methods in terms of diversity, coherence, and coverage.
Keywords: Trajectory; Semantic mobility pattern; Private vehicles

Corinna Gries, Mark R. Gahler, Paul C. Hanson, Timothy K. Kratz, Emily H. Stanley,
Information management at the North Temperate Lakes Long-term Ecological Research site — Successful support of research in a large, diverse, and long running project,
Ecological Informatics,
Volume 36,
2016,
Pages 201-208,
ISSN 1574-9541,
https://doi.org/10.1016/j.ecoinf.2016.08.007.
(https://www.sciencedirect.com/science/article/pii/S1574954116301273)
Abstract: Information management has been an integral part of the research process at the North Temperate Lakes Long-term Ecological Research (NTL LTER) program for over 30years. A combination of factors has made the information management system (IMS) at NTL very successful. Significant resources have been invested in the IMS from the beginning, the Information Manager has been part of the leadership team at NTL and later in various roles at the LTER network level; the NTL IMS was a very early adopter of database systems, standardized metadata, and a data delivery system based on those metadata. This approach has made data easily accessible to NTL researchers and the broader scientific community. Data management workflows have become increasingly more automated with adoption of modern technologies as they became available, making the system efficient enough to handle core data as well as all one-time research data generated within NTL and several related projects. More than three decades of core data from eleven lakes are reused extensively as critical background information and as the limnological go-to site for many synthesis projects within and beyond LTER. The NTL IMS continues to implement new technologies for improving data management efficiency, discovery, access, integration, and synthesis. Accordingly, the functionality of the original online data access system programmed in Java and JavaServer Pages (JSP) was ported to the modern content management system, Drupal and integrated into LTER's Drupal Ecological Information Management System (DEIMS). NTL has invested in sensor technology for studying lake conditions over the long term, which necessitated a sophisticated management system tailored to high frequency data streams. Several technologies have been used at different times for automation of management, quality control and archiving of these high volume data. Near real time lake conditions can be accessed on the NTL website and smart phone Apps. Easy access to long-term and sensor data in the NTL IMS has led NTL researchers to develop new analytical methods and the publication of several R statistical packages. Recent graduate students are now employed as data scientists helping define a new career path inspired by the availability of data. The NTL project has amassed one of the world's most comprehensive long-term datasets on lakes and their surrounding landscapes. The NTL IMS facilitates the use of these data by multiple groups for research, education, and communication of science to the public.
Keywords: Ecological information management; Long-term ecological research; Database; Sensor data; Data life cycle

W.J. Black, A. Rowley, M. Miwa, J. McNaught, S. Ananiadou,
Chapter 5 - Text Mining for Semantic Search in Europe PubMed Central Labs,
Editor(s): Emma L. Tonkin, Gregory J.L. Tourte,
In Chandos Information Professional Series,
Working with Text,
Chandos Publishing,
2016,
Pages 111-131,
ISBN 9781843347491,
https://doi.org/10.1016/B978-1-84334-749-1.00005-6.
(https://www.sciencedirect.com/science/article/pii/B9781843347491000056)
Abstract: This chapter describes an implemented and publicly available search aid designed for use in the context of a standard full-text retrieval service, which automatically suggests questions on the basis of what has been entered in the search engine’s query field. The service is developed on the basis of a content analysis achieved by merging information extraction of biomedical named entities with a syntactic analysis of the full text of an entire collection of scientific papers. We discuss the design and implementation of the system in contrast with alternative ways of providing a search application based on text mining for events of significance in biomedical sciences, and evaluate characteristics of the system against criteria established in collaboration with sample users.
Keywords: Biomedical named entities; Collection-scale syntactic analysis; Question generation; Query expansion; Semantic search

Bin Xiao, Rahim Rahmani, Yuhong Li, Theo Kanter,
Edge-based interoperable service-driven information distribution for intelligent pervasive services,
Pervasive and Mobile Computing,
Volume 40,
2017,
Pages 359-381,
ISSN 1574-1192,
https://doi.org/10.1016/j.pmcj.2017.07.002.
(https://www.sciencedirect.com/science/article/pii/S1574119217303541)
Abstract: Internet of Things (IoT)-based Intelligent Pervasive Service (IPS) systems face increasing pressure from the massive amounts of heterogeneous data generated; the heterogeneity hinders interoperability between data resources and IPSs, making data sharing inefficient and making it difficult to satisfy the needs and fulfill requirements of the IPSs. In response, this article proposes a method of interoperable service-driven information distribution on the edge side to enhance the service-level interoperability with feature-level interoperability by self-adapting data sharing according to the service needs, which will also help to release incremental data pressure and provide better data privacy by conducting service-driven and relevance-based data sharing.
Keywords: Pervasive sensing; Interoperability; Service-driven; IoT Edge-centric; Self-adaptable information distribution

M. Karthikeyan, P. Aruna,
Probability based document clustering and image clustering using content-based image retrieval,
Applied Soft Computing,
Volume 13, Issue 2,
2013,
Pages 959-966,
ISSN 1568-4946,
https://doi.org/10.1016/j.asoc.2012.09.013.
(https://www.sciencedirect.com/science/article/pii/S1568494612004334)
Abstract: Clustering of related or similar objects has long been regarded as a potentially useful contribution of helping users to navigate an information space such as a document collection. Many clustering algorithms and techniques have been developed and implemented but as the sizes of document collections have grown these techniques have not been scaled to large collections because of their computational overhead. To solve this problem, the proposed system concentrates on an interactive text clustering methodology, probability based topic oriented and semi-supervised document clustering. Recently, as web and various documents contain both text and large number of images, the proposed system concentrates on content-based image retrieval (CBIR) for image clustering to give additional effect to the document clustering approach. It suggests two kinds of indexing keys, major colour sets (MCS) and distribution block signature (DBS) to prune away the irrelevant images to given query image. Major colour sets are related with colour information while distribution block signatures are related with spatial information. After successively applying these filters to a large database, only small amount of high potential candidates that are somewhat similar to that of query image are identified. Then, the system uses quad modelling method (QM) to set the initial weight of two-dimensional cells in query image according to each major colour and retrieve more similar images through similarity association function associated with the weights. The proposed system evaluates the system efficiency by implementing and testing the clustering results with Dbscan and K-means clustering algorithms. Experiment shows that the proposed document clustering algorithm performs with an average efficiency of 94.4% for various document categories.
Keywords: Document clustering; Word frequency; Content-based image retrieval; Major colour set; Global colour signature; Distribution block signature; Hue saturation value; Region of Interest; RGB histogram-based image retrieval

Eike Jens Hoffmann, Karam Abdulahhad, Xiao Xiang Zhu,
Using social media images for building function classification,
Cities,
Volume 133,
2023,
104107,
ISSN 0264-2751,
https://doi.org/10.1016/j.cities.2022.104107.
(https://www.sciencedirect.com/science/article/pii/S0264275122005467)
Abstract: Urban land use on a building instance level is crucial geo-information for many applications yet challenging to obtain. Steet-level images are highly suited to predict building functions as the building façades provide clear hints. Social media image platforms contain billions of images, including but not limited to street perspectives. This study proposes a filtering pipeline to yield high-quality, ground-level imagery from large-scale social media image datasets to cope with this issue. The pipeline ensures all resulting images have complete and valid geotags with a compass direction to relate image content and spatial objects. We analyze our method on a culturally diverse social media dataset from Flickr with more than 28 million images from 42 cities worldwide. The obtained dataset is then evaluated in the context of a building function classification task with three classes: Commercial, residential, and other. Fine-tuned state-of-the-art architectures yield F1 scores of up to 0.51 on the filtered images. Our analysis shows that the quality of the labels from OpenStreetMap limits the performance. Human-validated labels increase the F1 score by 0.2. Therefore, we consider these labels weak and publish the resulting images from our pipeline and the depicted buildings as a weakly labeled dataset.
Keywords: Social media image analysis; Big data analytics; Building function classification; Urban land use

Zhang Tianlei, Zhang Xinyu, Guo Mu,
KeEL: knowledge enhanced entity linking in automatic biography construction,
The Journal of China Universities of Posts and Telecommunications,
Volume 22, Issue 1,
2015,
Pages 57-71,
ISSN 1005-8885,
https://doi.org/10.1016/S1005-8885(15)60625-2.
(https://www.sciencedirect.com/science/article/pii/S1005888515606252)
Abstract: Biography is a direct and extensive way to know the representation of well known peoples, however, for common people, there is poor knowledge for them to be recognized. In recent years, information extraction (IE) technologies have been used to automatically generate biography for any people with online information. One of the key challenges is the entity linking (EL) which can link biography sentence to Corresponding entities. Currently the used general EL systems usually generate errors originated from entity name variation and ambiguity. Compared with general text, biography sentences possess unique yet rarely studied relational knowledge (RK) and temporal knowledge (TK), which could sufficiently distinguish entities. This article proposed a new statistical framework called the knowledge enhanced EL (KeEL) system for automated biography construction. It utilizes commonsense knowledge like PK and TK to enhance Entity Linking. The performance of KeEL on Wikipedia data was evaluated. It is shown that, compared with state-of-the-art method, KeEL significantly improves the precision and recall of Entity Linking.
Keywords: knowledge enhanced entity linking; entity linking; biography construction; Markov logic network; Knowledge

Yanshan Wang, Liwei Wang, Majid Rastegar-Mojarad, Sungrim Moon, Feichen Shen, Naveed Afzal, Sijia Liu, Yuqun Zeng, Saeed Mehrabi, Sunghwan Sohn, Hongfang Liu,
Clinical information extraction applications: A literature review,
Journal of Biomedical Informatics,
Volume 77,
2018,
Pages 34-49,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2017.11.011.
(https://www.sciencedirect.com/science/article/pii/S1532046417302563)
Abstract: Background
With the rapid adoption of electronic health records (EHRs), it is desirable to harvest information and knowledge from EHRs to support automated systems at the point of care and to enable secondary use of EHRs for clinical and translational research. One critical component used to facilitate the secondary use of EHR data is the information extraction (IE) task, which automatically extracts and encodes clinical information from text.
Objectives
In this literature review, we present a review of recent published research on clinical information extraction (IE) applications.
Methods
A literature search was conducted for articles published from January 2009 to September 2016 based on Ovid MEDLINE In-Process & Other Non-Indexed Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, Web of Science, and ACM Digital Library.
Results
A total of 1917 publications were identified for title and abstract screening. Of these publications, 263 articles were selected and discussed in this review in terms of publication venues and data sources, clinical IE tools, methods, and applications in the areas of disease- and drug-related studies, and clinical workflow optimizations.
Conclusions
Clinical IE has been used for a wide range of applications, however, there is a considerable gap between clinical studies using EHR data and studies using clinical IE. This study enabled us to gain a more concrete understanding of the gap and to provide potential solutions to bridge this gap.
Keywords: Information extraction; Natural language processing; Application; Clinical notes; Electronic health records

Dragan Ivanović, Nikos Houssos,
Providing an Application-specific Interface over a CERIF Back-end: Challenges and Solutions,
Procedia Computer Science,
Volume 33,
2014,
Pages 11-17,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2014.06.003.
(https://www.sciencedirect.com/science/article/pii/S1877050914007935)
Abstract: This paper presents a case of presenting information modelled in CERIF through an application-specific programming interface which does not require CERIF expertise by the developers. The CERIF data model is semantically rich and can be used for detailed description of entities of scientific / research activity. A sophisticated application interface is required to exploit the full range of CERIF capabilities. On the other hand, it is obvious that data about scientific-research entities are utilised by users and software developers with various level of familiarity with the CERIF model. Because of the diversity of CRIS systems’ users’ knowledge of CERIF, in certain cases it is useful to create an additional application interface with elements of some simple model which can be easily understood and used by users with low level or without any knowledge of CERIF. The article presents the design and implementation of a wrapper that enables bidirectional conversion of entered data using a simple model application interface to a CERIF back-end and the use of the wrapper in the ENGAGE project as part of an open infrastructure for Public Sector Information.
Keywords: CERIF data model; wrapper; JPA; ENGAGE; REST API

Montserrat Batet, Arnau Erola, David Sánchez, Jordi Castellà-Roca,
Utility preserving query log anonymization via semantic microaggregation,
Information Sciences,
Volume 242,
2013,
Pages 49-63,
ISSN 0020-0255,
https://doi.org/10.1016/j.ins.2013.04.020.
(https://www.sciencedirect.com/science/article/pii/S0020025513003174)
Abstract: Query logs are of great interest for scientists and companies for research, statistical and commercial purposes. However, the availability of query logs for secondary uses raises privacy issues since they allow the identification and/or revelation of sensitive information about individual users. Hence, query anonymization is crucial to avoid identity disclosure. To enable the publication of privacy-preserved – but still useful – query logs, in this paper, we present an anonymization method based on semantic microaggregation. Our proposal aims at minimizing the disclosure risk of anonymized query logs while retaining their semantics as much as possible. First, a method to map queries to their formal semantics extracted from the structured categories of the Open Directory Project is presented. Then, a microaggregation method is adapted to perform a semantically-grounded anonymization of query logs. To do so, appropriate semantic similarity and semantic aggregation functions are proposed. Experiments performed using real AOL query logs show that our proposal better retains the utility of anonymized query logs than other related works, while also minimizing the disclosure risk.
Keywords: Privacy-preservation; Query logs; Data utility; Semantics; Microaggregation

Philip R.O. Payne,
Chapter 6 - From Data to Knowledge: An Introduction to Biomedical Informatics,
Editor(s): Geoffrey S. Ginsburg, Huntington F. Willard,
Genomic and Precision Medicine (Third Edition),
Academic Press,
2017,
Pages 89-104,
ISBN 9780128006818,
https://doi.org/10.1016/B978-0-12-800681-8.00006-2.
(https://www.sciencedirect.com/science/article/pii/B9780128006818000062)
Abstract: The field of Biomedical Informatics (BMI) is concerned with multimethod approaches to generating contextualized information and actionable knowledge from a variety of biological and healthcare-relevant data types. In doing so, BMI practitioners adopt and adapt a number of methods drawn from the computational, quantitative, and qualitative sciences. In this chapter, we provide an overview of such methods and a rationale for how they can be applied to address driving biological and clinical problems. Such use cases span a range from the biomolecular characterization of disease states to the comprehensive phenotyping of patients to the promotion of population health. In doing so, we hope to equip readers with the ability to critically understand and evaluate the use of multimethod approaches as are commonly encountered in the aforementioned application areas.
Keywords: Biomedical informatics; research design; evaluation; computer science; data science; qualitative research


Index,
Editor(s): Jan Höller, Vlasios Tsiatsis, Catherine Mulligan, Stamatis Karnouskos, Stefan Avesand, David Boyle,
From Machine-To-Machine to the Internet of Things,
Academic Press,
2014,
Pages 321-331,
ISBN 9780124076846,
https://doi.org/10.1016/B978-0-12-407684-6.00030-9.
(https://www.sciencedirect.com/science/article/pii/B9780124076846000309)

António Correia, Jorge Santos, Diogo Azevedo, Hugo Paredes, Benjamim Fonseca,
Putting “Human Crowds” in the Loop of Bibliography Evaluation: A Collaborative Working Environment for CSCW Publications,
Procedia Technology,
Volume 9,
2013,
Pages 573-583,
ISSN 2212-0173,
https://doi.org/10.1016/j.protcy.2013.12.064.
(https://www.sciencedirect.com/science/article/pii/S2212017313002181)
Abstract: The current impact of financial crisis on societal and scientific frameworks has raised the need to harvest and evaluate vast volumes of data in a socially-mediated interaction context to reduce knowledge gaps and accelerate innovation at a global scale. Existing mechanisms are inefficient for a single human to classify and transform data into knowledge patterns from a large number of publications. This time-consuming and computationally difficult activity requires a substantial cognitive effort grounded on scientific metrics and theoretical foundations to produce quality metadata through different knowledge representations. This paper reports on a work in progress community self-organizing bibliographic information system for semantic analytics focused on what scientific research data mean, and how they can be best interpreted through a division of intellectual labor among social, computer, and citizen scientists. Such a crowd labor ecosystem should be not restricted to traditional bibliometric approaches, attempting to uncover patterns and trends in publication data sets whilst intellectual connections can be examined to demonstrate how a scientific field is conceptually, intellectually, and socially structured.
Keywords: Crowdsourcing; CSCW; groupware; human computation; bibliographic information systems; scientific data repositories; scientometrics; socially-mediated bibliography evaluation

Karinne Ramirez-Amaro, Yezhou Yang, Gordon Cheng,
A survey on semantic-based methods for the understanding of human movements,
Robotics and Autonomous Systems,
Volume 119,
2019,
Pages 31-50,
ISSN 0921-8890,
https://doi.org/10.1016/j.robot.2019.05.013.
(https://www.sciencedirect.com/science/article/pii/S0921889018303932)
Abstract: This paper presents semantic-based methods for the understanding of human movements in robotic applications. To understand human movements, robots need to first, recognize the observed or demonstrated human activities, and secondly, learn different parameters to execute an action or robot behavior. In order to achieve that, several challenges need to be addressed such as the automatic segmentation of human activities, identification of important features of actions, determine the correct sequencing between activities, and obtain the correct mapping between the continuous data and the symbolic and semantic interpretations of the human movements. This paper aims to present state-of-the-art semantic-based approaches, especially the new emerging approaches that tackle the challenges of finding generic and compact semantic models for the robotics domain. Finally, we will highlight potential breakthroughs and challenges for the next years such as achieving scalability, better generalization, compact and flexible models, and higher system accuracy.
Keywords: Semantic representations; Understanding human movements; Human activity recognition; Robot action execution; Intelligent systems

A. Saridaki, G. Antonakos, A.L. Hager-Theodorides, E. Zoidis, G. Tsiamis, K. Bourtzis, A. Kominakis,
Combined haplotype blocks regression and multi-locus mixed model analysis reveals novel candidate genes associated with milk traits in dairy sheep,
Livestock Science,
Volume 220,
2019,
Pages 8-16,
ISSN 1871-1413,
https://doi.org/10.1016/j.livsci.2018.11.020.
(https://www.sciencedirect.com/science/article/pii/S1871141318307455)
Abstract: The aim of the present study was to identify ovine genomic regions and candidate genes impacting on milk traits i.e. milk yield, fat content, protein content and lactose content in the Frizarta dairy sheep. A total number of 481 ewes genotyped with the OvineSNP50 BeadChip were used. Haplotype regression analysis along with multi-locus mixed model analysis was employed to detect genomic markers associated with the traits. A total number of 9 single nucleotide polymorphisms were found to be associated with the traits under study. Four out of the 9 significant markers resided in genomic areas previously associated with milk traits according to the SheepQTLdb (http://www.animalgenome.org/cgi-bin/QTLdb/OA/search). A total number of 29 positional candidate genes located within a maximum distance of 250 kb upstream and downstream of 5 significant markers were identified. Gene prioritization analysis of the positional candidate genes based on the guilt-by-association principle suggested eleven genes i.e. ITGB4, H3F3B, ACOX1, WBP2, HTR1A, GPC5, GALR2, UNC13D, PRPSAP1, SRP68 and CDK3 as the most plausible functional candidate genes for milk traits in the Frizarta dairy sheep.
Keywords: Sheep; Milk traits; Association; Candidate genes; Markers

D. Rosaci, G.M.L. Sarné,
Recommending multimedia web services in a multi-device environment,
Information Systems,
Volume 38, Issue 2,
2013,
Pages 198-212,
ISSN 0306-4379,
https://doi.org/10.1016/j.is.2012.08.002.
(https://www.sciencedirect.com/science/article/pii/S0306437912001068)
Abstract: In the last years, the Web community has shown a broad interest in Web services that handle multimedia contents. To improve the usability of these services different tools have been proposed in the literature, and in this context agent-based recommender systems appear to be a promising solution. However, the recommender systems presented in the past do not take into account, in their recommendation algorithms, the effect of the device exploited by the user, while it is clear that the same user shows a different behavior in the presence of different devices. This paper tries to give a contribution in this setting, in order to match more accurately user preferences and interests. In particular, a new agent-based system is proposed, whose architecture allows to compute recommendations of multimedia Web services, considering the effect of the currently exploited device. Some experimental results confirm the high quality of the recommendations generated by the proposed approach.
Keywords: Device adaptivity; Multimedia web service; Recommender system


Proceedings,
Procedia Computer Science,
Volume 32,
2014,
Pages 1-10,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2014.05.386.
(https://www.sciencedirect.com/science/article/pii/S1877050914005869)

Ivan Moura, Ariel Teles, Davi Viana, Jean Marques, Luciano Coutinho, Francisco Silva,
Digital Phenotyping of Mental Health using multimodal sensing of multiple situations of interest: A Systematic Literature Review,
Journal of Biomedical Informatics,
Volume 138,
2023,
104278,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2022.104278.
(https://www.sciencedirect.com/science/article/pii/S1532046422002830)
Abstract: Many studies have used Digital Phenotyping of Mental Health (DPMH) to complement classic methods of mental health assessment and monitoring. This research area proposes innovative methods that perform multimodal sensing of multiple situations of interest (e.g., sleep, physical activity, mobility) to health professionals. In this paper, we present a Systematic Literature Review (SLR) to recognize, characterize and analyze the state of the art on DPMH using multimodal sensing of multiple situations of interest to professionals. We searched for studies in six digital libraries, which resulted in 1865 retrieved published papers. Next, we performed a systematic process of selecting studies based on inclusion and exclusion criteria, which selected 59 studies for the data extraction phase. First, based on the analysis of the extracted data, we describe an overview of this field, then presenting characteristics of the selected studies, the main mental health topics targeted, the physical and virtual sensors used, and the identified situations of interest. Next, we outline answers to research questions, describing the context data sources used to detect situations, the DPMH workflow used for multimodal sensing of situations, and the application of DPMH solutions in the mental health assessment and monitoring process. In addition, we recognize trends presented by DPMH studies, such as the design of solutions for high-level information recognition, association of features with mental states/disorders, classification of mental states/disorders, and prediction of mental states/disorders. We also recognize the main open issues in this research area. Based on the results of this SLR, we conclude that despite the potential and continuous evolution for using these solutions as medical decision support tools, this research area needs more work to overcome technology and methodological rigor issues to adopt proposed solutions in real clinical settings.
Keywords: Mental health; Human behavior; Digital phenotyping; Multimodal sensing; Ubiquitous computing

Nicolas Garcelon, Antoine Neuraz, Rémi Salomon, Hassan Faour, Vincent Benoit, Arthur Delapalme, Arnold Munnich, Anita Burgun, Bastien Rance,
A clinician friendly data warehouse oriented toward narrative reports: Dr. Warehouse,
Journal of Biomedical Informatics,
Volume 80,
2018,
Pages 52-63,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2018.02.019.
(https://www.sciencedirect.com/science/article/pii/S1532046418300388)
Abstract: Introduction
Clinical data warehouses are often oriented toward integration and exploration of coded data. However narrative reports are of crucial importance for translational research. This paper describes Dr. Warehouse®, an open source data warehouse oriented toward clinical narrative reports and designed to support clinicians’ day-to-day use.
Method
Dr. Warehouse relies on an original database model to focus on documents in addition to facts. Besides classical querying functionalities, the system provides an advanced search engine and Graphical User Interfaces adapted to the exploration of text. Dr. Warehouse is dedicated to translational research with cohort recruitment capabilities, high throughput phenotyping and patient centric views (including similarity metrics among patients). These features leverage Natural Language Processing based on the extraction of UMLS® concepts, as well as negation and family history detection.
Results
A survey conducted after 6 months of use at the Necker Children’s Hospital shows a high rate of satisfaction among the users (96.6%). During this period, 122 users performed 2837 queries, accessed 4,267 patients’ records and included 36,632 patients in 131 cohorts. The source code is available at this github link https://github.com/imagine-bdd/DRWH. A demonstration based on PubMed abstracts is available at https://imagine-plateforme-bdd.fr/dwh_pubmed/.
Keywords: Software; Computational biology; Method; Data warehouse; Rare diseases; Electronic health records; Information storage and retrieval; Text-mining

Hao Wu, Guo-hui Tian, Yan Li, Feng-yu Zhou, Peng Duan,
Spatial semantic hybrid map building and application of mobile service robot,
Robotics and Autonomous Systems,
Volume 62, Issue 6,
2014,
Pages 923-941,
ISSN 0921-8890,
https://doi.org/10.1016/j.robot.2013.01.001.
(https://www.sciencedirect.com/science/article/pii/S0921889013000067)
Abstract: In a mobile service robot, map building is a basis for navigation that allows the robot to perform a given task. When robots carry on mission planning and execution, they not only need structural information about the environment to navigation and self localization, but also need to possess a deeper knowledge to endow a robot with higher degrees of autonomy and intelligence. In this paper, the QR code based artificial labels are used to provide semantic concepts and relations of objects and rooms in indoor environments, which can solve the complexity and limitations of semantic recognition and scene understanding only with robot’s vision. Semantic knowledge offered by artificial labels is obtained to incarnate semantic representations of the environment, which are formed as the semantic map. In order to maintain the navigation abilities of the robot, the spatial modeling method of a human is imitated to describe the relations of function area as a global topological map by the spectrum clustering algorithm. The semantic map is combined with topological representations to build a spatial semantic hybrid map. Using the hybrid map, “semantic” path planning, the elementary management of objects, and the context based robot localization are studied to show the hybrid map’s practicability. The results of several experiments show that the spatial semantic hybrid map comes up to capture the human point-of-view of robot environments, enable a high-level reasonable service path, and achieve function-driven navigation.
Keywords: Artificial label; QR code; Semantic representation; Hybrid map; Service robot

Patrick Bertram, Christian Kränzler, Pascal Rübel, Martin Ruskowski,
Development of a Context-Aware Assistive System for Manual Repair Processes - A Combination of Probabilistic and Deterministic Approaches,
Procedia Manufacturing,
Volume 51,
2020,
Pages 598-604,
ISSN 2351-9789,
https://doi.org/10.1016/j.promfg.2020.10.084.
(https://www.sciencedirect.com/science/article/pii/S2351978920319417)
Abstract: Looking at the trend of mass customization of products, companies are confronted with an increasing number of different products and product variants. Especially for humans working in assembly and rework domain it is increasingly difficult to maintain an overview of all assembly paths and work steps. To tackle this problem, context aware assistance systems are introduced to the field. Although there is a lot of research in the area of context aware assistive systems, most work focuses on fixed work plans or purely probability-based activity recognition. As a result, these systems either restrict the worker’s personal way of working or the modelling of the work plans is complex and time-consuming. The goal of our approach is a context sensitive support for all performable steps at a given time. That system requires an intuitive modelling of work processes including an activity recognition. Therefore, we present a process model consisting of the combination of a petri net and aspects of Hidden Markov Models (HMM). Based on the modelled work process, the system determines performable work steps at the given time, while the activity recognition selects the most likely work step on the subset of feasible work steps. In this paper we describe the structure of the process model and how petri nets and approaches from the area of HMM are combined in this model. The outlook shows an implementation approach for the model.
Keywords: Assistive Systems; Information Modelling; Flexible Automation; Ergonomic; Health

Gul Esin Delipinar, Batuhan Kocaoglu,
Using SCOR Model to Gain Competitive Advantage: A Literature Review,
Procedia - Social and Behavioral Sciences,
Volume 229,
2016,
Pages 398-406,
ISSN 1877-0428,
https://doi.org/10.1016/j.sbspro.2016.07.150.
(https://www.sciencedirect.com/science/article/pii/S1877042816310850)
Abstract: In a developing and globalizing world, there is a fierce competition between the supply chains of the companies. The supply chain of a product involves activities from manufacturing raw materials to a final product which is eventually delivered to end user. The Supply-Chain Operations Reference (SCOR) provides a standard description of supply chain processes, performance metrics, best practice and enabling technologies. The basic structure of the reference model inspects the five supply-chain processes: Plan, Source, Make, Deliver, and Return. The purpose of this study is to develop and show application of the SCOR model for the companies. In this study, articles on SCOR model are evaluated. Observations and recommendations are shared according to the research gaps in the literature.
Keywords: Supply Chain; SCOR Model; Literature review

Fang Xie, Jian Wang, Ruibin Xiong, Neng Zhang, Yutao Ma, Keqing He,
An integrated service recommendation approach for service-based system development,
Expert Systems with Applications,
Volume 123,
2019,
Pages 178-194,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2019.01.025.
(https://www.sciencedirect.com/science/article/pii/S0957417419300260)
Abstract: With the wide adoption of service-oriented computing and cloud computing, service-based systems (SBSs), a kind of software systems that can offer certain functionalities by leveraging one or more Web services, become increasingly popular. A challenging issue in SBS development is to find suitable services from a variety of available (semantics different) services. Towards this issue, we propose a new service recommendation approach that can integrate diverse information of SBSs and their component services. In this research, SBSs, services, their respective attributes (e.g. content and categories) and SBS-service composition relations are modeled as a heterogeneous information network (HIN); and several semantic similarities between SBSs are measured on a set of meta-paths in the HIN. Particularly, a word embedding technique is used to learn word vectors from the content of SBSs and services, which contribute to better functional similarities between SBSs. Afterwards, the combinational weights of different similarities are optimized using a Bayesian personalized ranking algorithm. Services are finally recommended based on collaborative filtering. We identify two recommendation scenarios with different SBS requirements. By conducting a series of experiments on a real-world dataset crawled from the ProgrammableWeb, we validate the effectiveness of our approach and find out the optimal combinations of SBS similarities for those two scenarios.
Keywords: Service recommendation; Service-based system; Heterogeneous information network; Word embedding; Collaborative filtering

Olfa Mzoughi, Itheri Yahiaoui,
Deep learning-based segmentation for disease identification,
Ecological Informatics,
Volume 75,
2023,
102000,
ISSN 1574-9541,
https://doi.org/10.1016/j.ecoinf.2023.102000.
(https://www.sciencedirect.com/science/article/pii/S1574954123000298)
Abstract: Plant diseases have recently increased and exacerbated due to several factors such as climate change, chemicals’ misuse and pollution. They represent a severe threat for both economy and global food security. Recently, several researches have been proposed for plant disease identification through modern image-based recognition systems based on deep learning. However, several challenges still require further investigation. One is related to the high variety of leaf diseases/ species along with constraints related to the collection and annotation of real-world datasets. Other challenges are related to the study of leaf disease in uncontrolled environment. Compared to major existing researches, we propose in this article a new perspective to handle the problem with two main differences: First, while most approach aims to identify simultaneously a pair of species-disease, we propose to identify diseases independently of leaf species. This helps to recognize new species holding diseases that were previously learnt. Moreover, instead of using the global leaf image, we directly predict disease on the basis of the local disease symptom features. We believe that this may decrease the bias related to common context and/or background and enables to build a more generalised model for disease classification. In particular, we propose an hybrid system that combines strengths of deep learning-based semantic segmentation with classification capabilities to respectively extract infected regions and determine their identity. For that, an extensive experimentation including a comparison of different semantic segmentation and classification CNNs has been conducted on PlantVillage dataset (leaves within homogeneous background) in order to study the extent of use of local disease symptoms features to identify diseases. Specifically, a particular enhancement of disease identification accuracy has been demonstrated in IPM and BING datasets (leaves within uncontrolled background).
Keywords: Plant disease; Classification; Deep learning; Segmentation; Uncontrolled environment

Linlin Hou, Ji Zhang, Ou Wu, Ting Yu, Zhen Wang, Zhao Li, Jianliang Gao, Yingchun Ye, Rujing Yao,
Method and dataset entity mining in scientific literature: A CNN + BiLSTM model with self-attention,
Knowledge-Based Systems,
Volume 235,
2022,
107621,
ISSN 0950-7051,
https://doi.org/10.1016/j.knosys.2021.107621.
(https://www.sciencedirect.com/science/article/pii/S0950705121008832)
Abstract: The traditional literature analysis mainly focuses on the literature metadata such as topics, authors, keywords, references, and rarely pays attention to the main content of papers. However, in many scientific domains such as science, computing, engineering, the methods and datasets involved in the papers published carry important information and are quite useful for domain analysis and recommendation. Method and dataset entities have various forms, which are more difficult to recognize than common entities. In this paper, we propose a novel Method and Dataset Entity Recognition model (MDER), which is able to effectively extract the method and dataset entities from the main textual content of scientific papers. The model is the first to combine rule embedding, a parallel structure of Convolutional Neural Network (CNN) and a two-layer Bi-directional Long Short-Term Memory (BiLSTM) with the self-attention mechanism. We evaluate the proposed model on datasets constructed from the published papers of different research areas in computer science. Our model performs well in multiple areas and features a good capacity for cross-area learning and recognition. Ablation study indicates that building different modules collectively contributes to the good entity recognition performance as a whole. The data augmentation positively contributes to model training, making our model much more robust. We finally apply the proposed model on PAKDD papers published from 2009–2019 to mine insightful results over a long time span.11PAKDD is the abbreviation of Pacific-Asia Conference on Knowledge Discovery and Data Mining. Our source code and datasets are available at https://github.com/houlinlinvictoria/MDER.
Keywords: Literature analysis; Named entity recognition; Methods and datasets mining; CNN+BiLSTM-Attention-CRF structure

Lena Maier-Hein, Matthias Eisenmann, Duygu Sarikaya, Keno März, Toby Collins, Anand Malpani, Johannes Fallert, Hubertus Feussner, Stamatia Giannarou, Pietro Mascagni, Hirenkumar Nakawala, Adrian Park, Carla Pugh, Danail Stoyanov, Swaroop S. Vedula, Kevin Cleary, Gabor Fichtinger, Germain Forestier, Bernard Gibaud, Teodor Grantcharov, Makoto Hashizume, Doreen Heckmann-Nötzel, Hannes G. Kenngott, Ron Kikinis, Lars Mündermann, Nassir Navab, Sinan Onogur, Tobias Roß, Raphael Sznitman, Russell H. Taylor, Minu D. Tizabi, Martin Wagner, Gregory D. Hager, Thomas Neumuth, Nicolas Padoy, Justin Collins, Ines Gockel, Jan Goedeke, Daniel A. Hashimoto, Luc Joyeux, Kyle Lam, Daniel R. Leff, Amin Madani, Hani J. Marcus, Ozanan Meireles, Alexander Seitel, Dogu Teber, Frank Ückert, Beat P. Müller-Stich, Pierre Jannin, Stefanie Speidel,
Surgical data science – from concepts toward clinical translation,
Medical Image Analysis,
Volume 76,
2022,
102306,
ISSN 1361-8415,
https://doi.org/10.1016/j.media.2021.102306.
(https://www.sciencedirect.com/science/article/pii/S1361841521003510)
Abstract: Recent developments in data science in general and machine learning in particular have transformed the way experts envision the future of surgery. Surgical Data Science (SDS) is a new research field that aims to improve the quality of interventional healthcare through the capture, organization, analysis and modeling of data. While an increasing number of data-driven approaches and clinical applications have been studied in the fields of radiological and clinical data science, translational success stories are still lacking in surgery. In this publication, we shed light on the underlying reasons and provide a roadmap for future advances in the field. Based on an international workshop involving leading researchers in the field of SDS, we review current practice, key achievements and initiatives as well as available standards and tools for a number of topics relevant to the field, namely (1) infrastructure for data acquisition, storage and access in the presence of regulatory constraints, (2) data annotation and sharing and (3) data analytics. We further complement this technical perspective with (4) a review of currently available SDS products and the translational progress from academia and (5) a roadmap for faster clinical translation and exploitation of the full potential of SDS, based on an international multi-round Delphi process.
Keywords: Surgical data science; Artificial intelligence; Deep learning; Computer aided surgery; Clinical translation

Antonio Grilo, Ricardo Jardim-Goncalves,
Cloud-Marketplaces: Distributed e-procurement for the AEC sector,
Advanced Engineering Informatics,
Volume 27, Issue 2,
2013,
Pages 160-172,
ISSN 1474-0346,
https://doi.org/10.1016/j.aei.2012.10.004.
(https://www.sciencedirect.com/science/article/pii/S1474034612000973)
Abstract: The development of web-based collaborative platforms, BIM, and transactional e-marketplaces has been changing the way companies in the AEC sector work. However, due to interoperability issues, the main problems of distributed data and information management across AEC companies and projects are yet to be overcome. This paper presents the Cloud-Marketplace concept, which expands on earlier developments combining BIM, SOA, Cloud Computing, and e-marketplaces in order to create interoperable communities of e-platforms. The Vortaway case study is described to demonstrate the validation of the concept in simple AEC e-procurement processes.
Keywords: Electronic marketplaces; Building information modeling; Service-oriented architecture; Cloud computing; Cloud-Marketplaces

Imad Zeroual, Abdelhak Lakhouaja,
Data science in light of natural language processing: An overview,
Procedia Computer Science,
Volume 127,
2018,
Pages 82-91,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2018.01.101.
(https://www.sciencedirect.com/science/article/pii/S1877050918301121)
Abstract: The focus of data scientists is essentially divided into three areas: collecting data, analyzing data, and inferring information from data. Each one of these tasks requires special personnel, takes time, and costs money. Yet, the next and the fastidious step is how to turn data into products. Therefore, this field grabs the attention of many research groups in academia as well as industry. In the last decades, data-driven approaches came into existence and gained more popularity because they require much less human effort. Natural Language Processing (NLP) is strongly among the fields influenced by data. The growth of data is behind the performance improvement of most NLP applications such as machine translation and automatic speech recognition. Consequently, many NLP applications are frequently moving from rule-based systems and knowledge-based methods to data-driven approaches. However, collected data that are based on undefined design criteria or on technically unsuitable forms will be useless. Also, they will be neglected if the size is not enough to perform the required analysis and to infer the accurate information. The chief purpose of this overview is to shed some lights on the vital role of data in various fields and give a better understanding of data in light of NLP. Expressly, it describes what happen to data during its life-cycle: building, processing, analyzing, and exploring phases.
Keywords: Data science; Natural language processing; Data driven approches; Corpora; Machine learning

Irene Kilanioti, Alejandro Fernández-Montes, Damián Fernández-Cerero, Christos Mettouris, Valentina Nejkovic, Rabih Bashroush, George A. Papadopoulos,
A survey on cost-effective context-aware distribution of social data streams over energy-efficient data centres,
Simulation Modelling Practice and Theory,
Volume 93,
2019,
Pages 42-64,
ISSN 1569-190X,
https://doi.org/10.1016/j.simpat.2018.11.004.
(https://www.sciencedirect.com/science/article/pii/S1569190X18301709)
Abstract: Social media have emerged in the last decade as a viable and ubiquitous means of communication. The ease of user content generation within these platforms, e.g. check-in information, multimedia data, etc., along with the proliferation of Global Positioning System (GPS)-enabled, always-connected capture devices lead to data streams of unprecedented amount and a radical change in information sharing. Social data streams raise a variety of practical challenges, including derivation of real-time meaningful insights from effectively gathered social information, as well as a paradigm shift for content distribution with the leverage of contextual data associated with user preferences, geographical characteristics and devices in general. In this article we present a comprehensive survey that outlines the state-of-the-art situation and organizes challenges concerning social media streams and the infrastructure of the data centres supporting the efficient access to data streams in terms of content distribution, data diffusion, data replication, energy efficiency and network infrastructure. We systematize the existing literature and proceed to identify and analyse the main research points and industrial efforts in the area as far as modelling, simulation and performance evaluation are concerned.
Keywords: Social Data Streams; Social Media; Social Networks; Contextaware; Content Distribution; Multimedia Content; Energy-efficient; Data Centers; 5G; Infrastructure; Cost-effective

Qimeng Li, Raffaele Gravina, Ye Li, Saeed H. Alsamhi, Fangmin Sun, Giancarlo Fortino,
Multi-user activity recognition: Challenges and opportunities,
Information Fusion,
Volume 63,
2020,
Pages 121-135,
ISSN 1566-2535,
https://doi.org/10.1016/j.inffus.2020.06.004.
(https://www.sciencedirect.com/science/article/pii/S1566253520302980)
Abstract: Human activity recognition has attracted enormous research interest thanks to its fundamental importance in several domains spanning from health-care to security, safety, and entertainment. Robust and consolidated literature focused on the study of activities performed by single individuals, with a great variety of approaches in terms of sensing modalities, recognition techniques, a specific set of recognized activities, and final application objectives. However, much less research attention has been devoted to scenarios in which multiple people perform individual or joint actions and activities forming groups to achieve given common goals. This problem is often referred to as multi-user activity recognition. With the advent of the Internet-of-Things, smart objects are being pervasively spread in the environment and worn on the human body, enabling contextual and distributed recognition of group and multi-user activities. Therefore, this survey discusses clear motivations and advantages of multi-user activity recognition based on sensing methods, recognition approaches, and practical applications with attention to related data fusion challenges and techniques. By identifying the critical aspects of this multi-faceted problem, the survey aims to provide a systematic categorization and comparison framework of the state-of-the-art that drives the discussion to important open research challenges and future directions.
Keywords: Multi-user activity recognition; Group recognition; Data fusion; Collaboration

Franco Zambonelli, Andrea Omicini, Bernhard Anzengruber, Gabriella Castelli, Francesco L. De Angelis, Giovanna Di Marzo Serugendo, Simon Dobson, Jose Luis Fernandez-Marquez, Alois Ferscha, Marco Mamei, Stefano Mariani, Ambra Molesini, Sara Montagna, Jussi Nieminen, Danilo Pianini, Matteo Risoldi, Alberto Rosi, Graeme Stevenson, Mirko Viroli, Juan Ye,
Developing pervasive multi-agent systems with nature-inspired coordination,
Pervasive and Mobile Computing,
Volume 17, Part B,
2015,
Pages 236-252,
ISSN 1574-1192,
https://doi.org/10.1016/j.pmcj.2014.12.002.
(https://www.sciencedirect.com/science/article/pii/S1574119214001904)
Abstract: Pervasive computing systems can be modelled effectively as populations of interacting autonomous components. The key challenge to realizing such models is in getting separately-specified and -developed sub-systems to discover and interoperate with each other in an open and extensible way, supported by appropriate middleware services. In this paper, we argue that nature-inspired coordination models offer a promising way of addressing this challenge. We first frame the various dimensions along which nature-inspired coordination models can be defined, and survey the most relevant proposals in the area. We describe the nature-inspired coordination model developed within the SAPERE project as a synthesis of existing approaches, and show how it can effectively support the multifold requirements of modern and emerging pervasive services. We conclude by identifying what we think are the open research challenges in this area, and identify some research directions that we believe are promising.
Keywords: Pervasive computing; Multi-agent systems; Coordination models; Self-organization

Jawad Sadek, Farid Meziane,
Learning Causality for Arabic - Proclitics,
Procedia Computer Science,
Volume 142,
2018,
Pages 141-149,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2018.10.469.
(https://www.sciencedirect.com/science/article/pii/S1877050918321719)
Abstract: The use of prefixed particles is a prevalent linguistic form to express causation in Arabic Language. However, such particles are complicated and highly ambiguous as they imply different meanings according to their position in the text. This ambiguity emphasizes the high demand for a large-scale annotated corpus that contains instances of these particles. In this paper, we present the process of building our corpus, which includes a collection of annotated sentences each containing an instance of a candidate causal particle. We use the corpus to construct and optimize predictive models for the task of causation recognition. The performance of the best models is significantly better than the baselines. Arabic is a less–resourced language and we hope this work would help in building better Information Extraction systems.
Keywords: Causal Relations; Arabic Causality Extraction; Discourse Relation Recognition; Arabic Annotated Corpus

Pu Li, Bao Xiao, Wenjun Ma, Yuncheng Jiang, Zhifeng Zhang,
A graph-based semantic relatedness assessment method combining wikipedia features,
Engineering Applications of Artificial Intelligence,
Volume 65,
2017,
Pages 268-281,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2017.07.027.
(https://www.sciencedirect.com/science/article/pii/S0952197617301756)
Abstract: Semantic relatedness assessment between concepts is a critical issue in many domains such as artificial intelligence, information retrieval, psychology, biology, linguistics and cognitive science. Therefore, several methods assess relatedness by exploiting knowledge bases to express the semantics of concepts. However, there are some limitations such as high-dimensional space, high-computational complexity, fitting non-dynamic domains. Considering that Wikipedia, a domain-independent encyclopedic repository, which provides very large coverage, has been exploited by many methods as a huge semantic resource. In this paper, we propose a novel graph-based relatedness assessment method using Wikipedia features to avoid some of the limitations and drawbacks mentioned above. Firstly, for each term in a word pair, the top k most relevant Wikipedia concepts are returned by the Naive-ESA algorithm to reduce the dimensional space of Explicit Semantic Analysis (ESA) method. Secondly, for each different candidate concept in two relevant concept sets, we collect its categories set from the Wikipedia Category Graph (WCG). Based on the categories in WCG network, the relatedness between concepts at the correspondence position of the two sorted concept sets is computed as the association coefficient. Thirdly, based on this parameter, a novel relatedness assessment metric is presented. The evaluation is performed on some datasets well-recognized as benchmarks, using several widely used metrics and a new metric designed by ourselves. The result demonstrates that our method has a better correlation with the intuitions of human judgments than other related works.
Keywords: Semantic relatedness; Wikipedia features; Graph-based relatedness; Naive-ESA algorithm

A. Vaccaro, I. Pisica, L.L. Lai, A.F. Zobaa,
A review of enabling methodologies for information processing in smart grids,
International Journal of Electrical Power & Energy Systems,
Volume 107,
2019,
Pages 516-522,
ISSN 0142-0615,
https://doi.org/10.1016/j.ijepes.2018.11.034.
(https://www.sciencedirect.com/science/article/pii/S0142061518303405)
Abstract: The deployment of traditional computing, control and monitoring paradigms in modern smart grids is characterized by several severe limitations. These could restrict their capability to manage the huge complexities and the vast penetration of distributed and renewable energy resources that are expected in future operational scenarios. In this context, the design of more scalable and more flexible solution paradigms represents a relevant issue to address. The enabling technologies and methodologies aimed at addressing these complex challenges include decentralized computing, self-organizing sensor networks, proactive control, and holistic computing frameworks. Accordingly, in this paper a selection from literature published on these topics will be analyzed and reviewed, in order to outline the most relevant research trends, and the main open problems.
Keywords: Smart grids; Power distribution systems; Power systems operation; Computational intelligence

Kornel Skałkowski, Renata Słota, Dariusz Król, Jacek Kitowski,
QoS-based storage resources provisioning for grid applications,
Future Generation Computer Systems,
Volume 29, Issue 3,
2013,
Pages 713-727,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2012.08.011.
(https://www.sciencedirect.com/science/article/pii/S0167739X12001677)
Abstract: In recent years, distributed environments such as grids and clouds have evolved quickly and become widely used for both business and scientific purposes. Grid environments are used for solving increasingly complex problems in order to provide more accurate and up-to-date results. However, evolution of modern grid middlewares does not follow current trends in their utilization, which often leads to problems concerning provisioning of resources in grid environments. Many users of grid systems stumble on performance issues during execution of their applications. A special kind of grid applications which are dependent on effective provisioning of storage resources constitute data-intensive grid applications, i.e. applications which operate on large datasets. This paper addresses the issue of effective provisioning of storage resources for data-intensive grid applications based on the best-effort strategy. In order to cater applications demand on storage resources in heterogeneous and dynamic grid environments, we propose an approach relying on a combination of a cluster file system technology, a dedicated storage resources monitoring service and a management layer. The paper describes the way that this combination of technologies solves the issue of effective storage resources provisioning in grid environments by presentation of the FiVO/QStorMan toolkit, which constitutes an implementation of the proposed approach. In order to prove that the proposed approach actually reduces data-intensive applications execution time, extensive evaluation of the framework is presented for motivating scenarios which overlap most kinds of data-intensive applications.
Keywords: Grid environment; Cluster file system; QoS; Storage resources

K. Kosanke, F. Vernadat, M. Zelm,
Means to enable enterprise interoperation: CIMOSA Object Capability Profiles and CIMOSA Collaboration View,
Annual Reviews in Control,
Volume 39,
2015,
Pages 94-101,
ISSN 1367-5788,
https://doi.org/10.1016/j.arcontrol.2015.03.009.
(https://www.sciencedirect.com/science/article/pii/S1367578815000103)
Abstract: Collaboration between enterprises has moved from regional and national environments to a global one. This has tremendously increased the need for information exchange between collaborating partners. However, non-compatible (heterogeneous) ICT environments and inconsistent semantic and syntax of information to be exchanged are very often barriers in the interoperation process. Nevertheless, the relevant semantic information is usually collected in enterprise models. This information, amended by the corresponding syntax, can be collected in Object Capability Profiles to describe the required and provided information to be exchanged by the partners. An even more elaborate solution could be a new model view that collects all information needed to support intra- and inter-enterprise information exchanges. The authors first propose Object Capability Profiles for inter-organisational communication or interoperation applied to CIMOSA and the related international standard CEN/ISO 19440. Object Capability Profiles of objects potentially involved in information exchange will identify both semantic and syntax of the information to be exchanged. Comparison between required and provided information will detect any mismatch between the two and would allow automatic or manual corrections. Second, collaboration aspects are proposed to be modelled in a specific modelling view called Collaboration View. After recalling basics of the CIMOSA modelling language, the paper presents a proposal for both the Object Capability Profiling and the Collaboration View. A simplified illustrative example demonstrates the applicability of the two proposals.

Ping Ji, Xianhe Gao, Xueyou Hu,
Automatic image annotation by combining generative and discriminant models,
Neurocomputing,
Volume 236,
2017,
Pages 48-55,
ISSN 0925-2312,
https://doi.org/10.1016/j.neucom.2016.09.108.
(https://www.sciencedirect.com/science/article/pii/S0925231216313984)
Abstract: Generative model based image annotation methods have achieved good annotation performance. However, due to the problem of “semantic gap”, these methods always suffer from the images with similar visual features but different semantics. It seems promising to separate these images from semantic relevant ones by using discriminant models, since they have shown excellent generalization performance. Motivated to gain the benefits of both generative and discriminative approaches, in this paper, we propose a novel image annotation approach which combine the generative and discriminative models through local discriminant topics in the neighborhood of the unlabeled image. Singular Value Decomposition(SVD) is applied to group the images of the neighborhood into different topics according to their semantic labels. The semantic relevant images and the irrelevant ones are always assigned into different topics. By exploiting the discriminant information between different topics, Support Vector Machine(SVM) is applied to classify the unlabeled image into the relevant topic, from which the more accurate annotation will be obtained by reducing the bad influence of irrelevant images. The experiments on the ECCV 2002 [3] and NUS-WIDE [34] benchmark show that our method outperforms state-of-the-art annotation models.
Keywords: Image annotation; Multimedia; Content Analysis; Discriminant model

David Pinto, Helena Gómez-Adorno, Darnes Vilariño, Vivek Kumar Singh,
A graph-based multi-level linguistic representation for document understanding,
Pattern Recognition Letters,
Volume 41,
2014,
Pages 93-102,
ISSN 0167-8655,
https://doi.org/10.1016/j.patrec.2013.12.004.
(https://www.sciencedirect.com/science/article/pii/S0167865513004698)
Abstract: Document understanding goal requires discovery of meaningful patterns in text, which in turn requires analyzing documents and extracting information useful for a purpose. The documents to be analyzed are expected to be represented in some way. It is true that different representations of the same piece of text might have different information extraction outcomes. Therefore, it is very important to propose a reliable text representation schema that may incorporate as many features as possible, and at the same time provides use of efficient document understanding algorithms. In this paper, we propose a graph-based representation of textual documents that employs different levels of formal representation of natural language. This schema takes into account different linguistic levels, such as lexical, morphological, syntactical and semantics. The representation schema proposed is accompanied with a proposal for a technique which allows to extract useful text patterns based on the idea of minimum paths in the graph. The efficiency of the representation schema proposed has been tested in one case of study (Question-Answering for machine Reading Evaluation – QA4MRE), and the results of experiments carried in it, are described. The results obtained show that the proposed graph-based multi-level linguistic representation schema may be successfully used in the broader framework of document understanding.
Keywords: Text mining; Text representation; Graph-based representation

K. Kosanke, F. Vernadat, M. Zelm,
Means to enable Enterprise Interoperation: CIMOSA Object Capability Profiles and CIMOSA Collaboration View,
IFAC Proceedings Volumes,
Volume 47, Issue 3,
2014,
Pages 3292-3299,
ISSN 1474-6670,
ISBN 9783902823625,
https://doi.org/10.3182/20140824-6-ZA-1003.01294.
(https://www.sciencedirect.com/science/article/pii/S1474667016421142)
Abstract: Collaboration between enterprises has moved from regional and national environments to a global one. This has tremendously increased the need for information exchange between collaborating partners. However, non-compatible (heterogeneous) ICT environments and inconsistent semantic and syntax of information to be exchanged are very often barriers in the interoperation process. Nevertheless, the relevant semantic information is usually collected in enterprise models. This information, amended by the corresponding syntax, can be collected in Object Capability Profiles that describe the required and provided information to be exchanged by the partners. An even more elaborate solution could be a new model view that collects all information needed to support intra- and inter-enterprise information exchanges. The authors first propose the exploration of object capability profiles for inter-organisational communication or interoperation applied to CIMOSA and the related international standard CEN/ISO 19440. Object Capability Profiles of objects potentially involved in information exchange will identify both semantic and syntax of the information to be exchanged. Comparison between required and provided information will detect any mismatch between the two and would allow automatic or manual corrections. In addition, collaboration aspects are proposed to be modelled in a specific modelling view called Collaboration View. After recalling elements of the CIMOSA modelling language, the paper presents a proposal for both the Object Capability Profiling and the Collaboration View. A simplified illustrative example demonstrates the applicability of the two proposals.
Keywords: Enterprise Modelling; Enterprise Interoperability; Object Capability Profiles; Collaborative Networked Organisations; Collaborative View; CIMOSA

Manuel Díaz, Cristian Martín, Bartolomé Rubio,
State-of-the-art, challenges, and open issues in the integration of Internet of things and cloud computing,
Journal of Network and Computer Applications,
Volume 67,
2016,
Pages 99-117,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2016.01.010.
(https://www.sciencedirect.com/science/article/pii/S108480451600028X)
Abstract: The Internet of Things (IoT) is a paradigm based on the Internet that comprises many interconnected technologies like RFID (Radio Frequency IDentification) and WSAN (Wireless Sensor and Actor Networks) in order to exchange information. The current needs for better control, monitoring and management in many areas, and the ongoing research in this field, have originated the appearance and creation of multiple systems like smart-home, smart-city and smart-grid. However, the limitations of associated devices in the IoT in terms of storage, network and computing, and the requirements of complex analysis, scalability, and data access, require a technology like Cloud Computing to supplement this field. Moreover, the IoT can generate large amounts of varied data and quickly when there are millions of things feeding data to Cloud Computing. The latter is a clear example of Big Data, that Cloud Computing needs to take into account. This paper presents a survey of integration components: Cloud platforms, Cloud infrastructures and IoT Middleware. In addition, some integration proposals and data analytics techniques are surveyed as well as different challenges and open research issues are pointed out.
Keywords: Internet of things; Cloud computing; Infrastructure as a service; Cloud platforms; Iot middleware; Big data

Zengyang Li, Peng Liang, Paris Avgeriou,
Application of knowledge-based approaches in software architecture: A systematic mapping study,
Information and Software Technology,
Volume 55, Issue 5,
2013,
Pages 777-794,
ISSN 0950-5849,
https://doi.org/10.1016/j.infsof.2012.11.005.
(https://www.sciencedirect.com/science/article/pii/S0950584912002315)
Abstract: Context
Knowledge management technologies have been employed across software engineering activities for more than two decades. Knowledge-based approaches can be used to facilitate software architecting activities (e.g., architectural evaluation). However, there is no comprehensive understanding on how various knowledge-based approaches (e.g., knowledge reuse) are employed in software architecture.
Objective
This work aims to collect studies on the application of knowledge-based approaches in software architecture and make a classification and thematic analysis on these studies, in order to identify the gaps in the existing application of knowledge-based approaches to various architecting activities, and promising research directions.
Method
A systematic mapping study is conducted for identifying and analyzing the application of knowledge-based approaches in software architecture, covering the papers from major databases, journals, conferences, and workshops, published between January 2000 and March 2011.
Results
Fifty-five studies were selected and classified according to the architecting activities they contribute to and the knowledge-based approaches employed. Knowledge capture and representation (e.g., using an ontology to describe architectural elements and their relationships) is the most popular approach employed in architecting activities. Knowledge recovery (e.g., documenting past architectural design decisions) is an ignored approach that is seldom used in software architecture. Knowledge-based approaches are mostly used in architectural evaluation, while receive the least attention in architecture impact analysis and architectural implementation.
Conclusions
The study results show an increased interest in the application of knowledge-based approaches in software architecture in recent years. A number of knowledge-based approaches, including knowledge capture and representation, reuse, sharing, recovery, and reasoning, have been employed in a spectrum of architecting activities. Knowledge-based approaches have been applied to a wide range of application domains, among which “Embedded software” has received the most attention.
Keywords: Software architecture; Architecting activity; Knowledge-based approach; Systematic mapping study

Armando Ordoñez, Hugo Ordoñez, Juan Carlos Corrales, Carlos Cobos, Leandro Krug Wives, Lucinéia Heloisa Thom,
Grouping of business processes models based on an incremental clustering algorithm using fuzzy similarity and multimodal search,
Expert Systems with Applications,
Volume 67,
2017,
Pages 163-177,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2016.08.061.
(https://www.sciencedirect.com/science/article/pii/S0957417416304602)
Abstract: Nowadays, many companies standardize their operations through Business Process (BP), which are stored in repositories and reused when new functionalities are required. However, finding specific processes may become a cumbersome task due to the large size of these repositories. This paper presents MulTimodalGroup, a model for grouping and searching business processes. The grouping mechanism is built upon a clustering algorithm that uses a similarity function based on fuzzy logic; this grouping is performed using the results of each user request. By its part, the search is based on a multimodal representation that integrates textual and structural information of BP. The assessment of the proposed model was carried out in two phases: 1) internal quality assessment of groups and 2) external assessment of the created groups compared with an ideal set of groups. The assessment was performed using a closed BP collection designed collaboratively by 59 experts. The experimental results in each phase are promising and evidence the validity of the proposed model.
Keywords: Business process; Multimodal; Search; Fuzzy; Clustering; Assessment

Boris A. Galitsky,
Transfer learning of syntactic structures for building taxonomies for search engines,
Engineering Applications of Artificial Intelligence,
Volume 26, Issue 10,
2013,
Pages 2504-2515,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2013.08.010.
(https://www.sciencedirect.com/science/article/pii/S095219761300167X)
Abstract: We apply a paradigm of transfer learning to build a taxonomy of entities intended to improve search engine relevance in a vertical domain. The taxonomy construction process starts from the seed entities and mines available source domains for new entities associated with these seed entities. New entities are formed by applying the machine learning of syntactic parse trees (their generalizations) to the search results for existing entities to form commonalities between them. These commonality expressions then form parameters of existing entities, and are turned into new entities at the next learning iteration. To match natural language expressions between source and target domains, we use syntactic generalization, an operation which finds a set of maximal common sub-trees of constituency parse trees of these expressions. Taxonomy and syntactic generalization are applied to relevance improvement in search and text similarity assessment. We conduct an evaluation of the search relevance improvement in vertical and horizontal domains and observe significant contribution of the learned taxonomy in the former, and a noticeable contribution of a hybrid system in the latter domain. We also perform industrial evaluation of taxonomy and syntactic generalization-based text relevance assessment and conclude that a proposed algorithm for automated taxonomy learning is suitable for integration into industrial systems. The proposed algorithm is implemented as a component of Apache OpenNLP project.
Keywords: Learning taxonomy; Learning syntactic parse tree; Transfer learning; Syntactic generalization; Search relevance


Contents,
Procedia Computer Science,
Volume 35,
2014,
Pages 1-10,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2014.08.079.
(https://www.sciencedirect.com/science/article/pii/S1877050914010448)

Kostas Kolomvatsos, Christos Anagnostopoulos, Stathes Hadjiefthymiades,
An efficient Recommendation System based on the Optimal Stopping Theory,
Expert Systems with Applications,
Volume 41, Issue 15,
2014,
Pages 6796-6806,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2014.04.039.
(https://www.sciencedirect.com/science/article/pii/S0957417414002620)
Abstract: A Recommendation System (RS) aims to deliver meaningful recommendations to users for items (e.g., music and books), which are of high interest to them. We consider an RS which directly communicates with a set of providers in order to access the information of the items (e.g., descriptions), rate them according to the user’s preferences, and deliver an Item List (IL). The RS is enhanced with a mechanism, which sequentially observes the rating information (e.g., similarity degree) of the items and decides when to deliver the IL to the user, without exhausting the entire set of providers. Hence, the RS saves time and resources. We propose two mechanisms based on the theory of optimal stopping. Both mechanisms deliver an IL, which sufficiently matches to the user’s needs having examined a partial set of items. That is, the number of items in the delivered IL is optimal, producing a high level of user satisfaction, i.e., Quality of Recommendation (QoR). Our simulations reveal the efficiency of the mechanisms and quantify the benefits stemming from their adoption.
Keywords: Recommender Systems; Optimal Stopping Theory; Quality of Recommendation; Stochastic decision making

Julia A. Arinchekhina, Vyacheslav Orlov, Alexei V. Samsonovich, Vadim L. Ushakov,
Comparative Study of Semantic Mapping of Images,
Procedia Computer Science,
Volume 123,
2018,
Pages 47-56,
ISSN 1877-0509,
https://doi.org/10.1016/j.procs.2018.01.009.
(https://www.sciencedirect.com/science/article/pii/S1877050918300103)
Abstract: Semantic mapping is typically used to find deep connections between different words or text documents. The purpose of this study is to extend the semantic mapping method to images. For this purpose, the publicly available affective image database was used. Five methods of semantic map construction were compared, including the methods based on (a) human ranking, (b) fMRI data, (c) metadata, (d) predominant colors found in images, and (e) analysis of verbal description of images. The compatibility of outcomes was evaluated using traditional and canonical correlation. Overall, results obtained using different methods appear to be compatible with each other, except that the correlation of the predominant color and semantic measures was found not significant. Comparative strengths and weaknesses of these approaches are discussed. The obtained semantic map provides an insight into the interpretation of brain activity recorded via fMRI, and will be useful for building and validating models of human emotional cognition.
Keywords: semantic space; fMRI; image perception; affective computing

Carmen De Maio, Giuseppe Fenza, Vincenzo Loia, Mimmo Parente,
Time Aware Knowledge Extraction for microblog summarization on Twitter,
Information Fusion,
Volume 28,
2016,
Pages 60-74,
ISSN 1566-2535,
https://doi.org/10.1016/j.inffus.2015.06.004.
(https://www.sciencedirect.com/science/article/pii/S156625351500055X)
Abstract: Microblogging services like Twitter and Facebook collect millions of user generated content every moment about trending news, occurring events, and so on. Nevertheless, it is really a nightmare to find information of interest through the huge amount of available posts that are often noisy and redundant. In the era of Big Data, social media analytics services have caught increasing attention from both research and industry. Specifically, the dynamic context of microblogging requires to manage not only meaning of information but also the evolution of knowledge over the timeline. This work defines Time Aware Knowledge Extraction (briefly TAKE) methodology that relies on temporal extension of Fuzzy Formal Concept Analysis. In particular, a microblog summarization algorithm has been defined filtering the concepts organized by TAKE in a time-dependent hierarchy. The algorithm addresses topic-based summarization on Twitter. Besides considering the timing of the concepts, another distinguishing feature of the proposed microblog summarization framework is the possibility to have more or less detailed summary, according to the user’s needs, with good levels of quality and completeness as highlighted in the experimental results.
Keywords: Microblog summarization; Time awareness; Fuzzy Formal Concept Analysis; Big Data; Social media analytics

Daniel Cunliffe, Andreas Vlachidis, Daniel Williams, Douglas Tudhope,
Natural language processing for under-resourced languages: Developing a Welsh natural language toolkit,
Computer Speech & Language,
Volume 72,
2022,
101311,
ISSN 0885-2308,
https://doi.org/10.1016/j.csl.2021.101311.
(https://www.sciencedirect.com/science/article/pii/S088523082100108X)
Abstract: Language technology is becoming increasingly important across a variety of application domains which have become common place in large, well-resourced languages. However, there is a danger that small, under-resourced languages are being increasingly pushed to the technological margins. Under-resourced languages face significant challenges in delivering the underlying language resources necessary to support such applications. This paper describes the development of a natural language processing toolkit for an under-resourced language, Cymraeg (Welsh). Rather than creating the Welsh Natural Language Toolkit (WNLT) from scratch, the approach involved adapting and enhancing the language processing functionality provided for other languages within an existing framework and making use of external language resources where available. This paper begins by introducing the GATE NLP framework, which was used as the development platform for the WNLT. It then describes each of the core modules of the WNLT in turn, detailing the extensions and adaptations required for Welsh language processing. An evaluation of the WNLT is then reported. Following this, two demonstration applications are presented. The first is a simple text mining application that analyses wedding announcements. The second describes the development of a Twitter NLP application, which extends the core WNLT pipeline. As a relatively small-scale project, the WNLT makes use of existing external language resources where possible, rather than creating new resources. This approach of adaptation and reuse can provide a practical and achievable route to developing language resources for under-resourced languages.
Keywords: Natural language processing; Under-resourced languages; Welsh; Cymraeg; Language technology

Xiang Zhang, Nengcheng Chen, Zeqiang Chen, Lixin Wu, Xia Li, Liangpei Zhang, Liping Di, Jianya Gong, Deren Li,
Geospatial sensor web: A cyber-physical infrastructure for geoscience research and application,
Earth-Science Reviews,
Volume 185,
2018,
Pages 684-703,
ISSN 0012-8252,
https://doi.org/10.1016/j.earscirev.2018.07.006.
(https://www.sciencedirect.com/science/article/pii/S0012825217305044)
Abstract: In the last half-century, geoscience research has advanced due to multidisciplinary technologies, among which Information and Communication Technology (ICT) has played a vital role. However, scientifically organizing these ICTs toward improving geoscience measurements, data processing, and information services has encountered tremendous challenges. This paper reviews a profound revolution in geoscience that has resulted from the Geospatial Sensor Web (GSW), serving as a new cyber-physical spatio-temporal information infrastructure for geoscience on the World Wide Web (WWW). In contrast to previous experiment-based and sensor-based paradigms, the GSW-based paradigm is able to accomplish the following: (1) achieve integrated and sharable management of diverse sensing resources, (2) obtain real-time or near real-time and spatiotemporal continuous data, (3) conduct interoperable and online geoscience data processing and analysis, and (4) provide focusing services with web-based geoscience information and knowledge. As a benefit of the GSW, increasingly more geoscience disciplines are enjoying the value of real-time data, multi-source monitoring, online processing, and intelligence services. This paper reviews the evolution of geoscience research paradigm to demonstrate the scientific background of GSW. Then, we elaborates on four key methods provided by GSW, namely, integrated management, collaborative observation, scalable processing and fusion, and focusing service web capacity. Furthermore, current GSW prototypes and applications for environmental, hydrological, and natural disaster analysis are also reviewed. Moreover, four challenges to the future GSW in geoscience research are identified and analyzed, including integration with the Model Web initiative for sophisticated geo-processing, integration with humans for pervasive sensing, integration with Internet of Things (IoT) to achieve high-quality performance and data mining, and integration with Artificial Intelligence (AI) to provide smart geoservices. We have concluded that GSW has become an indispensable cyber-physical infrastructure, and will play a greater role in geoscience research and application.
Keywords: Collaborative observation; Focusing service; Geoscience; Geospatial; Infrastructure; Integrated management; Scalable processing; Sensor web

Nikolaos Vryzas, Lazaros Vrysis, Rigas Kotsakis, Charalampos Dimoulas,
A web crowdsourcing framework for transfer learning and personalized Speech Emotion Recognition,
Machine Learning with Applications,
Volume 6,
2021,
100132,
ISSN 2666-8270,
https://doi.org/10.1016/j.mlwa.2021.100132.
(https://www.sciencedirect.com/science/article/pii/S2666827021000669)
Abstract: Speech Emotion Recognition (SER) is an important part of Affective Computing and emotionally aware Human–Computer Interaction. Emotional expression may vary depending on the language, culture, and the speaker’s personality and vocal attributes. Speaker-adaptive systems can address this issue. In real-world applications, it is not feasible to obtain big datasets for deep learning model training from a specific speaker. This paper proposes a transfer learning approach for personalized SER based on convolutional neural networks. A CNN is trained in a multi-user dataset for generalization and then is fine-tuned for a small speaker-specific dataset. A VGGish model, pre-trained a large-scale dataset for audio event recognition is also evaluated for the task. This comparison highlights the significance of network capacity, dataset length, and domain-relativity for transfer learning. To enhance the applicability of this approach in real-world conditions, a web crowdsourcing application is implemented. An online platform is provided where contributors can follow a standard procedure to record and submit annotated utterances of emotional speech. The recordings are validated and added to the publicly available AESDD dataset of emotional speech. The platform can be used for the creation of personalized emotional speech datasets for speaker-adaptive SER, following the transfer learning strategies that have been evaluated.
Keywords: Speech Emotion Recognition; Transfer learning; Crowdsourcing; Convolutional neural networks; VGGish

Yasamin Eslami, Mario Lezoche, Philippe Kalitine, Sahand Ashouri,
How the Cooperative Cyber Physical Enterprise Information Systems (CCPEIS) improve the Semantic Interoperability in the domain of Industry 4.0 through the Knowledge formalization,
IFAC-PapersOnLine,
Volume 54, Issue 1,
2021,
Pages 924-929,
ISSN 2405-8963,
https://doi.org/10.1016/j.ifacol.2021.08.110.
(https://www.sciencedirect.com/science/article/pii/S2405896321008600)
Abstract: The cooperative enterprise information systems (CEIS) have admittance to a large amount of information and have to interoperate to achieve their goal. Industry 4.0 has been identified as a leading contributor in digitalization and automated manufacturing and it integrates what is called to be the cyber physical systems (CPS). This improvement opened various possibilities to enhance the semantic interoperability between the CEISs. The effort to formalize the knowledge extracted from the CPS contextualized data as compact clusters is a key point. This paper aims to present how the modern CEISs might improve their semantic interoperability through an adequate knowledge formalization.
Keywords: Collaborative Enterprise Information System; CPS; Semantic Interoperability; Knowledge formalization; Formal Concept Analysis

Rafael S. Costa, Andras Hartmann, Susana Vinga,
Kinetic modeling of cell metabolism for microbial production,
Journal of Biotechnology,
Volume 219,
2016,
Pages 126-141,
ISSN 0168-1656,
https://doi.org/10.1016/j.jbiotec.2015.12.023.
(https://www.sciencedirect.com/science/article/pii/S0168165615302248)
Abstract: Kinetic models of cellular metabolism are important tools for the rational design of metabolic engineering strategies and to explain properties of complex biological systems. The recent developments in high-throughput experimental data are leading to new computational approaches for building kinetic models of metabolism. Herein, we briefly survey the available databases, standards and software tools that can be applied for kinetic models of metabolism. In addition, we give an overview about recently developed ordinary differential equations (ODE)-based kinetic models of metabolism and some of the main applications of such models are illustrated in guiding metabolic engineering design. Finally, we review the kinetic modeling approaches of large-scale networks that are emerging, discussing their main advantages, challenges and limitations.
Keywords: Kinetic modeling of metabolism; Strain improvement; Systems metabolic engineering; Industrial biotechnology; Systems biology

Patrick Schneider, Fatos Xhafa,
Chapter 1 - IoT data streams: concepts and models: Introductory concepts and models,
Editor(s): Patrick Schneider, Fatos Xhafa,
Anomaly Detection and Complex Event Processing over IoT Data Streams,
Academic Press,
2022,
Pages 3-27,
ISBN 9780128238189,
https://doi.org/10.1016/B978-0-12-823818-9.00011-0.
(https://www.sciencedirect.com/science/article/pii/B9780128238189000110)
Abstract: This chapter introduces the main data stream characteristics, concepts, models, and processing in the context of IoT and Big Data. Further, the domain of Concept Drifts, an undesirable property of streaming data, will be described. Finally, the IoT in the context of healthcare applications and different stream scenarios will be introduced.
Keywords: IoT; Big Data; Data streams; Methods; Models; Drift concept; Healthcare IoT concepts; Healthcare IoT applications

Lucy Bastin, Dan Cornford, Richard Jones, Gerard B.M. Heuvelink, Edzer Pebesma, Christoph Stasch, Stefano Nativi, Paolo Mazzetti, Matthew Williams,
Managing uncertainty in integrated environmental modelling: The UncertWeb framework,
Environmental Modelling & Software,
Volume 39,
2013,
Pages 116-134,
ISSN 1364-8152,
https://doi.org/10.1016/j.envsoft.2012.02.008.
(https://www.sciencedirect.com/science/article/pii/S1364815212000564)
Abstract: Web-based distributed modelling architectures are gaining increasing recognition as potentially useful tools to build holistic environmental models, combining individual components in complex workflows. However, existing web-based modelling frameworks currently offer no support for managing uncertainty. On the other hand, the rich array of modelling frameworks and simulation tools which support uncertainty propagation in complex and chained models typically lack the benefits of web based solutions such as ready publication, discoverability and easy access. In this article we describe the developments within the UncertWeb project which are designed to provide uncertainty support in the context of the proposed ‘Model Web’. We give an overview of uncertainty in modelling, review uncertainty management in existing modelling frameworks and consider the semantic and interoperability issues raised by integrated modelling. We describe the scope and architecture required to support uncertainty management as developed in UncertWeb. This includes tools which support elicitation, aggregation/disaggregation, visualisation and uncertainty/sensitivity analysis. We conclude by highlighting areas that require further research and development in UncertWeb, such as model calibration and inference within complex environmental models.
Keywords: Uncertainty; Model web; UncertWeb; Web services; Uncertainty propagation; Visualisation; Interoperability

Hiteshwar Kumar Azad, Akshay Deepak,
Query expansion techniques for information retrieval: A survey,
Information Processing & Management,
Volume 56, Issue 5,
2019,
Pages 1698-1735,
ISSN 0306-4573,
https://doi.org/10.1016/j.ipm.2019.05.009.
(https://www.sciencedirect.com/science/article/pii/S0306457318305466)
Abstract: With the ever increasing size of the web, relevant information extraction on the Internet with a query formed by a few keywords has become a big challenge. Query Expansion (QE) plays a crucial role in improving searches on the Internet. Here, the user’s initial query is reformulated by adding additional meaningful terms with similar significance. QE – as part of information retrieval (IR) – has long attracted researchers’ attention. It has become very influential in the field of personalized social document, question answering, cross-language IR, information filtering and multimedia IR. Research in QE has gained further prominence because of IR dedicated conferences such as TREC (Text Information Retrieval Conference) and CLEF (Conference and Labs of the Evaluation Forum). This paper surveys QE techniques in IR from 1960 to 2017 with respect to core techniques, data sources used, weighting and ranking methodologies, user participation and applications – bringing out similarities and differences.
Keywords: Query expansion; Query reformulation; Information retrieval; Internet search


Contents,
Procedia Computer Science,
Volume 32,
2014,
Pages iii-ix,
ISSN 1877-0509,
https://doi.org/10.1016/S1877-0509(14)00756-X.
(https://www.sciencedirect.com/science/article/pii/S187705091400756X)

Jorge D. Camba, Manuel Contero, Pedro Company, David Pérez,
On the integration of model-based feature information in Product Lifecycle Management systems,
International Journal of Information Management,
Volume 37, Issue 6,
2017,
Pages 611-621,
ISSN 0268-4012,
https://doi.org/10.1016/j.ijinfomgt.2017.06.002.
(https://www.sciencedirect.com/science/article/pii/S0268401216308805)
Abstract: As CAD models continue to become more critical information sources in the product’s lifecycle, it is necessary to develop efficient mechanisms to store, retrieve, and manage larger volumes of increasingly complex data. Because of their unique characteristics, 3D annotations can be used to embed design and manufacturing information directly into a CAD model, which makes models effective vehicles to describe aspects of the geometry or provide additional information that can be connected to a particular geometric element. However, access to this information is often limited, difficult, and even unavailable to external applications. As model complexity and volume of information continue to increase, new and more powerful methods to interrogate these annotations are needed. In this paper, we demonstrate how 3D annotations can be effectively structured and integrated into a Product Lifecycle Management (PLM) system to provide a cohesive view of product-related information in a design environment. We present a strategy to organize and manage annotation information which is stored internally in a CAD model, and make it fully available through the PLM. Our method involves a dual representation of 3D annotations with enhanced data structures that provides shared and easy access to the information. We describe the architecture of a system which includes a software component for the CAD environment and a module that integrates with the PLM server. We validate our approach through a software prototype that uses a parametric modeling application and two commercial PLM packages with distinct data models.
Keywords: 3D annotations; PLM; PDM; Design intent; Model-based engineering

Szymon Bobek, Grzegorz J. Nalepa,
Uncertain context data management in dynamic mobile environments,
Future Generation Computer Systems,
Volume 66,
2017,
Pages 110-124,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2016.06.007.
(https://www.sciencedirect.com/science/article/pii/S0167739X1630187X)
Abstract: Building systems that acquire, process and reason with context data is a major challenge. Model updates and modifications are required for the mobile context-aware systems. Additionally, the nature of the sensor-based systems implies that the data required for the reasoning is not always available nor it is certain. Finally, the amount of context data can be significant and can grow fast, constantly being processed and interpreted under soft real-time constraints. Such characteristics make it a case for a challenging big data application. In this paper we argue, that mobile context-aware systems require specific methods to process big data related to context, at the same time being able to handle uncertainty and dynamics of this data. We identify and define main requirements and challenges for developing such systems. Then we discuss how these challenges were effectively addressed in the KnowMe project. In our solution, the acquisition of context data is made with the use of the AWARE platform. We extended it with techniques that can minimise the power consumption as well as conserve storage on a mobile device. The data can then be used to build rule models that can express user preferences and habits. We handle the missing or ambiguous data with number of uncertainty management techniques. Reasoning with rule models is provided by a rule engine developed for mobile platforms. Finally, we demonstrate how our tools can be used to visualise the stored data and simulate the operation of the system in a testing environment.
Keywords: Context-aware systems; Mobile systems; Uncertainty handling; Rule-based models; Big data management


Content List,
IFAC Proceedings Volumes,
Volume 47, Issue 3,
2014,
Pages i-ccclii,
ISSN 1474-6670,
ISBN 9783902823625,
https://doi.org/10.3182/20140824-6-ZA-1003.90001.
(https://www.sciencedirect.com/science/article/pii/S147466701641582X)

Saeed Aghaee, Cesare Pautasso,
End-User Development of Mashups with NaturalMash,
Journal of Visual Languages & Computing,
Volume 25, Issue 4,
2014,
Pages 414-432,
ISSN 1045-926X,
https://doi.org/10.1016/j.jvlc.2013.12.004.
(https://www.sciencedirect.com/science/article/pii/S1045926X14000020)
Abstract: Context: The emergence of the long-tail in the market of software applications is shifting the role of end-users from mere consumers to becoming developers of applications addressing their unique, personal, and transient needs. On the Web, a popular form of such applications is called mashup, built out of the lightweight composition of Web APIs (reusable software components delivered as a service through the Web). To enable end-users to build mashups, there is a key problem that must be overcome: End-users lack programming knowledge as well as the interest to learn how to master the complex set of Web technologies required to develop mashups. End-User Development (EUD) is an emerging research field dealing with this type of problems. Its main goal is to design tools and techniques facilitating the development of software applications by non-programmers. Objective: The paper describes the design and evaluation of NaturalMash, an innovative EUD tool for mashups (a mashup tool). NaturalMash aims at enabling non-professional users without any knowledge of programming languages and skills to create feature-rich, interactive, and useful mashups. Methods: The design of NaturalMash adopts a formative evaluation approach, and has completed three design and evaluation iterations. The formative evaluations utilize usability testing, think aloud protocol, questionnaires, observation, and unstructured interviews. Additionally, we compare the expressive power of naturalmash with the state-of-the-art mashup tools. Results: The results from the formative evaluations helped us identify important usability problems. From an assessment point of view, the results were promising and sggested that the proposed tool has a short and gentle learning curve in a way that even non-programmers are able to rapidly build useful mashups. Also, the comparative evaluation results showed that NaturalMash offers a competitive level of expressive power compared with existing mashup tools targeting non-programmers. Conclusion: As the evaluation results indicate, NaturalMash provides a high level of expressive power while it is still highly usable by non-programmers. These suggest that we have successfully achieved the objective of the proposed tool, distinguishing it from existing mashup tools that are either too limited or highly specialized for non-professional users.
Keywords: Mashups; End-User Development; Mashup tools; WYSIWYG; Natural language programming; Programming by Demonstration

Mark S. Nixon, Paulo L. Correia, Kamal Nasrollahi, Thomas B. Moeslund, Abdenour Hadid, Massimo Tistarelli,
On soft biometrics,
Pattern Recognition Letters,
Volume 68, Part 2,
2015,
Pages 218-230,
ISSN 0167-8655,
https://doi.org/10.1016/j.patrec.2015.08.006.
(https://www.sciencedirect.com/science/article/pii/S0167865515002615)
Abstract: Innovation has formed much of the rich history in biometrics. The field of soft biometrics was originally aimed to augment the recognition process by fusion of metrics that were sufficient to discriminate populations rather than individuals. This was later refined to use measures that could be used to discriminate individuals, especially using descriptions that can be perceived using human vision and in surveillance imagery. A further branch of this new field concerns approaches to estimate soft biometrics, either using conventional biometrics approaches or just from images alone. These three strands combine to form what is now known as soft biometrics. We survey the achievements that have been made in recognition by and in estimation of these parameters, describing how these approaches can be used and where they might lead to. The approaches lead to a new type of recognition, and one similar to Bertillonage which is one of the earliest approaches to human identification.
Keywords: Soft biometrics; Recognition; Surveillance; Body; Gender; Age

Javad Zarrin, Rui L. Aguiar, João Paulo Barraca,
Resource discovery for distributed computing systems: A comprehensive survey,
Journal of Parallel and Distributed Computing,
Volume 113,
2018,
Pages 127-166,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2017.11.010.
(https://www.sciencedirect.com/science/article/pii/S0743731517303088)
Abstract: Large-scale distributed computing environments provide a vast amount of heterogeneous computing resources from different sources for resource sharing and distributed computing. Discovering appropriate resources in such environments is a challenge which involves several different subjects. In this paper, we provide an investigation on the current state of resource discovery protocols, mechanisms, and platforms for large-scale distributed environments, focusing on the design aspects. We classify all related aspects, general steps, and requirements to construct a novel resource discovery solution in three categories consisting of structures, methods, and issues. Accordingly, we review the literature, analyzing various aspects for each category.
Keywords: Distributed systems; Resource sharing; Resource description; P2P; Grid computing; HPC

Jaime Chen, Eduardo Cañete, Daniel Garrido, Manuel Díaz, Krzysztof Piotrowski,
PICO: A platform independent communications middleware for heterogeneous devices in smart grids,
Computer Standards & Interfaces,
Volume 65,
2019,
Pages 1-14,
ISSN 0920-5489,
https://doi.org/10.1016/j.csi.2019.01.005.
(https://www.sciencedirect.com/science/article/pii/S0920548918300357)
Abstract: This paper presents a data-centric middleware responsible for real-time communication and data storage in smart grids. The middleware offers a high level programming model that provides ways of storing/getting information from/to the grid and encrypts messages thus providing a secure message exchange. The design has taken into account the heterogeneity of devices, software platforms and stakeholders involved in this kind of Cyber-physical System (CPS). A modular vision is followed in such a way that middleware components can be easily adapted to different platforms and a simple data interface is provided by using REST (Representational State Transfer) web services and a high level asynchronous API. The middleware has been used in the context of a European project (e-balance) where soft real-time requirements, security and low capacity devices were some of the requirements. The demonstration scenarios are detailed in this paper together with the validation tests that show that the use of this programming abstraction is feasible.
Keywords: Smart grid; Middleware; Communication platform; CPS; Security; REST

Mohamed Morchid, Richard Dufour, Georges Linarès,
Impact of Word Error Rate on theme identification task of highly imperfect human–human conversations,
Computer Speech & Language,
Volume 38,
2016,
Pages 68-85,
ISSN 0885-2308,
https://doi.org/10.1016/j.csl.2015.12.001.
(https://www.sciencedirect.com/science/article/pii/S0885230815001102)
Abstract: A review is proposed of the impact of word representations and classification methods in the task of theme identification of telephone conversation services having highly imperfect automatic transcriptions. We firstly compare two word-based representations using the classical Term Frequency-Inverse Document Frequency with Gini purity criteria (TF-IDF-Gini) method and the latent Dirichlet allocation (LDA) approach. We then introduce a classification method that takes advantage of the LDA topic space representation, highlighted as the best word representation. To do so, two assumptions about topic representation led us to choose a Gaussian Process (GP) based method. Its performance is compared with a classical Support Vector Machine (SVM) classification method. Experiments showed that the GP approach is a better solution to deal with the multiple theme complexity of a dialogue, no matter the conditions studied (manual or automatic transcriptions) (Morchid et al., 2014). In order to better understand results obtained using different word representation methods and classification approaches, we then discuss the impact of discriminative and non-discriminative words extracted by both word representations methods in terms of transcription accuracy (Morchid et al., 2014). Finally, we propose a novel study that evaluates the impact of the Word Error Rate (WER) in the LDA topic space learning process as well as during the theme identification task. This original qualitative study points out that selecting a small subset of words having the lowest WER (instead of using all the words) allows the system to better classify automatic transcriptions with an absolute gain of 0.9 point, in comparison to the best performance achieved on this dialogue classification task (precision of 83.3%).
Keywords: Speech analytics; Human–human dialogue; Latent Dirichlet allocation; Topic representation; Principal component analysis; Classification performance study

Jianxiao Liu, Keqing He, Jian Wang, Feng Liu, Xiaoxia Li,
Service organization and recommendation using multi-granularity approach,
Knowledge-Based Systems,
Volume 73,
2015,
Pages 181-198,
ISSN 0950-7051,
https://doi.org/10.1016/j.knosys.2014.10.002.
(https://www.sciencedirect.com/science/article/pii/S0950705114003694)
Abstract: Due to the rapid growth of all kinds of Web services on the Internet, how to help users discover services to meet their personalized and diverse requirements becomes a challenging and hot issue. In this paper, we propose a multi-granularity oriented service organization and recommendation approach in consideration of users’ role, requested goal, and service execution process. Based on modeling the Role (R), Goal (G), Process (P), and Services (S) models that are related to a specific domain problem (SDP), we use the dependency relationships among RGPS elements to realize on-demand service organization. Four kinds of service recommendation algorithms are designed according to different representations of users’ requirements. Then the services with different granularity and partnerships are provided to users. In addition, the corresponding services evolution algorithms are designed to make the organized services adapt to dynamic changing environments. Finally, we conduct experiments to validate the effectiveness of the proposed methods.
Keywords: On-demand organization; Multi-granularity; Service recommendation; RGPS; SDP

Aitor Almeida, Rubén Mulero, Piercosimo Rametta, Vladimir Urošević, Marina Andrić, Luigi Patrono,
A critical analysis of an IoT—aware AAL system for elderly monitoring,
Future Generation Computer Systems,
Volume 97,
2019,
Pages 598-619,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.03.019.
(https://www.sciencedirect.com/science/article/pii/S0167739X18321769)
Abstract: A growing number of elderly people (65+ years old) are affected by particular conditions, such as Mild Cognitive Impairment (MCI) and frailty, which are characterized by a gradual cognitive and physical decline. Early symptoms may spread across years and often they are noticed only at late stages, when the outcomes remain irrevocable and require costly intervention plans. Therefore, the clinical utility of early detecting these conditions is of substantial importance in order to avoid hospitalization and lessen the socio-economic costs of caring, while it may also significantly improve elderly people’s quality of life. This work deals with a critical performance analysis of an Internet of Things aware Ambient Assisted Living (AAL) system for elderly monitoring. The analysis is focused on three main system components: (i) the City-wide data capturing layer, (ii) the Cloud-based centralized data management repository, and (iii) the risk analysis and prediction module. Each module can provide different operating modes, therefore the critical analysis aims at defining which are the best solutions according to context’s needs. The proposed system architecture is used by the H2020 City4Age project to support geriatricians for the early detection of MCI and frailty conditions.
Keywords: Ambient assisted living; BLE; Internet of things; Big data; Data analytics; Performance

Isabel Moreno, Ester Boldrini, Paloma Moreda, M. Teresa Romá-Ferri,
DrugSemantics: A corpus for Named Entity Recognition in Spanish Summaries of Product Characteristics,
Journal of Biomedical Informatics,
Volume 72,
2017,
Pages 8-22,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2017.06.013.
(https://www.sciencedirect.com/science/article/pii/S1532046417301363)
Abstract: For the healthcare sector, it is critical to exploit the vast amount of textual health-related information. Nevertheless, healthcare providers have difficulties to benefit from such quantity of data during pharmacotherapeutic care. The problem is that such information is stored in different sources and their consultation time is limited. In this context, Natural Language Processing techniques can be applied to efficiently transform textual data into structured information so that it could be used in critical healthcare applications, being of help for physicians in their daily workload, such as: decision support systems, cohort identification, patient management, etc. Any development of these techniques requires annotated corpora. However, there is a lack of such resources in this domain and, in most cases, the few ones available concern English. This paper presents the definition and creation of DrugSemantics corpus, a collection of Summaries of Product Characteristics in Spanish. It was manually annotated with pharmacotherapeutic named entities, detailed in DrugSemantics annotation scheme. Annotators were a Registered Nurse (RN) and two students from the Degree in Nursing. The quality of DrugSemantics corpus has been assessed by measuring its annotation reliability (overall F=79.33% [95%CI: 78.35–80.31]), as well as its annotation precision (overall P=94.65% [95%CI: 94.11–95.19]). Besides, the gold-standard construction process is described in detail. In total, our corpus contains more than 2000 named entities, 780 sentences and 226,729 tokens. Last, a Named Entity Classification module trained on DrugSemantics is presented aiming at showing the quality of our corpus, as well as an example of how to use it.
Keywords: Corpus; Reliability; Precision; Named Entity Recognition; Spanish; Summary of Product Characteristics

David Newton,
Literature listing,
World Patent Information,
Volume 38,
2014,
Pages 95-104,
ISSN 0172-2190,
https://doi.org/10.1016/j.wpi.2014.04.001.
(https://www.sciencedirect.com/science/article/pii/S0172219014000453)

Ruowei Xiao, Zhanwei Wu, Dongyu Wang,
A Finite-State-Machine model driven service composition architecture for internet of things rapid prototyping,
Future Generation Computer Systems,
Volume 99,
2019,
Pages 473-488,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.04.050.
(https://www.sciencedirect.com/science/article/pii/S0167739X18322052)
Abstract: The emerging field of Internet of Things (IoT) offers an unprecedented opportunity for a wide spectrum of applications. However, most applications have been integrating IoT devices through proprietary mechanisms and with closed technology stacks. The monolithic, mostly vendor-specific development architecture leads to soaring customization costs and limited component reusability. It impedes the full-fledged IoT applications in cross-organizational, general-purpose and rapid-changing scenarios. This research intends to provide a coherent architecture that enables interoperable, low-cost and user-customizable IoT rapid prototyping. Under this architecture, each IoT component, either a physical device or a control logic, is abstracted into an independent web service that described by a set of transferable states. By concatenating a valid chain of state transfers between web services, IoT components are further assembled into customizable applications. In this research, a Finite-State-Machine (FSM) model driven architecture is established and a typical implementation of the proposed architecture, i.e. the Hyper Sensor Markup Language (HSML) is provided. We also discuss two practical use cases and related evaluations.
Keywords: IoT service composition; Rapid prototyping; IoT web application development

Stefano Balbi, Agustin del Prado, Patricia Gallejones, Chandanathil Pappachan Geevan, Guillermo Pardo, Elena Pérez-Miñana, Rosa Manrique, Cuitlahuac Hernandez-Santiago, Ferdinando Villa,
Modeling trade-offs among ecosystem services in agricultural production systems,
Environmental Modelling & Software,
Volume 72,
2015,
Pages 314-326,
ISSN 1364-8152,
https://doi.org/10.1016/j.envsoft.2014.12.017.
(https://www.sciencedirect.com/science/article/pii/S1364815214003740)
Abstract: Although agricultural ecosystems can provide humans with a wide set of benefits agricultural production system management is mainly driven by food production. As a consequence, a need to ensure food security globally has been accompanied by a significant decline in the state of ecosystems. In order to reduce negative trade-offs and identify potential synergies it is necessary to improve our understanding of the relationships between various ecosystem services (ES) as well as the impacts of farm management on ES provision. We present a spatially explicit application that captures and quantifies ES trade-offs in the crop systems of Llanada Alavesa in the Basque Country. Our analysis presents a quantitative assessment of selected ES including crop yield, water supply and quality, climate regulation and air quality. The study is conducted using semantic meta-modeling, a technique that enables flexible integration of models to overcome the service-by-service modeling approach applied traditionally in ES assessment.
Keywords: Agricultural systems; Ecosystem services; Trade-offs; Food provision; Model integration

David Newton,
Literature listing,
World Patent Information,
Volume 35, Issue 3,
2013,
Pages 281-288,
ISSN 0172-2190,
https://doi.org/10.1016/j.wpi.2013.04.003.
(https://www.sciencedirect.com/science/article/pii/S0172219013000604)

Sam Hume, Jozef Aerts, Surendra Sarnikar, Vojtech Huser,
Current applications and future directions for the CDISC Operational Data Model standard: A methodological review,
Journal of Biomedical Informatics,
Volume 60,
2016,
Pages 352-362,
ISSN 1532-0464,
https://doi.org/10.1016/j.jbi.2016.02.016.
(https://www.sciencedirect.com/science/article/pii/S1532046416000381)
Abstract: Introduction
In order to further advance research and development on the Clinical Data Interchange Standards Consortium (CDISC) Operational Data Model (ODM) standard, the existing research must be well understood. This paper presents a methodological review of the ODM literature. Specifically, it develops a classification schema to categorize the ODM literature according to how the standard has been applied within the clinical research data lifecycle. This paper suggests areas for future research and development that address ODM’s limitations and capitalize on its strengths to support new trends in clinical research informatics.
Methods
A systematic scan of the following databases was performed: (1) ABI/Inform, (2) ACM Digital, (3) AIS eLibrary, (4) Europe Central PubMed, (5) Google Scholar, (5) IEEE Xplore, (7) PubMed, and (8) ScienceDirect. A Web of Science citation analysis was also performed. The search term used on all databases was “CDISC ODM.” The two primary inclusion criteria were: (1) the research must examine the use of ODM as an information system solution component, or (2) the research must critically evaluate ODM against a stated solution usage scenario. Out of 2686 articles identified, 266 were included in a title level review, resulting in 183 articles. An abstract review followed, resulting in 121 remaining articles; and after a full text scan 69 articles met the inclusion criteria.
Results
As the demand for interoperability has increased, ODM has shown remarkable flexibility and has been extended to cover a broad range of data and metadata requirements that reach well beyond ODM’s original use cases. This flexibility has yielded research literature that covers a diverse array of topic areas. A classification schema reflecting the use of ODM within the clinical research data lifecycle was created to provide a categorized and consolidated view of the ODM literature. The elements of the framework include: (1) EDC (Electronic Data Capture) and EHR (Electronic Health Record) infrastructure; (2) planning; (3) data collection; (4) data tabulations and analysis; and (5) study archival. The analysis reviews the strengths and limitations of ODM as a solution component within each section of the classification schema. This paper also identifies opportunities for future ODM research and development, including improved mechanisms for semantic alignment with external terminologies, better representation of the CDISC standards used end-to-end across the clinical research data lifecycle, improved support for real-time data exchange, the use of EHRs for research, and the inclusion of a complete study design.
Conclusions
ODM is being used in ways not originally anticipated, and covers a diverse array of use cases across the clinical research data lifecycle. ODM has been used as much as a study metadata standard as it has for data exchange. A significant portion of the literature addresses integrating EHR and clinical research data. The simplicity and readability of ODM has likely contributed to its success and broad implementation as a data and metadata standard. Keeping the core ODM model focused on the most fundamental use cases, while using extensions to handle edge cases, has kept the standard easy for developers to learn and use.
Keywords: ODM; Define-XML; CDISC; Interoperability; Clinical trial; EHR

Duc-Thuan Vo, Ebrahim Bagheri,
Self-training on refined clause patterns for relation extraction,
Information Processing & Management,
Volume 54, Issue 4,
2018,
Pages 686-706,
ISSN 0306-4573,
https://doi.org/10.1016/j.ipm.2017.02.009.
(https://www.sciencedirect.com/science/article/pii/S0306457316303259)
Abstract: Within the context of Information Extraction (IE), relation extraction is oriented towards identifying a variety of relation phrases and their arguments in arbitrary sentences. In this paper, we present a clause-based framework for information extraction in textual documents. Our framework focuses on two important challenges in information extraction: 1) Open Information Extraction and (OIE), and 2) Relation Extraction (RE). In the plethora of research that focus on the use of syntactic and dependency parsing for the purposes of detecting relations, there has been increasing evidence of incoherent and uninformative extractions. The extracted relations may even be erroneous at times and fail to provide a meaningful interpretation. In our work, we use the English clause structure and clause types in an effort to generate propositions that can be deemed as extractable relations. Moreover, we propose refinements to the grammatical structure of syntactic and dependency parsing that help reduce the number of incoherent and uninformative extractions from clauses. In our experiments both in the open information extraction and relation extraction domains, we carefully evaluate our system on various benchmark datasets and compare the performance of our work against existing state-of-the-art information extraction systems. Our work shows improved performance compared to the state-of-the-art techniques.
Keywords: Relation extraction; Open information extraction; Self-training algorithm; Syntactic parsing; Dependency parsing

Henrik Leopold, Jan Mendling, Hajo A. Reijers, Marcello La Rosa,
Simplifying process model abstraction: Techniques for generating model names,
Information Systems,
Volume 39,
2014,
Pages 134-151,
ISSN 0306-4379,
https://doi.org/10.1016/j.is.2013.06.007.
(https://www.sciencedirect.com/science/article/pii/S0306437913001026)
Abstract: The increased adoption of business process management approaches, tools, and practices has led organizations to accumulate large collections of business process models. These collections can easily include from a hundred to a thousand models, especially in the context of multinational corporations or as a result of organizational mergers and acquisitions. A concrete problem is thus how to maintain these large repositories in such a way that their complexity does not hamper their practical usefulness as a means to describe and communicate business operations. This paper proposes a technique to automatically infer suitable names for business process models and fragments thereof. This technique is useful for model abstraction scenarios, as for instance when user-specific views of a repository are required, or as part of a refactoring initiative aimed to simplify the repository's complexity. The technique is grounded in an adaptation of the theory of meaning to the realm of business process models. We implemented the technique in a prototype tool and conducted an extensive evaluation using three process model collections from practice and a case study involving process modelers with different experience.
Keywords: Business process model; Model name; Process model repository; Refactoring; Model abstraction

Davide Colla, Enrico Mensa, Daniele P. Radicioni,
Novel metrics for computing semantic similarity with sense embeddings,
Knowledge-Based Systems,
Volume 206,
2020,
106346,
ISSN 0950-7051,
https://doi.org/10.1016/j.knosys.2020.106346.
(https://www.sciencedirect.com/science/article/pii/S0950705120305025)
Abstract: In the last years many efforts have been spent to build word embeddings, a representational device in which word meanings are described through dense unit vectors of real numbers over a continuous, high-dimensional Euclidean space, where similarity can be interpreted as a metric. Afterwards, sense-level embeddings have been proposed to describe the meaning of senses, rather than terms. More recently, additional intermediate representations have been designed, providing a vector description for pairs 〈term,sense〉, and mapping both term and sense descriptions onto a shared semantic space. However, surprisingly enough, this wealth of approaches and resources has not been supported by a parallel refinement in the metrics used to compute semantic similarity: to date, the semantic similarity featuring two input entities is mostly computed as the maximization of some angular distance intervening between vector pairs, typically cosine similarity. In this work we introduce two novel similarity metrics to compare sense-level representations, and show that by exploiting the features of sense-embeddings it is possible to substantially improve on existing strategies, by obtaining enhanced correlation with human similarity ratings. Additionally, we argue that semantic similarity needs to be complemented by another task, involving the identification of the senses at the base of the similarity rating. We experimentally verified that the proposed metrics are beneficial when dealing with both semantic similarity task and sense identification task. The experimentation also provides a detailed how-to illustrating how six important sets of sense embeddings can be used to implement the proposed similarity metrics.
Keywords: Semantic similarity; Semantic similarity metrics; Sense embeddings; Word embeddings; Sense identification; Lexical semantics; Cognitively plausible similarity metrics

Hrishikesh Bhaumik, Siddhartha Bhattacharyya, Mausumi Das Nath, Susanta Chakraborty,
Hybrid soft computing approaches to content based video retrieval: A brief review,
Applied Soft Computing,
Volume 46,
2016,
Pages 1008-1029,
ISSN 1568-4946,
https://doi.org/10.1016/j.asoc.2016.03.022.
(https://www.sciencedirect.com/science/article/pii/S1568494616301314)
Abstract: There has been an unrestrained growth of videos on the Internet due to proliferation of multimedia devices. These videos are mostly stored in unstructured repositories which pose enormous challenges for the task of both image and video retrieval. Users aim to retrieve videos of interest having content which is relevant to their need. Traditionally, low-level visual features have been used for content based video retrieval (CBVR). Consequently, a gap existed between these low-level features and the high level semantic content. The semantic differential was partially bridged by proliferation of research on interest point detectors and descriptors, which represented mid-level features of the content. The computational time and human interaction involved in the classical approaches for CBVR are quite cumbersome. In order to increase the accuracy, efficiency and effectiveness of the retrieval process, researchers resorted to soft computing paradigms. The entire retrieval task was automated to a great extent using individual soft computing components. Due to voluminous growth in the size of multimedia databases, augmented by an exponential rise in the number of users, integration of two or more soft computing techniques was desirable for enhanced efficiency and accuracy of the retrieval process. The hybrid approaches serve to enhance the overall performance and robustness of the system with reduced human interference. This article is targeted to focus on the relevant hybrid soft computing techniques which are in practice for content-based image and video retrieval.
Keywords: Content-based video retrieval; Video segmentation; Soft computing; Hybrid soft computing

Zhaoxia Wang, Arthur H.M. ter Hofstede, Chun Ouyang, Moe Wynn, Jianmin Wang, Xiaochen Zhu,
How to guarantee compliance between workflows and product lifecycles?,
Information Systems,
Volume 42,
2014,
Pages 195-215,
ISSN 0306-4379,
https://doi.org/10.1016/j.is.2014.01.006.
(https://www.sciencedirect.com/science/article/pii/S0306437914000246)
Abstract: Product lifecycle management (PLM) systems are widely used in the manufacturing industry. A core feature of such systems is to provide support for versioning of product data. As workflow functionality is increasingly used in PLM systems, the possibility emerges that the versioning transitions for product objects as encapsulated in process models do not comply with the valid version control policies mandated in the objects’ actual lifecycles. In this paper we propose a solution to tackle the (non-)compliance issues between processes and object version control policies. We formally define the notion of compliance between these two artifacts in product lifecycle management and then develop a compliance checking method which employs a well-established workflow analysis technique. This forms the basis of a tool which offers automated support to the proposed approach. By applying the approach to a collection of real-life specifications in a main PLM system, we demonstrate the practical applicability of our solution to the field.
Keywords: Product lifecycle management; Workflow management; Verification; Process model; Compliance

Zhengrong Cheng, Yongsheng Ma,
A functional feature modeling method,
Advanced Engineering Informatics,
Volume 33,
2017,
Pages 1-15,
ISSN 1474-0346,
https://doi.org/10.1016/j.aei.2017.04.003.
(https://www.sciencedirect.com/science/article/pii/S1474034616304347)
Abstract: With the advances in CAD technology, it has been increasingly convenient to model product shapes digitally. For example, in a feature-based parametric CAD system, the product shape could be parameterized and thus altered with the change of parameters. However, without a consistent and systematic CAD modeling method, CAD models are not robust enough to capture functional design knowledge and cope with design changes, especially functional changes. A poorly constructed CAD model could result in erroneous or inconsistent design that requires a lot of expertise, manpower and repetitive computation to rebuild a valid and consistent model. The situation can be worse if the model is complex. The gap between functional design considerations and procedural CAD modeling demands an integrated CAD modeling approach. This paper proposes a functional feature-based CAD modeling method to guide designers building CAD models that are valid and yet agile to represent functional design considerations. A case study is presented to demonstrate the feasibility of the proposed research.
Keywords: CAD; Functional modeling; Feature-based design; Modeling methodology

Jing Bai, Haonan Luo, Feiwei Qin,
Design pattern modeling and extraction for CAD models,
Advances in Engineering Software,
Volume 93,
2016,
Pages 30-43,
ISSN 0965-9978,
https://doi.org/10.1016/j.advengsoft.2015.12.005.
(https://www.sciencedirect.com/science/article/pii/S0965997815300119)
Abstract: Design pattern is widely used in the software engineering field, which enables designers to reuse existing mature designs from a high level perspective. Inspired by this idea, a novel approach is proposed to extract design patterns in the CAD field. First, the characteristics for a good design pattern are analyzed and the model for representing design patterns is elaborated. Then, given a set of 3D feature-based CAD models, the corresponding extraction approach is proposed, which includes three important phases: (1) extracting reusable regions with high cohesion, low coupling and moderate complexity so as to form a relative integrated function; (2) constructing candidate design patterns by clustering reusable regions using a graph-oriented agglomerative hierarchical clustering algorithm; (3) determining the final design patterns by choosing those candidate design patterns with high frequency and sufficient information. Finally, a design pattern extraction prototype system is developed, and the experimental results are presented to demonstrate the effectiveness of the approach.
Keywords: Design pattern; Design reuse; Design patterns modeling; Graph-oriented agglomerative hierarchical clustering; Design semantics; Reusable region

Sebastian Görg, Ralph Bergmann,
Social workflows—Vision and potential study,
Information Systems,
Volume 50,
2015,
Pages 1-19,
ISSN 0306-4379,
https://doi.org/10.1016/j.is.2014.12.007.
(https://www.sciencedirect.com/science/article/pii/S030643791400194X)
Abstract: Social workflows pervade peoples׳ everyday life. Whenever a group of persons works together on a challenging or multifaceted task, a social workflow begins. Unlike traditional business workflows, such social workflows aim at supporting processes that contain personal tasks and data. In this work, we envision a social workflow service as part of a social network that enables private individuals to construct social workflows according to their specific needs and to keep track of the workflow execution. The proposed features for a social workflow service could help individuals to accomplish their private goals. The presented idea is contrasted with established research areas and applications to show the degree of novelty of this work. It is shown how novel ideas for knowledge management, facilitated by a process-oriented case-based reasoning approach, support private individuals and how they can obtain an appropriate social workflow through sharing and reuse of respective experience. Two empirical studies confirm the potential benefits of a social workflow service in general and the core features of the developed concept.
Keywords: Social workflows; Potential study; Process-oriented case-based reasoning; Knowledge management

Nguyen Ngoc Chan, Nattawat Nonsung, Walid Gaaloul,
Service querying to support process variant development,
Journal of Systems and Software,
Volume 122,
2016,
Pages 538-552,
ISSN 0164-1212,
https://doi.org/10.1016/j.jss.2015.07.050.
(https://www.sciencedirect.com/science/article/pii/S0164121215001697)
Abstract: Developing process variants enables enterprises to effectively adapt their business models to different markets. Existing approaches focus on business process models to support the variant development. The assignment of services in a business process, which ensures the process variability, has not been widely examined. In this paper, we present an innovative approach that focuses on component services instead of process models. We target to recommend services to a selected position in a business process. We define the service composition context as the relationships between a service and its neighbors. We compute the similarity between services based on the matching of their composition contexts. Then, we propose a query language that considers the composition context matching for service querying. We developed an application to demonstrate our approach and performed different experiments on a public dataset of real process models. Experimental results show that our approach is feasible and efficient.
Keywords: Service-based business process; Service querying; Composition context matching

Eva Agapaki, Mohammad Nahangi,
Chapter 3 - Scene understanding and model generation,
Editor(s): Ioannis Brilakis, Carl Haas,
Infrastructure Computer Vision,
Butterworth-Heinemann,
2020,
Pages 65-167,
ISBN 9780128155035,
https://doi.org/10.1016/B978-0-12-815503-5.00003-6.
(https://www.sciencedirect.com/science/article/pii/B9780128155035000036)
Abstract: ICV scene understanding provides an overview about various object detection and fitting techniques that can produce accurate information for the digital twin (DT) generation of existing assets. The aim of this chapter is to acquire digital twins of existing infrastructure with the least amount of human intervention and give readers the appropriate tools to apply ICV scene understanding and fitting techniques for specific domain applications. The chapter gives an overview of scene understanding for infrastructure applications and user requirements and explores deep learning methods used so far on shape detection and the potential for its use in ICV scene understanding.
Keywords: 3D object fitting; BIM; Data management; Deep learning; Digital twins; IFC; Segmentation; Shape classification

Shiliang Zhang, Qi Tian, Gang Hua, Qingming Huang, Wen Gao,
ObjectPatchNet: Towards scalable and semantic image annotation and retrieval,
Computer Vision and Image Understanding,
Volume 118,
2014,
Pages 16-29,
ISSN 1077-3142,
https://doi.org/10.1016/j.cviu.2013.03.008.
(https://www.sciencedirect.com/science/article/pii/S1077314213001574)
Abstract: The ever increasing Internet image collection densely samples the real world objects, scenes, etc. and is commonly accompanied with multiple metadata such as textual descriptions and user comments. Such image data has potential to serve as a knowledge source for large-scale image applications. Facilitated by such publically available and ever-increasing loosely annotated image data on the Internet, we propose a scalable data-driven solution for annotating and retrieving Web-scale image data. We extrapolate from large-scale loosely annotated images a compact and informative representation, namely ObjectPatchNet. Each vertex in ObjectPatchNet, which is called as an ObjectPatchNode, is defined as a collection of discriminative image patches annotated with object category labels. The edge linking two ObjectPatchNodes models the co-occurrence relationship among different objects in the same image. Therefore, ObjectPatchNet models not only probabilistically labeled image patches, but also the contextual relationship between objects. It is well suited to scalable image annotation task. Besides, we further take ObjectPatchNet as a visual vocabulary with semantic labels, and hence are able to easily develop inverted file indexing for efficient semantic image retrieval. ObjectPatchNet is tested on both large-scale image annotation and large-scale image retrieval applications. Experimental results manifest that ObjectPatchNet is both discriminative and efficient in these applications.
Keywords: Visual vocabulary; Large-scale image retrieval; Image annotation

Zara Nasar, Syed Waqar Jaffry, Muhammad Kamran Malik,
Textual keyword extraction and summarization: State-of-the-art,
Information Processing & Management,
Volume 56, Issue 6,
2019,
102088,
ISSN 0306-4573,
https://doi.org/10.1016/j.ipm.2019.102088.
(https://www.sciencedirect.com/science/article/pii/S0306457319300044)
Abstract: With the advent of Web 2.0, there exist many online platforms that results in massive textual data production such as social networks, online blogs, magazines etc. This textual data carries information that can be used for betterment of humanity. Hence, there is a dire need to extract potential information out of it. This study aims to present an overview of approaches that can be applied to extract and later present these valuable information nuggets residing within text in brief, clear and concise way. In this regard, two major tasks of automatic keyword extraction and text summarization are being reviewed. To compile the literature, scientific articles were collected using major digital computing research repositories. In the light of acquired literature, survey study covers early approaches up to all the way till recent advancements using machine learning solutions. Survey findings conclude that annotated benchmark datasets for various textual data-generators such as twitter and social forms are not available. This scarcity of dataset has resulted into relatively less progress in many domains. Also, applications of deep learning techniques for the task of automatic keyword extraction are relatively unaddressed. Hence, impact of various deep architectures stands as an open research direction. For text summarization task, deep learning techniques are applied after advent of word vectors, and are currently governing state-of-the-art for abstractive summarization. Currently, one of the major challenges in these tasks is semantic aware evaluation of generated results.
Keywords: Automatic keyword extraction; Text summarization; Deep Learning

Zongda Wu, Hui Zhu, Guiling Li, Zongmin Cui, Hui Huang, Jun Li, Enhong Chen, Guandong Xu,
An efficient Wikipedia semantic matching approach to text document classification,
Information Sciences,
Volume 393,
2017,
Pages 15-28,
ISSN 0020-0255,
https://doi.org/10.1016/j.ins.2017.02.009.
(https://www.sciencedirect.com/science/article/pii/S0020025517304292)
Abstract: A traditional classification approach based on keyword matching represents each text document as a set of keywords, without considering the semantic information, thereby, reducing the accuracy of classification. To solve this problem, a new classification approach based on Wikipedia matching was proposed, which represents each document as a concept vector in the Wikipedia semantic space so as to understand the text semantics, and has been demonstrated to improve the accuracy of classification. However, the immense Wikipedia semantic space greatly reduces the generation efficiency of a concept vector, resulting in a negative impact on the availability of the approach in an online environment. In this paper, we propose an efficient Wikipedia semantic matching approach to document classification. First, we define several heuristic selection rules to quickly pick out related concepts for a document from the Wikipedia semantic space, making it no longer necessary to match all the concepts in the semantic space, thus greatly improving the generation efficiency of the concept vector. Second, based on the semantic representation of each text document, we compute the similarity between documents so as to accurately classify the documents. Finally, evaluation experiments demonstrate the effectiveness of our approach, i.e., which can improve the classification efficiency of the Wikipedia matching under the precondition of not compromising the classification accuracy.
Keywords: Wikipedia matching; Keyword matching; Document classification; Semantics

Chhavi Dhiman, Dinesh Kumar Vishwakarma,
A review of state-of-the-art techniques for abnormal human activity recognition,
Engineering Applications of Artificial Intelligence,
Volume 77,
2019,
Pages 21-45,
ISSN 0952-1976,
https://doi.org/10.1016/j.engappai.2018.08.014.
(https://www.sciencedirect.com/science/article/pii/S0952197618301775)
Abstract: The concept of intelligent visual identification of abnormal human activity has raised the standards of surveillance systems, situation cognizance, homeland safety and smart environments. However, abnormal human activity is highly diverse in itself due to the aspects such as (a) the fundamental definition of anomaly (b) feature representation of an anomaly, (c) its application, and henceforth (d) the dataset. This paper aims to summarize various existing abnormal human activity recognition (AbHAR) handcrafted and deep approaches with the variation of the type of information available such as two-dimensional or three-dimensional data. Features play a vital role in an excellent performance of an AbHAR system. The proposed literature provides feature designs of abnormal human activity recognition in a video with respect to the context or application such as fall detection, Ambient Assistive Living (AAL), homeland security, surveillance or crowd analysis using RGB, depth and skeletal evidence. The key contributions and limitations of every feature design technique, under each category: 2D and 3D AbHAR, in respective contexts are tabulated that will provide insight of various abnormal action detection approaches. Finally, the paper outlines newly added datasets for AbHAR by the researchers with added complexities for method validations.
Keywords: Two-dimensional anomaly detection; Three-dimensional anomaly detection; Crowd anomaly; Skeleton based fall detection; Ambient Assistive Living

Paloma Martínez, José L. Martínez, Isabel Segura-Bedmar, Julián Moreno-Schneider, Adrián Luna, Ricardo Revert,
Turning user generated health-related content into actionable knowledge through text analytics services,
Computers in Industry,
Volume 78,
2016,
Pages 43-56,
ISSN 0166-3615,
https://doi.org/10.1016/j.compind.2015.10.006.
(https://www.sciencedirect.com/science/article/pii/S0166361515300518)
Abstract: In the last years, the habit of discussing healthcare issues with family and friends, even with unknown people, in the context of social networks has increased and processing user generated content has become a new challenge. This can help in on-line crowd surveillance for different applications (pharmacovigilance and filtering health contents in blogs among others) as well as extracting knowledge from unstructured text sources. In this article, a system that monitors health social media streams is described. It is based on several text analytics processes supported, among others, by MeaningCloud, a commercial platform which provides meaning extraction from texts in a Software as a Service mode. In this architecture, several domain resources are integrated to detect drugs and drug effects such as CIMA (official information about authorized drugs in Spain maintained by the Spanish Agency of Medicines and Health Products), MedDRA (Medical Dictionary for Regulatory Activities) and the SpanishDrugEffectDB database that contains relations between drugs and effects. Different ways of visualizing data considering time lines and aggregated data have been implemented. In order to show performance, an evaluation has been carried out over Named Entities Recognition (NER) and Relation Extraction (RE) tasks.

G. Reali, M. Femminella, E. Nunzi, D. Valocchi,
Genomics as a service: A joint computing and networking perspective,
Computer Networks,
Volume 145,
2018,
Pages 27-51,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2018.08.005.
(https://www.sciencedirect.com/science/article/pii/S1389128618307205)
Abstract: This paper shows a global picture of the deployment of networked processing services for genomic data sets. Many current research and medical activities make an extensive use of genomic data, which are massive and rapidly increasing over time. They are typically stored in remote databases, accessible by using Internet connections. For this reason, the quality of the available network services could be a significant issue for effectively handling genomic data through networks. A first contribution of this paper consists in identifying the still unexploited features of genomic data that could allow optimizing their networked management. The second and main contribution is a methodological classification of computing and networking alternatives, which can be used to deploy what we call the Genomics-as-a-Service (GaaS) paradigm. In more detail, we analyze the main genomic processing applications, and classify both the computing alternatives to run genomics workflows, in either a local machine or a distributed cloud environment, and the main software technologies available to develop genomic processing services. Since an analysis encompassing only the computing aspects would provide only a partial view of the issues for deploying GaaS systems, we present also the main networking technologies that are available to efficiently support a GaaS solution. We first focus on existing service platforms, and analyze them in terms of service features, such as scalability, flexibility, and efficiency. Then, we present a taxonomy for both wide area and datacenter network technologies that may fit the GaaS requirements. It emerges that virtualization, both in computing and networking, is the key for a successful large-scale exploitation of genomic data, by pushing ahead the adoption of the GaaS paradigm. Finally, the paper illustrates a short and long-term vision on future research challenges in the field.
Keywords: Genomic; Pipeline; Cloud computing; Big data; Network virtualization

Juefei Yuan, Hameed Abdul-Rashid, Bo Li, Yijuan Lu, Tobias Schreck, Song Bai, Xiang Bai, Ngoc-Minh Bui, Minh N. Do, Trong-Le Do, Anh-Duc Duong, Kai He, Xinwei He, Mike Holenderski, Dmitri Jarnikov, Tu-Khiem Le, Wenhui Li, Anan Liu, Xiaolong Liu, Vlado Menkovski, Khac-Tuan Nguyen, Thanh-An Nguyen, Vinh-Tiep Nguyen, Weizhi Nie, Van-Tu Ninh, Perez Rey, Yuting Su, Vinh Ton-That, Minh-Triet Tran, Tianyang Wang, Shu Xiang, Shandian Zhe, Heyu Zhou, Yang Zhou, Zhichao Zhou,
A comparison of methods for 3D scene shape retrieval,
Computer Vision and Image Understanding,
Volume 201,
2020,
103070,
ISSN 1077-3142,
https://doi.org/10.1016/j.cviu.2020.103070.
(https://www.sciencedirect.com/science/article/pii/S1077314220301090)
Abstract: 3D scene shape retrieval is a brand new but important research direction in content-based 3D shape retrieval. To promote this research area, two Shape Retrieval Contest (SHREC) tracks on 2D scene sketch-based and image-based 3D scene model retrieval have been organized by us in 2018 and 2019, respectively. In 2018, we built the first benchmark for each track which contains 2D and 3D scene data for ten (10) categories, while they share the same 3D scene target dataset. Four and five distinct 3D scene shape retrieval methods have competed with each other in these two contests, respectively. In 2019, to measure and compare the scalability performance of the participating and other promising Query-by-Sketch or Query-by-Image 3D scene shape retrieval methods, we built a much larger extended benchmark for each type of retrieval which has thirty (30) classes and organized two extended tracks. Again, two and three different 3D scene shape retrieval methods have contended in these two tracks, separately. To solicit state-of-the-art approaches, we perform a comprehensive comparison of all the above methods and an additional new retrieval methods by evaluating them on the two benchmarks. The benchmarks, evaluation results and tools are publicly available at our track websites (Yuan et al., 2019 [1]; Abdul-Rashid et al., 2019 [2]; Yuan et al., 2019 [3]; Abdul-Rashid et al., 2019 [4]), while code for the evaluated methods are also available: http://github.com/3DSceneRetrieval.
Keywords: 3D scenes; 3D shape retrieval; Scene benchmark; Performance evaluation; Query-by-Sketch; Query-by-Image; Scene understanding; Scene semantics; SHREC

Sören Brügmann, Nadjet Bouayad-Agha, Alicia Burga, Serguei Carrascosa, Alberto Ciaramella, Marco Ciaramella, Joan Codina-Filba, Enric Escorsa, Alex Judea, Simon Mille, Andreas Müller, Horacio Saggion, Patrick Ziering, Hinrich Schütze, Leo Wanner,
Towards content-oriented patent document processing: Intelligent patent analysis and summarization,
World Patent Information,
Volume 40,
2015,
Pages 30-42,
ISSN 0172-2190,
https://doi.org/10.1016/j.wpi.2014.10.003.
(https://www.sciencedirect.com/science/article/pii/S0172219014001410)
Abstract: In this article, we present an operational prototype of a workbench for intelligent patent document analysis and summarization that has been developed in the context of the R&D project TOPAS, partially funded by the European Commission. The workbench uses the GATE environment as infrastructure for document representation and algorithm integration. It contains, apart from several preprocessing tools, five modules for the individual aspects of patent analysis (entity recognition, lexical chain identification, invention composition derivation, segmentation, and claim – description alignment) and a module for patent summarization. The workbench, which has been tested in different application settings, can be used as a standalone engine or as component within a more global patent processing line. Most of its modules can be also used separately.
Keywords: Entity recognition; Segmentation; Lexical chain identification; Claim description alignment; Summarization; TOPAS; Patent analysis; Document processing

Shilpak Chatterjee, Anusara Daenthanasanmak, Paramita Chakraborty, Megan W. Wyatt, Payal Dhar, Shanmugam Panneer Selvam, Jianing Fu, Jinyu Zhang, Hung Nguyen, Inhong Kang, Kyle Toth, Mazen Al-Homrani, Mahvash Husain, Gyda Beeson, Lauren Ball, Kristi Helke, Shahid Husain, Elizabeth Garrett-Mayer, Gary Hardiman, Meenal Mehrotra, Michael I. Nishimura, Craig C. Beeson, Melanie Gubbels Bupp, Jennifer Wu, Besim Ogretmen, Chrystal M. Paulos, Jeffery Rathmell, Xue-Zhong Yu, Shikhar Mehrotra,
CD38-NAD+Axis Regulates Immunotherapeutic Anti-Tumor T Cell Response,
Cell Metabolism,
Volume 27, Issue 1,
2018,
Pages 85-100.e8,
ISSN 1550-4131,
https://doi.org/10.1016/j.cmet.2017.10.006.
(https://www.sciencedirect.com/science/article/pii/S1550413117306186)
Abstract: Summary
Heightened effector function and prolonged persistence, the key attributes of Th1 and Th17 cells, respectively, are key features of potent anti-tumor T cells. Here, we established ex vivo culture conditions to generate hybrid Th1/17 cells, which persisted long-term in vivo while maintaining their effector function. Using transcriptomics and metabolic profiling approaches, we showed that the enhanced anti-tumor property of Th1/17 cells was dependent on the increased NAD+-dependent activity of the histone deacetylase Sirt1. Pharmacological or genetic inhibition of Sirt1 activity impaired the anti-tumor potential of Th1/17 cells. Importantly, T cells with reduced surface expression of the NADase CD38 exhibited intrinsically higher NAD+, enhanced oxidative phosphorylation, higher glutaminolysis, and altered mitochondrial dynamics that vastly improved tumor control. Lastly, blocking CD38 expression improved tumor control even when using Th0 anti-tumor T cells. Thus, strategies targeting the CD38-NAD+ axis could increase the efficacy of anti-tumor adoptive T cell therapy.

Arun Kumar Singh, Neda Firoz, Ashish Tripathi, K.K. Singh, Pushpa Choudhary, Prem Chand Vashist,
Chapter 7 - Internet of Things: from hype to reality,
Editor(s): Valentina Emilia Balas, Vijender Kumar Solanki, Raghvendra Kumar,
An Industrial IoT Approach for Pharmaceutical Industry Growth,
Academic Press,
2020,
Pages 191-230,
ISBN 9780128213261,
https://doi.org/10.1016/B978-0-12-821326-1.00007-3.
(https://www.sciencedirect.com/science/article/pii/B9780128213261000073)
Abstract: The era of the Internet of Things (IoT) is sweeping over and replacing the Internet creating a world where smart things exist connected to each other intelligently. This was predicted by Eric Emerson Schmidt, the former C.E.O. of Google over 20 years ago. The physical world is now connecting to the digital world so quickly with the emergence of the IoT that it seems the Internet will become invisible soon, meaning the physical world will be connecting to the digital world seamlessly. The world will enjoy smart connectivity in the same way that the city of Barcelona has emerged to be the smartest city in the world. We are moving toward system-to-system connection, with smart networking reaching its peak. The idea of software-defined autonomous machines is about to become hugely important, which will become ubiquitous. With the advent of the IoT, we explore how it is becoming a reality and whether it has any limits. Maciej Kranz in his book on the IoT explains the very essential detailed and inclusive idea of the IoT, with IoT expanding to businesses, and covering and impacting on a variety of technology areas. Artificial intelligence and machine learning have a huge scope because of the enormous data generated by sensors and devices connected through the IoT. We will explore in this chapter the hype around the IoT and the reality. We will also discover improved metrics in the IoT that is allowing it to be a leader in the technological world. We are witnessing the fourth revolution in the digitization world and discuss the reasons behind its exponential growth. The protocols that differentiate them from others have evolved for IOT in a new set of patterns. This also creates security concerns and data are described as the new oil, raising further challenges of data privacy.
Keywords: IoT; Health IoT; IoT Hype; Reality; artificial intelligence; IT industry; RFID

Gerd Gröner, Marko Bošković, Fernando Silva Parreiras, Dragan Gašević,
Modeling and validation of business process families,
Information Systems,
Volume 38, Issue 5,
2013,
Pages 709-726,
ISSN 0306-4379,
https://doi.org/10.1016/j.is.2012.11.010.
(https://www.sciencedirect.com/science/article/pii/S0306437912001524)
Abstract: Process modeling is an expensive task that needs to encompass requirements of different stakeholders, assure compliance with different standards, and enable the flexible adaptivity to newly emerging requirements in today's dynamic global market. Identifying reusability of process models is a promising direction towards reducing the costs of process modeling. Recent research has offered several solutions. Such solutions promote effective and formally sound methods for variability modeling and configuration management. However, ensuring behavioral validity of reused process models with respect to the original process models (often referred to as reference process models) is still an open research challenge. To address this challenge, in this paper, we propose the notion of business process families by building upon the well-known software engineering discipline—software product line engineering. Business process families comprise (i) a variability modeling perspective, (ii) a process model template (or reference model), and (iii) mappings between (i) and (ii). For business process families, we propose a correct validation algorithm ensuring that each member of a business process family adheres to the core intended behavior that is specified in the process model template. The proposed validation approach is based on the use of Description Logics, variability is represented by using the well-known Feature Models and behavior of process models is considered in terms of control flow patterns. The paper also reports on the experience gained in two external trial cases and results obtained by measuring the tractability of the implementation of the proposed validation approach.
Keywords: Business process families; Control flow relations; Validation; Process model variability; Process model configuration

Paul Reedy,
Interpol review of digital evidence 2016 - 2019,
Forensic Science International: Synergy,
Volume 2,
2020,
Pages 489-520,
ISSN 2589-871X,
https://doi.org/10.1016/j.fsisyn.2020.01.015.
(https://www.sciencedirect.com/science/article/pii/S2589871X20300152)
Abstract: This review paper covers the forensic-relevant literature in digital evidence from 2016 to 2019 as a part of the 19th Interpol International Forensic Science Managers Symposium. The review papers are also available at the Interpol website at: https://www.interpol.int/content/download/14458/file/Interpol Review Papers 2019.pdf
Keywords: Digital forensics; Digital evidence; Network forensics

